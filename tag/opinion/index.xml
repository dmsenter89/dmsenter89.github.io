<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>opinion | Michael&#39;s Site</title>
    <link>https://dmsenter89.github.io/tag/opinion/</link>
      <atom:link href="https://dmsenter89.github.io/tag/opinion/index.xml" rel="self" type="application/rss+xml" />
    <description>opinion</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 25 Nov 2024 14:02:54 -0500</lastBuildDate>
    <image>
      <url>https://dmsenter89.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>opinion</title>
      <link>https://dmsenter89.github.io/tag/opinion/</link>
    </image>
    
    <item>
      <title>The Data Don&#39;t Speak for Themselves</title>
      <link>https://dmsenter89.github.io/post/24/11-the-data-dont-speak-for-themselves/</link>
      <pubDate>Mon, 25 Nov 2024 14:02:54 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/24/11-the-data-dont-speak-for-themselves/</guid>
      <description>&lt;p&gt;Ever heard someone say they were &amp;ldquo;letting the data speak for itself?&amp;rdquo; I
often encounter this phrase on the internet by someone claiming
not to be interpreting the data, but merely relaying facts. I don&amp;rsquo;t
believe that&amp;rsquo;s actually true in the sense that it&amp;rsquo;s typically meant.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a good minimal example I&amp;rsquo;ve used in a math modeling class before.
I got this table from slides in an epidemiology course where it was
presented in the context of racial disparities in health care. It shows rates
of infant mortality for two time points in two different racial groups
in the US. Here&amp;rsquo;s a reproduction of the table:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Year&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;White (Non-Hispanic)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Black (Non-Hispanic)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1950&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;26.8&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;43.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1998&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;13.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This represents the number of infants who died per 1,000 live births, so lower is better. The first thing to note and credit is
that the numbers have been plummeting in both columns, a testament to
our improvements in infant care.&lt;/p&gt;
&lt;p&gt;Now, if all I&amp;rsquo;m trying to do is to say that there is a disparity because
the numbers don&amp;rsquo;t match, that&amp;rsquo;s true. But it also trivial and
uninteresting. I&amp;rsquo;m usually much more interested in patterns and trends.
Given that we have two different time points, it is reasonable to ask a
question like &amp;ldquo;are the disparities getting better or worse?&amp;rdquo; As it turns out, and
we did this exercise in class, the question happens to be a bit vague, so it&amp;rsquo;s&lt;br&gt;
easy to come up with different answers both in direction and
magnitude of the disparity. I&amp;rsquo;ll group a couple of examples each by
outcome.&lt;/p&gt;
&lt;p&gt;Say I want to see the data as saying the racial disparity is getting
bigger, i.e. the gaps are widening in some sense. To measure the
disparity, the US Office of Minority Health uses the ratio of the
non-Hispanic Black to the non-Hispanic White rates. That would give us
$\frac{43.9}{26.8} \approx 1.63$ for 1950 versus of
$\frac{13.8}{6} = 2.3$ for 1998. This would tell a story of the gap
widening. This would imply the gap has grown by about 40%.&lt;/p&gt;
&lt;p&gt;Alternatively, we could construct the relative percentage difference
between the Black and White rates, which would give
$\frac{43.9 - 26.8}{26.8} \approx 0.64$ for 1950 vs
$\frac{13.8 - 6}{6} = 1.3$ for 1998. This also tells a story of
disparities growing, but it appears even more alarming than using the
previous method &amp;ndash; the 1998 difference is about twice that of 1950!&lt;/p&gt;
&lt;p&gt;If I want to take the opposite route and see the disparities as
improving, I could compare the rates of improvement in the infant mortality rates.
I have two data points for each racial category, so I can fit a line to each and
compare the slopes. That would give us
$\frac{6 - 26.8}{1998 - 1950} \approx - 0.43$ for Whites and
$\frac{13.8 - 43.9}{1998 - 1950} \approx - 0.63$ for Blacks. If I choose
this metric, improvements have been substantial. The rates have gone down
for Blacks about 50% faster than for Whites.&lt;/p&gt;
&lt;p&gt;We could also work with the number of deaths more directly. In 1950, there were
$43.9 - 26.8 = 17.1$ excess infant deaths per 1,000 live births amongst
Blacks compared to Whites, while in 1998 there were only
$13.8 - 6 = 7.8$ excess deaths per 1,000 live births. This metric could
also be framed as a success &amp;ndash; thanks to improvements in disparities, we
have nearly 10 fewer Black infant deaths per 1,000 live births than we
would have had the disparities of the 1950s persisted.&lt;/p&gt;
&lt;p&gt;Anybody wanting to opine on the matter could calculate any of these
measures and claim they are fairly representing the data as they see
them. All of these choices reflect what in statistics would be called an
&lt;em&gt;estimand&lt;/em&gt; &amp;ndash; a particular, mathematically defined answer to a
question that we seek to infer from the data using an estimator. Each
estimand is related to the scientific question being asked, but makes it
more precise. One issue that arises is that because the scientific
question of interest is by necessity a bit vague, at least when offered
initially, there are multiple valid routes to go about answering it.
This leads to what Gelman termed the 
&lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Garden of Forking
Paths&amp;rdquo;&lt;/a&gt;
and the different possible answers we can get to our question from a
data set.&lt;/p&gt;
&lt;p&gt;Another issue to consider is proper conditioning on factors playing a
role in the outcome of interest. A relatable illustration of this is the
famous 
&lt;a href=&#34;https://www.pewresearch.org/social-trends/2023/03/01/the-enduring-grip-of-the-gender-pay-gap/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;80 cents on the
dollar&amp;rdquo;&lt;/a&gt;
line about the US gender pay gap. This is an overall measure of group
differences often raised in a political context. While this represents true
observable group differences in wages, it is often used to imply wage
discrimination. Because that&amp;rsquo;s a more interesting question from a policy perspective.
But wage discrimination is usually thought of as two
individuals with the same background characteristics, say qualifications
and personality traits indicative of productivity, and differing in only
one aspect of interest, say race, gender, or sexual orientation, having
meaningfully different wages. So to be able to infer discrimination, we
have to also collect and analyze the necessary covariates that can
reflect differences in qualifications and ability. Wages have high
variability and different industries have very different pay-bands which
are often stratified to some degree by educational achievement. And we
know there are large gender gaps in fields of study. Here&amp;rsquo;s an
interesting link to an overview of this issue by

&lt;a href=&#34;https://www.bankrate.com/loans/student-loans/top-paying-college-majors-gender-gap/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bankrate.&lt;/a&gt;
Out of the six majors listed with a median salary of $100,000 or more,
only one was even close to even in enrollment by gender. Without taking
differences such as these (and others) into account, the noted group difference
in and of itself is not particularly enlightening.&lt;/p&gt;
&lt;p&gt;Back to our example on infant mortality. Since this is a medical issue,
we can look at whether there are established 
&lt;a href=&#34;https://www.cdc.gov/maternal-infant-health/infant-mortality/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;risk
factors&lt;/a&gt;
for it whose distribution might differ between groups. Here is a list of
the top 5 risk factors identified by the CDC:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;birth defects,&lt;/li&gt;
&lt;li&gt;preterm birth and low birth weight,&lt;/li&gt;
&lt;li&gt;sudden infant death syndrome,&lt;/li&gt;
&lt;li&gt;unintentional injuries, and&lt;/li&gt;
&lt;li&gt;maternal pregnancy complications.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since each of these factors are believed to affect the infant mortality
rate, it would be appropriate to include them in an analysis. After all, it&amp;rsquo;s
not a given that they do not differ by racial groups and variation in
risk factors may explain some of the variation in the observed
difference in infant mortality rates. For example: Blacks have about

&lt;a href=&#34;https://www.marchofdimes.org/peristats/data?reg=99&amp;amp;top=4&amp;amp;stop=45&amp;amp;lev=1&amp;amp;slev=1&amp;amp;obj=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;twice the rate of low birth weight
infants&lt;/a&gt;
compared to Whites. Among pregnancy complications, 
&lt;a href=&#34;https://doi.org/10.1001/jama.2017.3439&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;preeclampsia is more
common among Blacks&lt;/a&gt; than
Whites. For other risk factors, like gestational diabetes, the 
&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/10.1111/j.1365-3016.2010.01140.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rates
are
similar.&lt;/a&gt;
Including these risk factors in an analysis could increase or decrease
the observed disparity, but would simultaneously provide a richer
picture and ultimately a better answer to our scientific question. This is particularly
true if we&amp;rsquo;re asking this question from a public health perspective to guide
funding allocations in an effort to alleviate disparaties.&lt;/p&gt;
&lt;p&gt;So all this to say that I don&amp;rsquo;t believe the data speak for themselves.
Analysts use data to tell a story. And I don&amp;rsquo;t mean that maliciously. You can
sincerely tell different stories with the same data. Since there are many ways of
reasoning with and analyzing data, openness is key. And part of that
openness is clarity on the estimands we choose, and what they imply
about the specific question we&amp;rsquo;re asking. This becomes especially
important when the stakes are high, such as in regulatory review of
clinical trials or when policy decisions are at stake. We&amp;rsquo;re currently
seeing a move to more transparency here, see for example 
&lt;a href=&#34;https://www.fda.gov/regulatory-information/search-fda-guidance-documents/e9r1-statistical-principles-clinical-trials-addendum-estimands-and-sensitivity-analysis-clinical&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here for the
E9(R1)
Addendum&lt;/a&gt;
on estimands and sensitivity analysis in clinical trials. There&amp;rsquo;s also a
relatively recent 
&lt;a href=&#34;https://www.routledge.com/Estimands-Estimators-and-Sensitivity-Analysis-in-Clinical-Trials/Mallinckrodt-Molenberghs-Lipkovich-Ratitch/p/book/9781032242620?srsltid=AfmBOor0NBn9COUmntJla4NGNZKI2pSg-7OeMi4R0Qlb4vMd76iYhYDh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;book by Mallinckrodt et
al&lt;/a&gt;
offering relevant examples from the clinical setting on picking and
justifying appropriate estimands.&lt;/p&gt;
&lt;p&gt;This also shows that it pays to take a second look at published data,
something I&amp;rsquo;ve recently become more interested in. See 
&lt;a href=&#34;https://www.sensible-med.com/p/the-value-of-reanalysis-of-a-clinical&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this
post&lt;/a&gt;
by John Mandrola and the topic of reanalyzing clinical trial data and
their perhaps surprising finding: in their admittedly small sample, 35%
of the reanalysis of &lt;em&gt;existing, published trial data&lt;/em&gt; lead to different
interpretations than the originally published article.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why I&#39;m Not Worried About ChatGPT</title>
      <link>https://dmsenter89.github.io/post/23-01-why-im-not-worried-about-chatgpt/</link>
      <pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-01-why-im-not-worried-about-chatgpt/</guid>
      <description>&lt;p&gt;ChatGPT has been all over my newsfeed lately, with a considerable amount of hype. In particular, many are wondering or  even worrying whether the emergence of this technology will threaten jobs with moderate to high education requirments. See for example 
&lt;a href=&#34;https://www.theatlantic.com/ideas/archive/2023/01/chatgpt-ai-economy-automation-jobs/672767/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How ChatGPT Will Destabilize White-Collar Work&amp;rdquo; (The Atlantic)&lt;/a&gt;, where Annie Lowrey leads with &amp;ldquo;In the next five years, it is likely that AI will begin to reduce employment for college-educated workers.&amp;rdquo; I do not share these views. In fact, I am somewhat underwhelmed by the threat of ChatGPT for a number of reasons. Since this topic has come up a few times for me lately, I will write down my thoughts here so I can reference them more easily.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt-cannot-think&#34;&gt;ChatGPT Cannot Think&lt;/h2&gt;
&lt;p&gt;The first issue I take with many of the AI hype articles is that despite what the news coverage may imply, ChatGPT cannot think. To be honest, when I see articles talking about ChatGPT as &amp;ldquo;intelligent&amp;rdquo; or &amp;ldquo;thinking,&amp;rdquo; the first thing that comes to mind is 
&lt;a href=&#34;http://smbc-comics.com/comic/2011-08-17&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this SMBC&lt;/a&gt; from 2011-08-17:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.smbc-comics.com/comics/20110817.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In my view, ChatGPT is a lot like this parrot - except that I do think it is fundamentally different, and ChatGPT is not &amp;ldquo;conscious&amp;rdquo; and does not &amp;ldquo;think&amp;rdquo; in a meaningful way. Despite the many advances made, artificial intelligence (AI) functions differently than a natural intelligence (NI), and in any ChatGPT is not designed to &amp;ldquo;think.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Before giving a big picture view of how a large language model like ChatGPT works, I want to illustrate the limited flexibility of AI with an example from image recognition. An NI can readily distinguish between what is in the foreground and background of an image. Think of an image like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Insect_on_blue_flower.jpg/500px-Insect_on_blue_flower.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;A human will have no problem distinguishing between the insect in the image, the flower it is on, and the foliage in the background as distinct objects in different planes. This holds true even if the individual is not familiar with the particular plant or insect in the image. If given additional images of either the insect on a different background or the same background without the insect, we would not mistake the the plant for the insect or the other way around.&lt;/p&gt;
&lt;p&gt;Now consider an AI model trained to recognize insects. The algorithm doesn&amp;rsquo;t have a concept of &amp;ldquo;insect&amp;rdquo; or &amp;ldquo;plant,&amp;rdquo; per se. Rather, it notices patterns in images that are labeled &amp;ldquo;insect&amp;rdquo; or labeled with a particular insect. The pattern it learns does not depend on it having a concept of &amp;ldquo;insect.&amp;rdquo; What that means in practice, is that our model might learn that the background is equally or even more important than the foreground. If we train our data set with bees on flowers, but not flowers without bees, we may end up with a model that declares flower photos &amp;ldquo;bees.&amp;rdquo; This phenomeon is known in image recognition, and people are actively working on methods around this problem. But it nicely illustrates how AI is not &amp;ldquo;smart,&amp;rdquo; and humans need to do a lot of heavy lifting to get the AI algorithm to perform as intended, even if the application domain is relatively limited. For more information on this application, see 
&lt;a href=&#34;https://gradientscience.org/background/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this article&lt;/a&gt; from GradientScience.&lt;/p&gt;
&lt;h2 id=&#34;how-does-it-work&#34;&gt;How does it Work?&lt;/h2&gt;
&lt;p&gt;With this background, let&amp;rsquo;s get an overview of how models like ChatGPT work. A good summary of the techniques involved is detailed in 
&lt;a href=&#34;https://www.assemblyai.com/blog/how-chatgpt-actually-works/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt; by AssemblyAI. In simple terms, a model is exposed to large amounts of data in order to learn about the structure of words and how the are aligned in sentences. In principle, this is not too different from the text prediction feature you have on your phone while texting. But this methodology only works to help produce coherent or seemingly coherent sentences by completion. Marked language modeling is a method use to help the model learn about syntax as well to improve the output.&lt;/p&gt;
&lt;p&gt;What is new with ChatGPT is that in addition labeled training material, it utilizes human feedback to improve its output. Deep down, AI models can be thought of as optimizing some (very complicated) function. This goal function need not necessarily be written down explicitly. OpenAI uses a method where a model gives two possible outputs for a prompt, and then a human judges which is &amp;ldquo;better,&amp;rdquo; somewhat similar to when an optometrist asks you if &amp;ldquo;1&amp;rdquo; or &amp;ldquo;2&amp;rdquo; is better. It then uses this feedback to improve its output iteratively. See 
&lt;a href=&#34;https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this blog post&lt;/a&gt; from OpenAI where they use this methodology to animate a backflip.&lt;/p&gt;
&lt;p&gt;ChatGPT uses 
&lt;a href=&#34;https://www.assemblyai.com/blog/how-chatgpt-actually-works/#reinforcement-learning-from-human-feedback&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;three steps&lt;/a&gt; for human feedback based reinforcement learning. You can already imagine some of the issues that can arise from using this method. For one, if human feedback is used to train the model, then we can expect the model to reflect the thoughts and opinions of the labelers to some degree. Labelers may be mistaken and might not be experts in whatever topic they are reviewing. They may be fundamentally mistaken or biased about what we would consider high school-level knowledge.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; This is on top of the issues of the large amount of source text used in the initial training phase. These source texts may vary wildly in style and accuracy. Even humans reviewing an article may not be able to distinguish facts from opinion, let alone a language model using many source texts as input. Which leads us to what I see as a main problem for ChatGPT.&lt;/p&gt;
&lt;h2 id=&#34;factual-inaccuracies&#34;&gt;Factual Inaccuracies&lt;/h2&gt;
&lt;p&gt;Despite the confidence exuded by ChatGPTs output, it will readily produce a number of factual inaccuracies or give bad advice when explaining how to do tasks. See for example Avram Piltch&amp;rsquo;s 
&lt;a href=&#34;https://www.tomshardware.com/news/chatgpt-told-me-break-my-cpu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;I Asked ChatGPT How to Build a PC. It Told Me to Break My CPU&amp;rdquo; (Tom&amp;rsquo;s Hardware)&lt;/a&gt;, where ChatGPT gives instructions for a computer assembly that is potentially damaging to the hardware.&lt;/p&gt;
&lt;p&gt;Or 
&lt;a href=&#34;https://toolguyd.com/ai-chatgpt-cordless-drill-recommendation-2023/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this article (ToolGuyd)&lt;/a&gt; where Stuart asked ChatGPT to recommend a cordless powerdrill. ChatGPT made three recommendations. In explaining its recommendations, it gave several tech specs about the recommended products. The only problem is that it got several of these items wrong. It made mistakes about what type of drill a particular model was, whether the battery is included in the particular SKU it listed or not, and how many BPM the model delivers. It also recommended a discontinued model.&lt;/p&gt;
&lt;p&gt;As a third example, consider 
&lt;a href=&#34;https://betonit.substack.com/p/chatgpt-takes-my-midterm-and-gets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt; where economics professor Bryan Caplan attempts to let ChatGPT take one his more recent midterms. It&amp;rsquo;s quite detailed and includes the questions, answers, and grading rubric Bryan used. He gave ChatGPT a D on this exam, substantially below the average grade human students in the class received.&lt;/p&gt;
&lt;p&gt;I would like to highlight that my argument isn&amp;rsquo;t that ChatGPT gets everything wrong - it doesn&amp;rsquo;t. It can even perform exceptionally well at certain tasks. See 
&lt;a href=&#34;https://mackinstitute.wharton.upenn.edu/wp-content/uploads/2023/01/Christian-Terwiesch-Chat-GTP.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this white paper&lt;/a&gt; by Christian Terwiesch grading ChatGPT&amp;rsquo;s attempt at the final exam Wharton Business School MBA core course for just one example. A little googling will quickly lead to other examples, such as it passing law school exams or giving decent answers to tech sector interview questions.&lt;/p&gt;
&lt;p&gt;My concern is that it sounds very confident in its answers, but it is not always trivial for the average person to verify whether or not ChatGPT&amp;rsquo;s output is trustworthy. As Rupert Goodwin 
&lt;a href=&#34;https://www.theregister.com/2022/12/12/chatgpt_has_mastered_the_confidence/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;put it&lt;/a&gt;, ChatGPT is &amp;ldquo;a Dunning-Kruger effect knowledge simulator par excellence.&amp;rdquo; And that&amp;rsquo;s a problem if people decide to just trust it to produce truth, when ChatGPT has no idea what &amp;ldquo;truth&amp;rdquo; is. It&amp;rsquo;s important to know that OpenAI is aware of this and it even says so on it&amp;rsquo;s 
&lt;a href=&#34;https://help.openai.com/en/articles/6783457-chatgpt-faq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FAQ page&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Can I trust that the AI is telling me the truth?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;a. ChatGPT is not connected to the internet, and it can occasionally produce incorrect answers. It has limited knowledge of world and events after 2021 and may also occasionally produce harmful instructions or biased content.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;d recommend checking whether responses from the model are accurate or not. If you find an answer is incorrect, please provide that feedback by using the &amp;ldquo;Thumbs Down&amp;rdquo; button.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In my opinion this is reasonable and to be expected. I think some people may get too excited and feel too confident in this technology when it just isn&amp;rsquo;t as reliable as many would wish at this stage. And for those reasons, I don&amp;rsquo;t think it&amp;rsquo;s coming for our jobs any time soon.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; If you use ChatGPT, be careful to not give it any sensitive information. OpenAI isn&amp;rsquo;t making this very expensive model available to you for free out of the goodness of their hearts. They&amp;rsquo;re using your interaction with it to 
&lt;a href=&#34;https://help.openai.com/en/articles/6783457-chatgpt-faq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;further train the model&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update 3/21:&lt;/em&gt; There is a 
&lt;a href=&#34;http://web.archive.org/web/20230318145629/https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;good article&lt;/a&gt; in the New Yorker regarding my point that ChatGPT doesn&amp;rsquo;t &amp;ldquo;think.&amp;rdquo; This is &lt;em&gt;contra&lt;/em&gt; Daniel Miessler&amp;rsquo;s 
&lt;a href=&#34;https://danielmiessler.com/blog/yes-gpts-llms-understand-argument/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;argument&lt;/a&gt; that ChatGPT and similar models exhibit &amp;ldquo;understanding.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update 4/4:&lt;/em&gt; And here a 
&lt;a href=&#34;https://fakenous.substack.com/p/how-much-should-you-freak-out-about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;good post&lt;/a&gt; by Michael Huemer on this issue.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;For a good review of the many ways in which typical adults are uninformed and mistaken about issues contra accepted expert opinion, see: B. Caplan, &lt;em&gt;The Myth of the Rational Voter: Why Democracies Choose Bad Policies&lt;/em&gt;, Princeton University Press, Princeton, NJ, 2007. And B. Caplan, &lt;em&gt;The Case against Education: Why the Education System Is a Waste of Time and Money&lt;/em&gt;, Princeton University Press, Princeton, NJ, 2019. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
