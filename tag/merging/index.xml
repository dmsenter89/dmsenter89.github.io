<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>merging | Michael&#39;s Site</title>
    <link>https://dmsenter89.github.io/tag/merging/</link>
      <atom:link href="https://dmsenter89.github.io/tag/merging/index.xml" rel="self" type="application/rss+xml" />
    <description>merging</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 05 Sep 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dmsenter89.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>merging</title>
      <link>https://dmsenter89.github.io/tag/merging/</link>
    </image>
    
    <item>
      <title>Some Basic SQL Joins</title>
      <link>https://dmsenter89.github.io/post/23-09-basic-sql-joins/</link>
      <pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-09-basic-sql-joins/</guid>
      <description>&lt;p&gt;A non-technical friend recently asked me for help with a merge problem. They had two separate data pulls of electronic medical records based on specific study parameters. The set of people in the database who fit the study parameters changed in between the data pulls, for example by having people age into our out of a study, or by having new diagnoses added to their records that cause them to either be newly included or excluded. Let&amp;rsquo;s call the older data set A and the newer data set B. The goal was to get all those entries from B that don&amp;rsquo;t also show up in A. The data sets were pulled by a staff data scientist at that company who, despite their title, said they couldn&amp;rsquo;t figure out how to remove those entries from B that were already in A. Barring any special circumstances, this is a fairly standard problem so let&amp;rsquo;s look at a couple of tools we could use to solve it.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with some made-up sample data:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-minimal-sample-data-for-demonstration-purposes&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/sample-data_huec327b38ea9381c4b4e186ea675e12fc_20885_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Minimal sample data for demonstration purposes.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/sample-data_huec327b38ea9381c4b4e186ea675e12fc_20885_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;474&#34; height=&#34;322&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Minimal sample data for demonstration purposes.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- ```shell
$ bat *.csv
───────┬────────────────────────────────────────────────────────────────────
       │ File: A.csv
───────┼────────────────────────────────────────────────────────────────────
   1   │ MRN,Weight,Chol_Status
   2   │ 23356,140,
   3   │ 74592,,Desirable
   4   │ 79602,139,High
───────┴────────────────────────────────────────────────────────────────────
───────┬────────────────────────────────────────────────────────────────────
       │ File: B.csv
───────┼────────────────────────────────────────────────────────────────────
   1   │ MRN,Weight,Chol_Status
   2   │ 64836,129,High
   3   │ 79602,139,High
   4   │ 2466,127,Borderline
───────┴────────────────────────────────────────────────────────────────────
``` --&gt;
&lt;p&gt;The MRN here stands for &amp;ldquo;medical record number,&amp;rdquo; a common unique identifier present in clinical data sets. Each of our data sets has three rows, but only one row is shared between both - that associated with MRN 79602. We could theoretically merge on multiple columns or coalesce data if we think some missing fields might have been updated in the meantime, but for purposes of this example we&amp;rsquo;ll keep it simple and just merge on the MRN.&lt;/p&gt;
&lt;h2 id=&#34;sql-merge-types&#34;&gt;SQL Merge Types&lt;/h2&gt;
&lt;p&gt;There are four basic types of merge: left join, right join, outer join, and inner join. There&amp;rsquo;s also the cross join but that one shows up less frequently in my experience. A picture speaks a thousand words, so here&amp;rsquo;s a Venn diagram illustrating the idea behind these joins.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-standard-sql-joins&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/sql-joins_hu519d137be5f72d83cc0fb24e7060e243_236726_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Standard SQL Joins.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/sql-joins_hu519d137be5f72d83cc0fb24e7060e243_236726_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2593&#34; height=&#34;1814&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Standard SQL Joins.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In our case, we actually want left/right &amp;ldquo;inner&amp;rdquo; or &amp;ldquo;exclusive&amp;rdquo; joins, like this:&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/exclusive-joins60_hu03ae2ec37b24cdee5a828b5b640777a8_45542_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/exclusive-joins60_hu03ae2ec37b24cdee5a828b5b640777a8_45542_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;377&#34; height=&#34;514&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;implementations&#34;&gt;Implementations&lt;/h2&gt;
&lt;p&gt;I figured I would go over three basic tools: SAS, SQL, and Pandas.&lt;/p&gt;
&lt;h3 id=&#34;only-in-a&#34;&gt;Only in A&lt;/h3&gt;
&lt;p&gt;For starters, we want all entries in $A$ that are not also in $B$. In set notation that is the set denoted $A-B$ (sometimes $A\backslash B$).
Merges like this is what SQL excels at, so let&amp;rsquo;s see the SQL statment first:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table res1 as
  select A.* from
    A left join B 
    on A.MRN=B.MRN
    where B.MRN is NULL;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should run in any typical SQL implementation, including PROC SQL in SAS and SQLite3. We expect the following table as output:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;MRN&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;th&gt;Chol_Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;23356&lt;/td&gt;
&lt;td&gt;140&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;74592&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Desirable&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To do a left outer join instead, we would just omit the &lt;code&gt;where&lt;/code&gt; clause. We could do the same with a data step merge statement, but unlike SQL this would assume our input data sets are sorted by the merge key:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data res1;
  merge A (IN = X) B (IN=Y);
  by MRN;
  If X and not Y;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pandas&#39; merge statement allows for the creation of an indicator variable, similar to the &lt;code&gt;in&lt;/code&gt; keyword used in the SAS data step merge. That indicator will tell us if the particular row is present in both the left and the right tables (value &lt;code&gt;both&lt;/code&gt;), or just in one of them (values &lt;code&gt;left_only&lt;/code&gt; and &lt;code&gt;right_only&lt;/code&gt;). We can then query on that indicator variable to subset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;res1 = (pd.merge(A, B, how=&#39;outer&#39;, indicator=True)
            .query(&#39;_merge==&amp;quot;left_only&amp;quot;&#39;)
            .drop(&#39;_merge&#39;, axis=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;only-in-b&#34;&gt;Only in B&lt;/h3&gt;
&lt;p&gt;Same idea, but reversed: $B-A$. The implementation is identical except that we are using a right join instead:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table res2 as
  select B.* from
  A right join B
  on A.MRN=B.MRN
  where A.MRN is NULL;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Expected output:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;MRN&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;th&gt;Chol_Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;64836&lt;/td&gt;
&lt;td&gt;129&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2466&lt;/td&gt;
&lt;td&gt;127&lt;/td&gt;
&lt;td&gt;Borderline&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is interesting to note that SQLite, at least as of 3.37.2, still doesn&amp;rsquo;t support right joins, so if you&amp;rsquo;re using that you&amp;rsquo;ll just want to use the left join method above but switch the &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; around. The data step implementation is also straight forward:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data res2;
  merge A (IN = X) B (IN=Y);
  by MRN;
  If Y and not X;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;as is the Pandas version:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;res2 = (pd.merge(A, B, how=&#39;outer&#39;, indicator=True)
            .query(&#39;_merge==&amp;quot;right_only&amp;quot;&#39;)
            .drop(&#39;_merge&#39;, axis=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;whats-in-common&#34;&gt;What&amp;rsquo;s in common?&lt;/h3&gt;
&lt;p&gt;Finally, you  might be curious to see which rows both data sets have in common, that is $A \cap B$. That&amp;rsquo;s a simple inner join:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table res3 as
  select A.* from 
  A inner join B
  on A.MRN=B.MRN;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Expected output:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;MRN&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;th&gt;Chol_Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;79602&lt;/td&gt;
&lt;td&gt;139&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In SAS:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data res3;
  merge A (IN = X) B (IN=Y);
  by MRN;
  If X and Y;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and in Pandas:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;res3 = (pd.merge(A, B, how=&#39;outer&#39;, indicator=True)
            .query(&#39;_merge==&amp;quot;both&amp;quot;&#39;)
            .drop(&#39;_merge&#39;, axis=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it. All that&amp;rsquo;s left to do is to save the data in a format your customer or colleagues can work with and we&amp;rsquo;re done.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The INDSNAME Option in SAS</title>
      <link>https://dmsenter89.github.io/post/22-04-sas-indsname-option/</link>
      <pubDate>Wed, 20 Apr 2022 11:42:02 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-04-sas-indsname-option/</guid>
      <description>&lt;p&gt;I frequently find myself needing to concatenate data sets but also wanting to be able to distinguish
which row came from which data set originally. Introductory SAS courses tend to teach the &lt;code&gt;in&lt;/code&gt; keyword,
for a workflow similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data Concat1;
set data1(in = ds0)  
    data2(in = ds1);
if ds0 then source = &amp;quot;data1&amp;quot;;
else if ds1 then source = &amp;quot;data2&amp;quot;;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With more than two input data sets, this can get unwieldy and repetitive. In an old 
&lt;a href=&#34;https://blogs.sas.com/content/iml/2015/08/03/indsname-option.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;
on Rick Wicklin&amp;rsquo;s DO LOOP, a better method is introduced - the &lt;code&gt;indsname&lt;/code&gt; option. Using this method, the above code looks much nicer:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data Concat2;
set data1-data2 indsname = source;  /* the INDSNAME= option is on the SET statement */
libref = scan(source,1,&#39;.&#39;);        /* extract the libref */
dsname = scan(source,2,&#39;.&#39;);        /* extract the data set name */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As long as your input data sets are reasonably named, you&amp;rsquo;ll now have access to all the information needed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>North Carolina Housing Data</title>
      <link>https://dmsenter89.github.io/post/20-11-north-carolina-housing/</link>
      <pubDate>Fri, 06 Nov 2020 10:10:01 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/20-11-north-carolina-housing/</guid>
      <description>&lt;p&gt;A popular beginners machine learning problem is the prediction of housing prices. A frequently used data set for this purpose uses housing prices in California along some additional  gathered through the 1990 Census. One such data set is available 
&lt;a href=&#34;https://www.kaggle.com/camnugent/california-housing-prices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; at Kaggle. Unfortunately, that data set is rather old. And I live in North Carolina, not California! So I figured I might as well create a new housing data set, but this time with more up-to-date information and using North Carolina as the state to be analyzed. One thing that may be interesting about North Carlina as compared to California is the position of major populations centers. In California, major population centers are near the beach, while major population centers in North Carolina are in the interior of the state. Both large citites and proximity to the beach tend to correlate with higher housing prices. In California, unlike in North Carolina, both of these go together.&lt;/p&gt;
&lt;p&gt;This post will describe the Kaggle data set with California housing prices and then walk you through how the relevant data can be acquired from the Census Bureau. I&amp;rsquo;ll also show how to clean the data. For those who just want to explore the complete data set, I have made it available for download &lt;a href=&#34;https://dmsenter89.github.io/files/NC_Housing_Prices_2018.csv&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#the-source-data-set&#34;&gt;The Source Data Set&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acquiring-the-census-data-set&#34;&gt;Acquiring the Census Data Set&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#census-variables&#34;&gt;Census Variables&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#geography-considerations&#34;&gt;Geography Considerations&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acquiring-location-data&#34;&gt;Acquiring Location Data&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-merge-with-geoid-matching&#34;&gt;Data Merge with GEOID Matching&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-cleaning&#34;&gt;Data Cleaning&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;the-source-data-set&#34;&gt;The Source Data Set&lt;/h2&gt;
&lt;p&gt;The geographic unit of the Kaggle data set is the Census block group, which means we will have several thousand data points for our analysis. For a good big-picture overview of Census geography divisions, see this 
&lt;a href=&#34;https://pitt.libguides.com/uscensus/understandinggeography&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt; from the University of Pittsburgh library. The data set&amp;rsquo;s ten columns contain geographic, housing, and Census information that can be broken down as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;geographic information
&lt;ul&gt;
&lt;li&gt;longitude&lt;/li&gt;
&lt;li&gt;latitude&lt;/li&gt;
&lt;li&gt;ocean proximity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;housing information
&lt;ul&gt;
&lt;li&gt;median age of homes&lt;/li&gt;
&lt;li&gt;median value of homes&lt;/li&gt;
&lt;li&gt;total number of rooms in area&lt;/li&gt;
&lt;li&gt;total number of bedrooms in the area&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Census information
&lt;ul&gt;
&lt;li&gt;population&lt;/li&gt;
&lt;li&gt;number of households&lt;/li&gt;
&lt;li&gt;median income&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most of these exist directly in the Census API data that we have 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;covered previously&lt;/a&gt;. The ocean proximity variable is a categorical giving approximate distance from the beach. My data set will not include this last categorical variable.&lt;/p&gt;
&lt;h2 id=&#34;acquiring-the-census-data-set&#34;&gt;Acquiring the Census Data Set&lt;/h2&gt;
&lt;h3 id=&#34;census-variables&#34;&gt;Census Variables&lt;/h3&gt;
&lt;p&gt;The first, and most time consuming aspect, is to figure out where the data we want is located. We know that the US has a decennial census, so accurate information is available every ten years at every level of geography that the Census Bureau tracks. Since it is currently a census year 2020 and the newest information hasn&amp;rsquo;t been tabulated yet, that means the last census count that is available is from 2010. While this is 20 years more current than the California set from 1990, it still seems a bit outdated. Luckily, since the introduction of the American Community Survey (ACS) we have annually updated information available - but not for every level of geography. Only the 5-year ACS average gives us census block-level information for the whole state, making it comparable to the Kaggle data set. The most recent of these is the 2018.&lt;/p&gt;
&lt;p&gt;I start by creating a data dictionary from the 
&lt;a href=&#34;https://api.census.gov/data/2018/acs/acs5/groups.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;groups&lt;/a&gt; and 
&lt;a href=&#34;https://api.census.gov/data/2018/acs/acs5/variables.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;variables&lt;/a&gt; pages of the &amp;ldquo;American Community Survey: 1-Year Estimates: Detailed Tables 5-Year&amp;rdquo; data set. Note that median home age is not directly available. Instead, we will use the median year structures were built to calculate the median home age. Our data dictionary also does not include any data for the longitude and latitude of each row. We will get that data separately.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dictionary = {
    &#39;B01001_001E&#39; : &amp;quot;population&amp;quot;,
    &#39;B11001_001E&#39; : &amp;quot;households&amp;quot;,
    &#39;B19013_001E&#39; : &amp;quot;median_income&amp;quot;, 
    &#39;B25077_001E&#39; : &amp;quot;median_house_value&amp;quot;,
    &#39;B25035_001E&#39; : &amp;quot;median_year_structure_built&amp;quot;,
    &#39;B25041_001E&#39; : &amp;quot;total_bedrooms&amp;quot;,
    &#39;B25017_001E&#39; : &amp;quot;total_rooms&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;geography-considerations&#34;&gt;Geography Considerations&lt;/h3&gt;
&lt;p&gt;The next step is figuring out exactly what level of geography we want. Our data set goes down to the Census block level at its most granular. Unfortunately, the Census API won&amp;rsquo;t let us pull the data for all the Census blocks in a state at once. Census tracts on the other hand can be acquired in one go. If we were to shortcut and use only tract data, this would be a pretty quick API call build:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;primary_geo = &amp;quot;tract:*&amp;quot;
secondary_geo = &amp;quot;state:37&amp;quot;
query = base_URL + &amp;quot;?get=&amp;quot; + &amp;quot;,&amp;quot;.join(data_dictionary.keys()) + f&amp;quot;&amp;amp;for={primary_geo}&amp;amp;in={secondary_geo}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But let&amp;rsquo;s try and do it for the Census blocks instead. This will require us to build a sequence of API calls that loops over a larger geographic area, say the different counties in the state, and pull in the respective census block data for that geographic unit. While the FIPS codes for the state counties are sorted alphabetically, they are not contiguous. A full listing of North Carolina county FIPS codes is availalbe 
&lt;a href=&#34;https://www.lib.ncsu.edu/gis/countyfips&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;from NCSU here&lt;/a&gt;. It appears to be that the county FIPS codes are three digits long, starting at &lt;code&gt;001&lt;/code&gt; and go up to &lt;code&gt;199&lt;/code&gt; in increments of 2, meaning only odd numbers are in the county set. So it looks like we will be using &lt;code&gt;range(1,200,2)&lt;/code&gt; with zero-padding to create the list of county FIPS codes. So we could use a loop similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vars_requested = &amp;quot;,&amp;quot;.join(data_dictionary.keys())

for i in range(1,200,2):
    geo_request = f&amp;quot;for=block%20group:*&amp;amp;in=state:37%20county:{i:03}&amp;quot;
    query = base_URL + f&amp;quot;?get={vars_requested}&amp;amp;{geo_request}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While practicing to write the appropriate API call, you may find it useful to give it frequent, quick tests using curl. If you are using Jupyter or IPython, you can use &lt;code&gt;!curl &amp;quot;{query}&amp;quot;&lt;/code&gt; to test your API query. Don&amp;rsquo;t forget the quotation marks, since the ampersand has special meaning in the shell. It may be helpful to test the output of your call at the county or city level with that reported on the 
&lt;a href=&#34;https://www.census.gov/quickfacts/fact/table/US/PST045219&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Census Quickfacts page&lt;/a&gt;, if your variable is listed there. This can help make sure you are pulling the data you actually want.&lt;/p&gt;
&lt;p&gt;Now that we have figured out the loop necessary for creation of the API calls, we can put everything together and create a list of Pandas DataFrames which we then concatenate to create our master list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import requests

# create the base-URL
host_name = &amp;quot;https://api.census.gov/data&amp;quot;
year = &amp;quot;2018&amp;quot;
dataset_name = &amp;quot;acs/acs5&amp;quot;
base_URL = f&amp;quot;{host_name}/{year}/{dataset_name}&amp;quot;

# build the api calls as a list
query_vars = base_URL + &amp;quot;?get=&amp;quot; + &amp;quot;,&amp;quot;.join(list(data_dictionary.keys()) + [&amp;quot;NAME&amp;quot;,&amp;quot;GEO_ID&amp;quot;])
api_calls = [query_vars + f&amp;quot;&amp;amp;for=block%20group:*&amp;amp;in=state:37%20county:{i:03}&amp;quot; for i in range(1,200,2) ]

# running the API calls will take a moment
rjson_list = [requests.get(call).json() for call in api_calls]

# create the data frame by concatenation
df_list = [pd.DataFrame(data[1:], columns=data[0]) for data in rjson_list]
df = pd.concat(df_list, ignore_index=True)

# save the raw output to disk
df.to_csv(&amp;quot;raw_census.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we have the data set! We do still have to address the issue of our values all being imported as strings as mentioned in my 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;Census API post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;acquiring-location-data&#34;&gt;Acquiring Location Data&lt;/h2&gt;
&lt;p&gt;As mentioned above, we are still missing information regarding the latitude and longitude of the different block groups. The Census Bureau makes a lot of geographically coded data available on its 
&lt;a href=&#34;https://tigerweb.geo.census.gov/tigerwebmain/TIGERweb_main.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TIGERweb&lt;/a&gt; page. You can interact with it both using a REST API and its web-interface. A page with map information exists 
&lt;a href=&#34;https://tigerweb.geo.census.gov/arcgis/rest/services/Generalized_ACS2018/Tracts_Blocks/MapServer/4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Dealing with shapefiles and the TIGERweb API can get a little complicated. Luckily, I know someone with expertise in GIS and shapefiles so we will be using a CSV file of the geographic data we need courtesy of 
&lt;a href=&#34;https://www.linkedin.com/in/summer-faircloth-652797137&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Summer Faircloth&lt;/a&gt;, a GIS intern at the North Carolina Department of Transportation. She downloaded the TIGER/Line Shapefiles for the 20189 ACS 
&lt;a href=&#34;https://catalog.data.gov/dataset/tiger-line-shapefile-2018-state-north-carolina-current-block-group-state-based&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Block Groups&lt;/a&gt; and 
&lt;a href=&#34;https://catalog.data.gov/dataset/tiger-line-shapefile-2018-state-north-carolina-current-census-tract-state-based&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Census Tracts&lt;/a&gt; and joined the data sets in ArcMap, from where she exported our CSV file, which is now &lt;a href=&#34;https://dmsenter89.github.io/files/BlockGroup_Tract2018.csv&#34; target=&#34;_blank&#34;&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t need all of the columns in the CSV file, so we will limit the import to the parts we need with the &lt;code&gt;usecols&lt;/code&gt; keyword.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&amp;quot;raw_census.csv&amp;quot;, dtype={})
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;shapedata = pd.read_csv(&amp;quot;BlockGroup_Tract2018.csv&amp;quot;, 
                        dtype={&amp;quot;GEOID&amp;quot;: str},
                        usecols=[&#39;GEOID&#39;,&#39;NAMELSAD&#39;,&#39;INTPTLAT&#39;,&#39;INTPTLON&#39;,&#39;NAMELSAD_1&#39;] )

shapedata = shapedata.rename(columns={&#39;INTPTLAT&#39; : &#39;latitude&#39;, &#39;INTPTLON&#39; : &#39;longitude&#39; })
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-merge-with-geoid-matching&#34;&gt;Data Merge with GEOID Matching&lt;/h2&gt;
&lt;p&gt;At this stage we have two data frames - the first consists of all the Census information sans the geographic coordinates of the block groups, and a second data set containing the block groups&#39; location. Both data sets contain a GEOID column that can be used for merging. The GEOID returned by the Census API includes additional information to the regular FIPS code based GEOID used in the TIGERweb system. For example, &amp;ldquo;1500000US370010204005&amp;rdquo; in the census data set is actually GEOID &amp;ldquo;370010204005&amp;rdquo; for purposes of the TIGERweb data set. We&amp;rsquo;ll use a string split to make our GEO_ID variable from the Census API compatible with the FIPS code based GEOID from the TIGERweb service.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;GEO_ID&amp;quot;] = df[&amp;quot;GEO_ID&amp;quot;].str.split(&#39;US&#39;).str[1]

df = df.merge(shapedata, left_on=&#39;GEO_ID&#39;, right_on=&amp;quot;GEOID&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-cleaning&#34;&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;Now that our data set has been assembled, we can work on cleaning up the merged data set. We have the following tasks left:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;convert column data types to numeric&lt;/li&gt;
&lt;li&gt;drop unnecessary columns&lt;/li&gt;
&lt;li&gt;rename columns&lt;/li&gt;
&lt;li&gt;handle missing values&lt;/li&gt;
&lt;li&gt;calculate median age of homes&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for col in data_dictionary.keys():
    if col not in [&amp;quot;NAME&amp;quot;, &amp;quot;GEO_ID&amp;quot;]:
        df[col] = pd.to_numeric(df[col])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To indicate missing values, the Census API returns a value of &amp;ldquo;-666666666&amp;rdquo; in numeric columns. As all of our variables - except for longitude - ought to be positive, we can use the &lt;code&gt;mask&lt;/code&gt; function to convert all negative values to missing. We&amp;rsquo;ll start by filtering out the string columns that are no longer necessary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# filter down to our numerical columns
keeps = list(data_dictionary.keys()) +[&amp;quot;latitude&amp;quot;, &amp;quot;longitude&amp;quot;]
df = df.filter(items=keeps)

# replace vals &amp;lt; 0 with missing
k = df.loc[:, df.columns != &#39;longitude&#39;]
k = k.mask(k &amp;lt; 0)
df.loc[:, df.columns != &#39;longitude&#39;] = k
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the missing values have been handled, we can go ahead and calculate our median home age.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.rename(columns=data_dictionary, inplace=True)
df[&amp;quot;housing_median_age&amp;quot;] = 2018 - df[&amp;quot;median_year_structure_built&amp;quot;]
df.drop(columns=&amp;quot;median_year_structure_built&amp;quot;, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we&amp;rsquo;re done! We will save our output data set to disk for future analysis in a different post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.to_csv(&amp;quot;NC_Housing_Prices_2018.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
