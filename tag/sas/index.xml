<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sas | Michael&#39;s Site</title>
    <link>https://dmsenter89.github.io/tag/sas/</link>
      <atom:link href="https://dmsenter89.github.io/tag/sas/index.xml" rel="self" type="application/rss+xml" />
    <description>sas</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 18 Dec 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dmsenter89.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>sas</title>
      <link>https://dmsenter89.github.io/tag/sas/</link>
    </image>
    
    <item>
      <title>Lotteries and Pascal&#39;s Mugging</title>
      <link>https://dmsenter89.github.io/post/24/12-lotteries-and-pascals-mugging/</link>
      <pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/24/12-lotteries-and-pascals-mugging/</guid>
      <description>&lt;p&gt;Most have heard of 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Pascal%27s_wager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pascal&amp;rsquo;s wager&lt;/a&gt;, but have you heard of the thought experiment known as 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Pascal%27s_mugging&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pascal&amp;rsquo;s mugging&lt;/a&gt;? The mugging attempts to reframe the essence of the wager argument using only finite values, thereby getting around some standard objections to the wager argument.&lt;/p&gt;
&lt;h2 id=&#34;pascals-mugging&#34;&gt;Pascal&amp;rsquo;s Mugging&lt;/h2&gt;
&lt;p&gt;It appears the term was coined in a 
&lt;a href=&#34;https://web.archive.org/web/20241213091126/https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt; by Eliezer Yudkowsky and framed in terms of potential risks posed by AI tasked with solving a problem. Nick Bostrom retells the mugging as 
&lt;a href=&#34;https://doi.org/10.1093/analys/anp062&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a conversation&lt;/a&gt; between Pascal and a hypothetical extra-dimensional mugger. The second part of &amp;ldquo;deal&amp;rdquo; in the paper is a bit extreme, but the initial part &amp;ndash; offering a fixed cost of money &lt;em&gt;now&lt;/em&gt; for a low probability payoff &lt;em&gt;tomorrow&lt;/em&gt; &amp;ndash; reminded me a bit of one of my most frequently discussed posts from 2022 on 
&lt;a href=&#34;../../22-09-lottery/&#34;&gt;whether it makes sense to play lottery&lt;/a&gt;. To be fair, that was two posts rolled into one &amp;ndash; the first part is about how point estimates can be misleading, particularly for skewed distributions, and the second consisted of a mini-benchmark using a simple example calculation to make that point.&lt;/p&gt;
&lt;p&gt;The beginning of Bostrom&amp;rsquo;s version goes something like this: a mugger approaches Pascal and asks for his wallet. Unfortunately, the mugger forgot his weapon so Pascal is disinclined to acquiesce to his request. To still get the wallet, the mugger offers Pascal a deal: give the mugger the wallet anyways, valued at $x$ USD, and the next day the mugger will return and pay Pascal $N x$ USD in return. As the story progresses, $N$ gets larger. We obviously don&amp;rsquo;t just believe the mugger, so there is some (small) probability $p$ that the mugger will return with the promised reward. The idea behind the experiment is that if $N$ grows sufficiently large then for any non-zero $p$ the expected value of paying the mugger becomes positive. It then veers off talking about other utility issues to make the payout better, but this early part of the conversation is essentially equivalent to a lottery game. Pay for the ticket now in the hopes that come game night the right numbers show up and you&amp;rsquo;re rich.&lt;/p&gt;
&lt;p&gt;As stated, it would appear that it is reasonable for Pascal to pay the mugger and &amp;ndash; for a sufficiently large jackpot &amp;ndash; to play the lottery, even though intuitively it strikes us as the &amp;ldquo;wrong answer&amp;rdquo; given the low probability of winning. This got me thinking: can we steelman the case for playing the lottery by ignoring the magnitude of the win?&lt;/p&gt;
&lt;h2 id=&#34;lets-crunch-some-numbers&#34;&gt;Let&amp;rsquo;s Crunch Some Numbers&lt;/h2&gt;
&lt;p&gt;If we ignore the question of the exact magnitude of the lottery win, we can divide the event space into three possibilities &amp;ndash; we are either worse off (cost of buying ticket), win back the ticket cost only, or win more money than the ticket cost so that we have a net gain from playing. Working off of the published odds again as a shortcut, we wind up with the following probabilities:&lt;/p&gt;
&lt;p&gt;$$
\begin{cases} Pr(\text{loss}) = 24/25 \\&lt;br&gt;
Pr(\text{even}) = 1/38 \\&lt;br&gt;
Pr(\text{gain}) = 13/950
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll postulate that people don&amp;rsquo;t really care about just breaking even on the ticket, so from a decision point of view it probably makes sense to reduce this to a binomial problem: I either win or loose on each ticket, where loosing includes the case of breaking even. People usually buy a handful of tickets, call it $n$. So now I can write a random variable representing my number of winning tickets as&lt;/p&gt;
&lt;p&gt;$$
X \sim \mathrm{Bin}\left(n, \frac{13}{950} \right).
$$&lt;/p&gt;
&lt;p&gt;We already showed that within any reasonable lifetime, there is a vanishingly small chance you&amp;rsquo;ll become rich from playing the lottery. To steelman, we&amp;rsquo;ll say that the utility of playing the lottery comes not from the money won but from the fun of playing.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll start by asserting that winning is more fun than loosing. So how often can we loose and still have fun? We definitely don&amp;rsquo;t want to &lt;em&gt;always&lt;/em&gt; loose, so $x&amp;gt;0$ is required. Winning more often than we loose is unlikely by design of the lottery so that can be our upper bound. Our realistic expectation should then be something like $0 &amp;lt; x \leq \left \lfloor{n/2}\right \rfloor$. If $x$ is in this window, I&amp;rsquo;ll call it a &amp;ldquo;Good Game&amp;rdquo; of lottery.&lt;/p&gt;
&lt;p&gt;Realistically, you&amp;rsquo;re not going to buy 100+ lottery tickets. Let&amp;rsquo;s say an average person likely wouldn&amp;rsquo;t buy more than 20 tickets, which still feels like plenty. So how likely is it you&amp;rsquo;ll have a good game, given as a function of the number of lottery tickets purchased? Let&amp;rsquo;s do a quick DATA step and find out.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data lottery;
  p=13/950;

  do N=2 to 20;
    gg=cdf(&#39;Binomial&#39;, floor(N/2), p, N) - cdf(&#39;Binomial&#39;, 0, p, N);
    output;
  end;
; run;

proc sgplot data=lottery;
  scatter x=N y=gg;
  xaxis label=&#39;Number of Tickets&#39;;
  yaxis label=&#39;Probability of a Good Game&#39;;
run;
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/24/12-lotteries-and-pascals-mugging/gg_hu8a67ac3c7eccf0e3fa506f4b6959062b_12022_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/24/12-lotteries-and-pascals-mugging/gg_hu8a67ac3c7eccf0e3fa506f4b6959062b_12022_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;So in the best case scenario, where we buy 20 tickets, we only have a chance of approximately 0.24 of having a good time. In other words, even with the generous assumption you&amp;rsquo;d be happy to loose 19 games if you win 1 you&amp;rsquo;d still be disappointed most of the time. Given my experience, I&amp;rsquo;d say many people will probably purchase fewer than 20 tickets. Perhaps 2 to 6. In such cases, you&amp;rsquo;re still bound to be disappointed. I&amp;rsquo;d say even with this steelmanning, it doesn&amp;rsquo;t make sense. Instead of buying a lottery ticket, perhaps buy a coffee and a donut for similar cost but with a guaranteed happiness payoff.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From p-Values to Bayes Factors</title>
      <link>https://dmsenter89.github.io/post/24/11-from-p-values-to-bayes-factor/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/24/11-from-p-values-to-bayes-factor/</guid>
      <description>&lt;p&gt;Most medical papers featuring statistical analysis still utilize a hypothesis
testing framework. Data is collected, an analysis is run, a &lt;em&gt;p&lt;/em&gt;-value reported,
and &amp;ndash; if it is found to be below the magic threshold of 0.05 &amp;ndash; the finding is
declared &amp;ldquo;(statistically) significant.&amp;rdquo; The authors then suggest, with varying
degrees of explicitness, that their results support their preferred hypothesis,
or &amp;ndash; in some less modest cases &amp;ndash; may go as far as claiming to have found
&amp;ldquo;proof&amp;rdquo; their preferred hypothesis. Criticism of this methodology is old, and
will not be rehashed here. Suffice it to say that the &lt;em&gt;p&lt;/em&gt;-value is typically
constructed conditional on a strawman &amp;ldquo;null hypothesis&amp;rdquo; being true, and as such
doesn&amp;rsquo;t provide direct evidence concerning any specific alternative hypothesis
the researchers are actually interested in. What we&amp;rsquo;re left with is a general
desire to say something along the lines of &amp;ldquo;Based on the data I have collected,
I now have more/less reason to believe that my preferred hypothesis is correct.&amp;rdquo;
&lt;em&gt;P&lt;/em&gt;-values aside, there exists a measure to express this idea: it is often
called the Bayes factor.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-bayes-factor-and-its-bound&#34;&gt;The Bayes Factor and Its Bound&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with the basic idea. We can frame the problem of how likely a
particular model $M$ is given data $D$ that we have collected by using Bayes
theorem:&lt;/p&gt;
&lt;p&gt;$$\Pr(M|D) = \frac{\Pr(D|M)\Pr(M)}{\Pr(D)}$$&lt;/p&gt;
&lt;p&gt;Suppose you have two competing models as explanations for your data,
$M_{1}$ and $M_{2}$. If you want to know if the data favors one model
over another, you could simply take that ratio:&lt;/p&gt;
&lt;p&gt;$$\frac{\Pr(M_{2}|D)}{\Pr(M_{1}|D)} = \underset{\text{ Bayes Factor}}{\underbrace{\frac{\Pr(D|M_{2})}{\Pr(D|M_{1})}}} \times \underset{\text{Prior Odds}}{\underbrace{\frac{\Pr(M_{2})}{\Pr(M_{1})}}}$$&lt;/p&gt;
&lt;p&gt;Which model is in the numerator or denominator can be chosen by convenience. As
written above, a larger value of this ratio favors $M_2$ over $M_1$, while small
values of the ratio favor $M_1$ over $M_2$.&lt;/p&gt;
&lt;p&gt;Substitute in your traditional notation for a null-hypothesis $H_{0}$
and alternative hypothesis $H_{1}$ and we can compare two different
Bayes factors, $\text{BF}_{10}$ which compares the alternative
hypothesis to the null, or $\text{BF}_{01}$ which compares the null to
the the alternative. For illustrative purposes, consider the case of&lt;/p&gt;
&lt;p&gt;$$\mathrm{BF}_{10} = \frac{1}{\mathrm{BF}_{01}} = 10$$&lt;/p&gt;
&lt;p&gt;We can interpret this to say that the data favor the alternative
hypothesis 10 to 1 compared to the null. In other words, bigger values
of $ \text{ BF }_{10} $ correspond to greater evidence in favor of $H_{1}$
over $H_{0}$, whereas smaller values of $\text{BF}_{01}$ favor $H_{1}$
over $H_{0}$.&lt;/p&gt;
&lt;p&gt;In general, constructing a Bayes factor requires modeling. What we &lt;em&gt;can&lt;/em&gt;
do without modeling, however, is provide reasonable bounds for the Bayes
factor - a minimum bound for $\text{BF}_{01}$ or &amp;ndash; equivalently &amp;ndash; a
maximum bound for $\text{BF}_{10}$. Benjamin and Berger (2019) recommend
a particularly simple upper bound for the Bayes factor that can be shown
to hold in a wide variety of situations:&lt;/p&gt;
&lt;p&gt;$$\text{ BF}_{10} \leq \text{ BFB } = \frac{1}{- ep\log(p)}$$&lt;/p&gt;
&lt;p&gt;This approximation is valid for $p &amp;lt; \frac{1}{e} \approx 0.367$.&lt;/p&gt;
&lt;h2 id=&#34;adding-bayes-factors-to-sas-output&#34;&gt;Adding Bayes Factors to SAS Output&lt;/h2&gt;
&lt;p&gt;There is an easy way to start adding such Bayes Factor bounds to your
existing SAS workflow. Did you know that you can convert any ODS output
table to a SAS data set? That way you can access any reported value for
later analysis. For our purposes, this means we can access any test statistic
or &lt;em&gt;p&lt;/em&gt;-value reported by SAS and use them to calculate the appropriate Bayes
factor bounds. I&amp;rsquo;ll show two simple examples.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say I want to employ a bound for the output of PROC TTEST. I can look up
the relevant table name in the SAS/STAT User&amp;rsquo;s Guide in the &amp;ldquo;ODS Table Names&amp;rdquo;
section under the PROCs &amp;ldquo;Details&amp;rdquo; &amp;ndash; in this case, I want to use the table named
&lt;code&gt;TTests&lt;/code&gt;. I can then save this table to a SAS data set using &lt;code&gt;ods output&lt;/code&gt;. To
save typing, we&amp;rsquo;ll use the &lt;code&gt;filename&lt;/code&gt; and &lt;code&gt;include&lt;/code&gt; statements to utilize the
available code for the Getting Started example in TTest:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;filename tgs url
  &#39;https://raw.githubusercontent.com/sassoftware/doc-supplement-statug/refs/heads/main/Examples/r-z/ttegs1.sas&#39;;
ods output TTests=res; /* this line exports the TTests table to &amp;quot;res&amp;quot; */
%include tgs;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at my new data set, I can see that the &lt;em&gt;p&lt;/em&gt;-value is saved to a variable
named &lt;code&gt;Probt&lt;/code&gt;. I can now use a DATA step to calculate both the BFB and the
reciprocal of it, the minimum Bayes factor:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data bayes;
  set res;
  BFB = 1/(-CONSTANT(&#39;E&#39;)*Probt*Log(Probt));
  BFmin = 1/BFB;
run;
proc print; run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I&amp;rsquo;m omitting the &lt;code&gt;data&lt;/code&gt; keyword from the PROC PRINT call to keep things concise.
This way, it automatically uses the last data set in use. If you run these two
code snippets, you&amp;rsquo;ll get the following table:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-bayes-factor-bounds-from-our-proc-ttest-example&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/24/11-from-p-values-to-bayes-factor/ttest_hu495a71d51f4100df1b1cbf22bbde0982_13685_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;The Bayes factor bounds from our PROC TTest example.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/24/11-from-p-values-to-bayes-factor/ttest_hu495a71d51f4100df1b1cbf22bbde0982_13685_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;737&#34; height=&#34;103&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Bayes factor bounds from our PROC TTest example.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This suggests that for our particular example, the most favorable
interpretation would favor the alternative hypothesis over the null by a
rate of about 5.4-to-1.&lt;/p&gt;
&lt;p&gt;Another common source for &lt;em&gt;p&lt;/em&gt;-values is regression output. Each parameter estimate
is accompanied by a &lt;em&gt;p&lt;/em&gt;-value. We can use the same procedure as above to look
up the relevant ODS table name and use &lt;code&gt;ODS OUTPUT&lt;/code&gt; to save that table for later
use:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;ods output ParameterEstimates=ParmEst;
proc reg data=sashelp.baseball;
   id name team league;
   model logSalary = nhits nbb yrmajor crhits;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This table contains a little more detail and a pretty large BFB, so I decided to
specify which variables I want to print and added a &lt;code&gt;FORMAT&lt;/code&gt; to the &lt;code&gt;PRINT&lt;/code&gt;
call.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data bayes;
  set ParmEst;
  BFB = 1/(-CONSTANT(&#39;E&#39;)*Probt*Log(Probt));
  BFmin = 1/BFB;
run;

proc print noobs;
  var Variable Label Estimate Probt BF:;
  format BF: COMMA14.2;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing that&amp;rsquo;s neat to notice here is that the &lt;em&gt;p&lt;/em&gt;-value is printed by SAS
using a special format. Since user&amp;rsquo;s are normally not interested in the &lt;em&gt;exact&lt;/em&gt;
value when it is less than $10^{-4}$ ODS just prints &amp;ldquo;&amp;lt;.0001.&amp;rdquo; This doesn&amp;rsquo;t mean
SAS doesn&amp;rsquo;t calculate the exact &lt;em&gt;p&lt;/em&gt;-value as we can see from the data set
produced with &lt;code&gt;ODS OUTPUT&lt;/code&gt;. It stores the actual numeric value, so the BFB
computation can proceed without issues. This is what it looks like:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-bayes-factor-bounds-from-our-proc-reg-example&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/24/11-from-p-values-to-bayes-factor/reg_hu8bfa6fceface903fe712ffe709e7ead3_58561_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;The Bayes factor bounds from our PROC REG example.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/24/11-from-p-values-to-bayes-factor/reg_hu8bfa6fceface903fe712ffe709e7ead3_58561_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1291&#34; height=&#34;354&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Bayes factor bounds from our PROC REG example.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;You can try these code snippets out yourself using 
&lt;a href=&#34;https://welcome.oda.sas.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS OnDemand for Academics&lt;/a&gt;
or 
&lt;a href=&#34;https://www.sas.com/en_us/software/viya-for-learners.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Viya for Learners&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The bound used in this post is one recommendation by Benjamin and Berger (2019)
to improve scientific result reporting during this time in which we&amp;rsquo;re slowly
trying to move away from &lt;em&gt;p&lt;/em&gt;-values. To learn more about alternative bounds and
the conditions in which they hold, I would recommend the very readable overview
of the subject of by Held and Ott (2018).&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Benjamin, D. J., and Berger, J. O. (2019), &amp;ldquo;Three Recommendations for Improving
the Use of p-Values,&amp;rdquo; &lt;em&gt;The American Statistician&lt;/em&gt;, ASA Website, 73, 186–191.
&lt;a href=&#34;https://doi.org/10.1080/00031305.2018.1543135&#34;&gt;https://doi.org/10.1080/00031305.2018.1543135&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Held, L., and Ott, M. (2018), &amp;ldquo;On p-Values and Bayes Factors,&amp;rdquo; &lt;em&gt;Annual Review of
Statistics and Its Application&lt;/em&gt;, Annual Reviews, 5, 393–419.
&lt;a href=&#34;https://doi.org/https://doi.org/10.1146/annurev-statistics-031017-100307&#34;&gt;https://doi.org/https://doi.org/10.1146/annurev-statistics-031017-100307&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;This doesn&amp;rsquo;t solve all problems with hypothesis testing. In
particular, see section 7.4 in BDA3 for limitations. You may also
enjoy the critique offered at

&lt;a href=&#34;https://datacolada.org/78&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DataColada&lt;/a&gt;. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>New MI Feature: Flux Statistics</title>
      <link>https://dmsenter89.github.io/post/24/04-new-mi-feature-flux/</link>
      <pubDate>Thu, 18 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/24/04-new-mi-feature-flux/</guid>
      <description>&lt;p&gt;The 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_050/pgmsaswn/n1l6ng10yj6s1an1v0rt9nj79ktc.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Viya 2024.04 release&lt;/a&gt;
includes a brand new MI feature: new missing data statistics. An important
choice when building an imputation model is the selection of variables to be
included. One method to help in the variable selection process is the usage of
summary statistics such as influx and outflux, as proposed by 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/missing-data-pattern.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;van
Buuren&lt;/a&gt;. In his
words: &amp;ldquo;Influx and outflux are summaries of the missing data pattern intended to
aid in the construction of imputation models. Keeping everything else constant,
variables with high influx and outflux are preferred. Realize that outflux
indicates the potential (and not actual) contribution to impute other variables&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The MI statement now supports the new FLUX option. When specified, MI produces a table
including the influx, outflux, average inbound and outbound, and FICO statistics
along with a column indicating the percent of cases for which the particular
variable has been observed. When ODS graphics are turned on, MI additionally
produces a scatter plot of the variables&#39; influx and outflux. For details,
see the new section on 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_050/statug/statug_mi_details57.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Missing Data Statistics&lt;/a&gt; in the MI chapter of the SAS/STAT User&amp;rsquo;s Guide.&lt;/p&gt;
&lt;p&gt;One thing that&amp;rsquo;s cool about this new feature for all users, not just those interested
in multiple imputation, is the fact that this new feature allows you to get a
complete overview of the percent of observed/missing cases for &lt;em&gt;all&lt;/em&gt; variables &amp;mdash;
both character and numeric! Previously, you either needed to use

&lt;a href=&#34;https://blogs.sas.com/content/iml/2011/09/19/count-the-number-of-missing-values-for-each-variable.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;separately procedures&lt;/a&gt; for character and numeric variables, or 
&lt;a href=&#34;https://www.sas.com/content/dam/SAS/en_ca/User%20Group%20Presentations/TASS/Zdeb-MissingData.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;expend some work&lt;/a&gt; to get a macro written that creates a table
of both types of variables for you.&lt;/p&gt;
&lt;p&gt;With this new feature, you can simply use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;/* optional: creates output ds with PctObs and PctMiss vars */
ods output Flux=Flux;

/* sample code using the sashelp.heart data set */
proc mi data=sashelp.heart flux
      nimpute=0
      displaypattern=nomeans;
   class _character_;
   var _all_;
   fcs;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we can include all variables in our data set with &lt;code&gt;var _all_&lt;/code&gt;. If our
data set includes character variables, we need &lt;code&gt;class _character_&lt;/code&gt; to label all
character variables as classification variables. If you are only interested in
a subset of the variables, you can of course specify them here. We use the FCS
statement to accomodate classification variables and we set &lt;code&gt;nimpute=0&lt;/code&gt; since we
don&amp;rsquo;t actually want to create imputations, just view the missing data statistics.
The &lt;code&gt;ods output&lt;/code&gt; statement is completely optional. It creates a data set with
variables PctObs and PctMiss for every variable in the analysis that you could
then further process with PROC SQL or some other method.&lt;/p&gt;
&lt;p&gt;In this example, the table will look as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;flux-table.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;For a full walkthrough of this code, see the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_050/statug/statug_mi_examples19.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;new example&lt;/a&gt;
in the MI chapter of the SAS/STATS User&amp;rsquo;s Guide.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Calling R From SAS</title>
      <link>https://dmsenter89.github.io/post/23-12-calling-r-from-sas/</link>
      <pubDate>Fri, 22 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-12-calling-r-from-sas/</guid>
      <description>&lt;p&gt;The statistics literature is filled with example code and sample data in R. Sometimes I
find myself wanting to work through some provided sample data and compare the output from
R with SAS code. In this post, I&amp;rsquo;ll show how to connect R and SAS so that you can load and
execute R code straight from within SAS.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;In order to use this feature, you will want to have both R and SAS/IML installed on the
same computer. Make sure both SAS and R are in your path. In order to call R code from
SAS, you will need to start SAS with the &lt;code&gt;rlang&lt;/code&gt; option. You can call SAS from the command
line with the &lt;code&gt;-rlang&lt;/code&gt; option or you can add the option in your &amp;ldquo;sasv9.cfg&amp;rdquo; file.&lt;/p&gt;
&lt;p&gt;Once SAS is started, you can verify that the setup worked by running&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc options option=rlang;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The log will list &lt;code&gt;RLANG&lt;/code&gt; if the option was specified. If you forgot to add the option
prior to startup, you&amp;rsquo;ll see &lt;code&gt;NORLANG&lt;/code&gt; in the log instead.&lt;/p&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;R code can be called from within IML via a submit statement. The basic structure
is this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc iml;
  submit / R;
    /* R code her */
  endsubmit;
quit;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this we can run R code from within SAS. But the real power comes from our
ability to move data between R and SAS. The following functions are available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ExportDatasetToR(&amp;quot;libname.dsname&amp;quot;, RDataFrame);&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ExportMatrixToR(IMLMatrix, RMatrix);&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ImportDataSetFromR(r-expr, &amp;quot;libname.dsname&amp;quot;)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ImportMatrixFromR(r-expr, IMLMatrix)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Parameters can be passed to R as well, similar to how parameters can be passed
from IML to SAS PROCs.&lt;/p&gt;
&lt;p&gt;For more details, see the SAS/IML manual.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some Basic SQL Joins</title>
      <link>https://dmsenter89.github.io/post/23-09-basic-sql-joins/</link>
      <pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-09-basic-sql-joins/</guid>
      <description>&lt;p&gt;A non-technical friend recently asked me for help with a merge problem. They had two separate data pulls of electronic medical records based on specific study parameters. The set of people in the database who fit the study parameters changed in between the data pulls, for example by having people age into our out of a study, or by having new diagnoses added to their records that cause them to either be newly included or excluded. Let&amp;rsquo;s call the older data set A and the newer data set B. The goal was to get all those entries from B that don&amp;rsquo;t also show up in A. The data sets were pulled by a staff data scientist at that company who, despite their title, said they couldn&amp;rsquo;t figure out how to remove those entries from B that were already in A. Barring any special circumstances, this is a fairly standard problem so let&amp;rsquo;s look at a couple of tools we could use to solve it.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with some made-up sample data:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-minimal-sample-data-for-demonstration-purposes&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/sample-data_huec327b38ea9381c4b4e186ea675e12fc_20885_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Minimal sample data for demonstration purposes.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/sample-data_huec327b38ea9381c4b4e186ea675e12fc_20885_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;474&#34; height=&#34;322&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Minimal sample data for demonstration purposes.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- ```shell
$ bat *.csv
───────┬────────────────────────────────────────────────────────────────────
       │ File: A.csv
───────┼────────────────────────────────────────────────────────────────────
   1   │ MRN,Weight,Chol_Status
   2   │ 23356,140,
   3   │ 74592,,Desirable
   4   │ 79602,139,High
───────┴────────────────────────────────────────────────────────────────────
───────┬────────────────────────────────────────────────────────────────────
       │ File: B.csv
───────┼────────────────────────────────────────────────────────────────────
   1   │ MRN,Weight,Chol_Status
   2   │ 64836,129,High
   3   │ 79602,139,High
   4   │ 2466,127,Borderline
───────┴────────────────────────────────────────────────────────────────────
``` --&gt;
&lt;p&gt;The MRN here stands for &amp;ldquo;medical record number,&amp;rdquo; a common unique identifier present in clinical data sets. Each of our data sets has three rows, but only one row is shared between both - that associated with MRN 79602. We could theoretically merge on multiple columns or coalesce data if we think some missing fields might have been updated in the meantime, but for purposes of this example we&amp;rsquo;ll keep it simple and just merge on the MRN.&lt;/p&gt;
&lt;h2 id=&#34;sql-merge-types&#34;&gt;SQL Merge Types&lt;/h2&gt;
&lt;p&gt;There are four basic types of merge: left join, right join, outer join, and inner join. There&amp;rsquo;s also the cross join but that one shows up less frequently in my experience. A picture speaks a thousand words, so here&amp;rsquo;s a Venn diagram illustrating the idea behind these joins.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-standard-sql-joins&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/sql-joins_hu519d137be5f72d83cc0fb24e7060e243_236726_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Standard SQL Joins.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/sql-joins_hu519d137be5f72d83cc0fb24e7060e243_236726_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2593&#34; height=&#34;1814&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Standard SQL Joins.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In our case, we actually want left/right &amp;ldquo;inner&amp;rdquo; or &amp;ldquo;exclusive&amp;rdquo; joins, like this:&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/exclusive-joins60_hu03ae2ec37b24cdee5a828b5b640777a8_45542_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/exclusive-joins60_hu03ae2ec37b24cdee5a828b5b640777a8_45542_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;377&#34; height=&#34;514&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;implementations&#34;&gt;Implementations&lt;/h2&gt;
&lt;p&gt;I figured I would go over three basic tools: SAS, SQL, and Pandas.&lt;/p&gt;
&lt;h3 id=&#34;only-in-a&#34;&gt;Only in A&lt;/h3&gt;
&lt;p&gt;For starters, we want all entries in $A$ that are not also in $B$. In set notation that is the set denoted $A-B$ (sometimes $A\backslash B$).
Merges like this is what SQL excels at, so let&amp;rsquo;s see the SQL statment first:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table res1 as
  select A.* from
    A left join B 
    on A.MRN=B.MRN
    where B.MRN is NULL;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should run in any typical SQL implementation, including PROC SQL in SAS and SQLite3. We expect the following table as output:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;MRN&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;th&gt;Chol_Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;23356&lt;/td&gt;
&lt;td&gt;140&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;74592&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Desirable&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To do a left outer join instead, we would just omit the &lt;code&gt;where&lt;/code&gt; clause. We could do the same with a data step merge statement, but unlike SQL this would assume our input data sets are sorted by the merge key:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data res1;
  merge A (IN = X) B (IN=Y);
  by MRN;
  If X and not Y;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pandas&#39; merge statement allows for the creation of an indicator variable, similar to the &lt;code&gt;in&lt;/code&gt; keyword used in the SAS data step merge. That indicator will tell us if the particular row is present in both the left and the right tables (value &lt;code&gt;both&lt;/code&gt;), or just in one of them (values &lt;code&gt;left_only&lt;/code&gt; and &lt;code&gt;right_only&lt;/code&gt;). We can then query on that indicator variable to subset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;res1 = (pd.merge(A, B, how=&#39;outer&#39;, indicator=True)
            .query(&#39;_merge==&amp;quot;left_only&amp;quot;&#39;)
            .drop(&#39;_merge&#39;, axis=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;only-in-b&#34;&gt;Only in B&lt;/h3&gt;
&lt;p&gt;Same idea, but reversed: $B-A$. The implementation is identical except that we are using a right join instead:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table res2 as
  select B.* from
  A right join B
  on A.MRN=B.MRN
  where A.MRN is NULL;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Expected output:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;MRN&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;th&gt;Chol_Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;64836&lt;/td&gt;
&lt;td&gt;129&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2466&lt;/td&gt;
&lt;td&gt;127&lt;/td&gt;
&lt;td&gt;Borderline&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is interesting to note that SQLite, at least as of 3.37.2, still doesn&amp;rsquo;t support right joins, so if you&amp;rsquo;re using that you&amp;rsquo;ll just want to use the left join method above but switch the &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; around. The data step implementation is also straight forward:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data res2;
  merge A (IN = X) B (IN=Y);
  by MRN;
  If Y and not X;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;as is the Pandas version:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;res2 = (pd.merge(A, B, how=&#39;outer&#39;, indicator=True)
            .query(&#39;_merge==&amp;quot;right_only&amp;quot;&#39;)
            .drop(&#39;_merge&#39;, axis=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;whats-in-common&#34;&gt;What&amp;rsquo;s in common?&lt;/h3&gt;
&lt;p&gt;Finally, you  might be curious to see which rows both data sets have in common, that is $A \cap B$. That&amp;rsquo;s a simple inner join:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table res3 as
  select A.* from 
  A inner join B
  on A.MRN=B.MRN;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Expected output:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;MRN&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;th&gt;Chol_Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;79602&lt;/td&gt;
&lt;td&gt;139&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In SAS:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data res3;
  merge A (IN = X) B (IN=Y);
  by MRN;
  If X and Y;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and in Pandas:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;res3 = (pd.merge(A, B, how=&#39;outer&#39;, indicator=True)
            .query(&#39;_merge==&amp;quot;both&amp;quot;&#39;)
            .drop(&#39;_merge&#39;, axis=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it. All that&amp;rsquo;s left to do is to save the data in a format your customer or colleagues can work with and we&amp;rsquo;re done.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Univariate Missing Data with PROC MI</title>
      <link>https://dmsenter89.github.io/post/23-08-univariate-mi/</link>
      <pubDate>Sun, 13 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-08-univariate-mi/</guid>
      <description>&lt;p&gt;In Chapter 3 of van Buuren&amp;rsquo;s &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt; a variety of methods for imputing univariate missing data are presented. This post will summarize these techniques and show how to implement them in SAS.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#preliminaries&#34;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#imputing-under-a-normal-linear-model&#34;&gt;Imputing under a Normal Linear Model&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#regression-imputation&#34;&gt;Regression Imputation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#stochastic-regression-imputation&#34;&gt;Stochastic Regression Imputation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#bayesianbootstrap-multiple-imputation&#34;&gt;Bayesian/Bootstrap Multiple Imputation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#what-if-my-data-are-non-normal&#34;&gt;What if my data are non-normal?&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#predictive-mean-matching&#34;&gt;Predictive Mean Matching&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#classification-and-regression-trees&#34;&gt;Classification and Regression Trees&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-propensity-score-method&#34;&gt;The Propensity Score Method&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#categorical-and-count-data&#34;&gt;Categorical and Count Data&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#the-logistic-and-logit-models&#34;&gt;The Logistic and Logit Models&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-discriminant-function-method&#34;&gt;The Discriminant Function Method&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;

&lt;h2 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;Van Buuren demonstrates various techniques using data set 88 from Hand et al (1994). This data set is availabe from R&amp;rsquo;s MASS library as &lt;code&gt;data(&amp;quot;whiteside&amp;quot;)&lt;/code&gt;. The original data set can be downloaded from the 
&lt;a href=&#34;https://www.routledge.com/A-Handbook-of-Small-Data-Sets/Hand-Daly-McConway-Lunn-Ostrowski/p/book/9780367449667&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;publisher&amp;rsquo;s website&lt;/a&gt;. The name of the relevant data file is INSULATE.DAT. If you want to follow along using SAS, you can use 
&lt;a href=&#34;./code/whiteside.sas&#34;&gt;this data step&lt;/a&gt;. It matches the way the data appears in R except that I have added a variable &lt;code&gt;R&lt;/code&gt; indicating the observation that van Buuren deletes for demonstration purposes.&lt;/p&gt;
&lt;p&gt;For purposes of this post, we assume one or more predictors $x$ are completely observed, while some variable of interest $y$ is only partially observed. Methods for dealing with this type of problem are available using the 
&lt;a href=&#34;http://documentation.sas.com/doc/en/statug/15.2/statug_mi_syntax09.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;monotone&lt;/a&gt; keyword in PROC MI. A data set has a monotone missing pattern if it consists of variables $Y_1$, $Y_2$, $\ldots$, $Y_p$ such that if $Y_j$ is missing for one individual, all subsequent variables $Y_k$ for $j &amp;lt; k \leq p$ are also missing. Schematically, the data set will look like this:&lt;/p&gt;
&lt;p&gt;$$R = \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 1 \\ 1 &amp;amp; 1 &amp;amp; 0 \\ 1 &amp;amp; 0 &amp;amp; 0 \end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;where 1 indicates an observed value and 0 a missing value. The monotone statement in SAS can impute missing values by completing the columns in turn using univariate methods. See the 
&lt;a href=&#34;http://documentation.sas.com/doc/en/statug/15.2/statug_mi_details06.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt; for specifics.&lt;/p&gt;
&lt;h2 id=&#34;imputing-under-a-normal-linear-model&#34;&gt;Imputing under a Normal Linear Model&lt;/h2&gt;
&lt;p&gt;For completion, I will mention all of the main linear model methods van Buuren mentions in his text, even though the first two are not implemented in PROC MI.&lt;/p&gt;
&lt;h3 id=&#34;regression-imputation&#34;&gt;Regression Imputation&lt;/h3&gt;
&lt;p&gt;Van Buuren also refers to this as the &amp;ldquo;prediction&amp;rdquo; method. In essence, the complete cases are used to create a linear model. This linear model is then used to fill in the missing values:&lt;/p&gt;
&lt;p&gt;$$ \dot{y} = \hat\beta_0 + X_\text{mis}\,\hat\beta_1$$&lt;/p&gt;
&lt;p&gt;where $\hat\beta_i$ are least squares estimates.&lt;/p&gt;
&lt;p&gt;This method has a variety of drawbacks. For one, it artificially strengthens the relationships between variables as they appear in the linear model by increasing correlations. Variability in the data is reduced. See section 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-simplesolutions.html#sec:regimp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1.3.4&lt;/a&gt; in van Buuren for details.&lt;/p&gt;
&lt;p&gt;The mice package implements this method as &lt;code&gt;norm.predict&lt;/code&gt;. PROC MI does not implement this method; to use this technique in SAS, you could use the regression PROCs or IML.&lt;/p&gt;
&lt;h3 id=&#34;stochastic-regression-imputation&#34;&gt;Stochastic Regression Imputation&lt;/h3&gt;
&lt;p&gt;This method proceeds as above, except that Gaussian noise is added to the imputed value:
$$ \dot{y} = \hat\beta_0 + X_\text{mis}\,\hat\beta_1 + \dot\epsilon$$
where $\dot\epsilon \sim N(0, \hat\sigma^2)$. An advantage of this method over plain regression is that it can preserve correlation between variables.&lt;/p&gt;
&lt;p&gt;The mice package implements this method as &lt;code&gt;norm.nob&lt;/code&gt;. It is not available in PROC MI but can be implemented with IML.&lt;/p&gt;
&lt;h3 id=&#34;bayesianbootstrap-multiple-imputation&#34;&gt;Bayesian/Bootstrap Multiple Imputation&lt;/h3&gt;
&lt;p&gt;Van Buuren also refrers to this as &amp;ldquo;predict + noise + parameters uncertainty.&amp;rdquo; This technique is based on a Bayesian linear regression using draws from the posterior as parameters:
$$\dot y = \dot\beta_0 + X_\text{mis}\, \dot\beta_1 + \dot\epsilon$$
where $\dot\epsilon\sim N(0,\dot\sigma^2)$ and $\dot\beta_i$, $\dot\sigma$ are random draws from the posterior distribution.&lt;/p&gt;
&lt;p&gt;This the default method in PROC MI for continuous data. Both SAS and mice use an algorithm based on Rubin (1987, pp. 166-167). See the 
&lt;a href=&#34;http://documentation.sas.com/doc/en/statug/15.2/statug_mi_details07.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt; and 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:norm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Algorithm 3.1&lt;/a&gt; in van Buuren for details. The mice package implements this method as &lt;code&gt;norm&lt;/code&gt;. Here is an example of how the Bayesian regression can be used in PROC MI:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mi data=whiteside_miss out=regimp nimpute=5;
	var temp gas;
	monotone regression(gas);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The regression keyword may be abbreviated as &lt;code&gt;reg&lt;/code&gt;. A fully worked example is available in the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_examples03.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt;, with the associated code available on 
&lt;a href=&#34;https://github.com/sassoftware/doc-supplement-statug/blob/main/Examples/m-n/miex3.sas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The mice package also implements a variant of this method using bootstrapping instead of a Bayesian model. This method is available as &lt;code&gt;norm.boot&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;what-if-my-data-are-non-normal&#34;&gt;What if my data are non-normal?&lt;/h3&gt;
&lt;p&gt;In case the data are non-normal, one could proceed to a non-regression technique like 
&lt;a href=&#34;#predictive-mean-matching&#34;&gt;predictive mean matching&lt;/a&gt;. Alternatively, one could adjust the regression methods to utizilise a generalized linear model instead. That technique is implemented in the 
&lt;a href=&#34;https://github.com/dsalfran/ImputeRobust&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ImputeRobust&lt;/a&gt; package for R. See section 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-nonnormal.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3.3&lt;/a&gt; in van Buuren for details.&lt;/p&gt;
&lt;h2 id=&#34;predictive-mean-matching&#34;&gt;Predictive Mean Matching&lt;/h2&gt;
&lt;p&gt;Similar to 
&lt;a href=&#34;#bayesianbootstrap-multiple-imputation&#34;&gt;Bayesian regression&lt;/a&gt; above, a predicted value is calculated for each missing observation. Instead of adding noise to this prediction, however, a set of $k$ observations whose predicted values are close to the predicted missing value are sought. The missing value is then replaced by a random draw from this set of candidate donors. In mice, this method is available as &lt;code&gt;pmm&lt;/code&gt;. In PROC MI, you can use the &lt;code&gt;regpredmeanmatch&lt;/code&gt; keyword:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mi data=whiteside_miss out=regimp nimpute=5;
	var temp gas;
	monotone regpredmeanmatch(gas);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The keyword &lt;code&gt;regpredmeanmatch&lt;/code&gt; may be abbreviated as &lt;code&gt;regpmm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The predictive mean matching method is robust to transformations of the target variable. It may be used with both continuous and discrete data and will always generate realistic data in the sense that all generated data has been observed. Since this does not require an explicit model to describe the distribution of missing values, it is more resilient to model misspecification.&lt;/p&gt;
&lt;p&gt;See the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_details08.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt; and 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-pmm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;section 3.4&lt;/a&gt; in van Buuren for details.&lt;/p&gt;
&lt;h2 id=&#34;classification-and-regression-trees&#34;&gt;Classification and Regression Trees&lt;/h2&gt;
&lt;p&gt;An idea borrowed from the machine learning community and implemented in some R packages. In essence, the idea is similar to utiziling linear regression models except that a regression tree is utilized instead. See 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-cart.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;section 3.5&lt;/a&gt; in van Buuren.&lt;/p&gt;
&lt;h2 id=&#34;the-propensity-score-method&#34;&gt;The Propensity Score Method&lt;/h2&gt;
&lt;p&gt;With this method, propensity scores are generated for each observation estimating the probability of it being missing. The observations are then grouped by their propensity scores and an approximate Bayesian bootstrap imputation is carried out for each group. See 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_details18.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;A fully worked example is available in the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_examples02.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt;, with the associated code available on 
&lt;a href=&#34;https://github.com/sassoftware/doc-supplement-statug/blob/main/Examples/m-n/miex2.sas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mi data=Fish1 out=outex2;
   monotone propensity;
   var Length1 Length2 Length3;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This method is not implemented in the mice package.&lt;/p&gt;
&lt;h2 id=&#34;categorical-and-count-data&#34;&gt;Categorical and Count Data&lt;/h2&gt;
&lt;h3 id=&#34;the-logistic-and-logit-models&#34;&gt;The Logistic and Logit Models&lt;/h3&gt;
&lt;p&gt;Logit based regression models can be used both for nominal and ordinal data. The imputed value is generated from a Bayesian logistic regression model. The mice package implements this method as &lt;code&gt;logreg&lt;/code&gt;. PROC MI uses the &lt;code&gt;logistic&lt;/code&gt; keyword. An example of its usage is given in the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_examples04.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt;, with the associated code available on 
&lt;a href=&#34;https://github.com/sassoftware/doc-supplement-statug/blob/main/Examples/m-n/miex4.sas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;. Here&amp;rsquo;s the example code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mi data=Fish2 out=outex4;
   class Species;
   monotone reg(Width/ details)
            logistic(Species = Length Width Length*Width/ details);
   var Length Width Species;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This imputes the width variable using the 
&lt;a href=&#34;#bayesianbootstrap-multiple-imputation&#34;&gt;Bayesian linear regression&lt;/a&gt; while imputing the categorical species variable using the logistig regression method.&lt;/p&gt;
&lt;p&gt;See the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_details13.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt; for details.&lt;/p&gt;
&lt;h3 id=&#34;the-discriminant-function-method&#34;&gt;The Discriminant Function Method&lt;/h3&gt;
&lt;p&gt;This method is the default for categorical data in PROC MI. See the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_details09.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;A fully worked example is available in the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_examples05.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt;, with the associated code available on 
&lt;a href=&#34;https://github.com/sassoftware/doc-supplement-statug/blob/main/Examples/m-n/miex5.sas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;. Here is the MI call:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mi data=Fish2 out=outex5;
   class Species;
   monotone discrim(Species = Length Width/ details);
   var Length Width Species;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mice package does not implement this method.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Hand, D. J., F. Daly, A. D. Lunn, K. J. McConway, and Ostrowski,  E. (1994), &lt;em&gt;A Handbook of Small Data Sets&lt;/em&gt;, London: Chapman &amp;amp; Hall.&lt;/p&gt;
&lt;p&gt;Rubin, D. B. (1987), &lt;em&gt;Multiple Imputation for Nonresponse in Surveys&lt;/em&gt;, New York: John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;p&gt;van Buuren, S. (2018), &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt;, Chapman and Hall/CRC interdisciplinary statistics series, Boca Raton: CRC Press, Taylor and Francis Group. Available at &lt;a href=&#34;https://stefvanbuuren.name/fimd/&#34;&gt;https://stefvanbuuren.name/fimd/&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sampling Regression Lines</title>
      <link>https://dmsenter89.github.io/post/23-05-sample-regression-lines/</link>
      <pubDate>Mon, 08 May 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-05-sample-regression-lines/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://dmsenter89.github.io/post/23-05-simple-regression-with-proc-mcmc/&#34;&gt;Last week&lt;/a&gt; we saw how to generate posterior samples using PROC MCMC for simple linear and logistic regression models. This week, I want to show how to sample regression lines from the data set returned by MCMC by plotting several sample regression linse on top of a scatter plot of the source data.&lt;/p&gt;
&lt;h2 id=&#34;writing-the-macro&#34;&gt;Writing the Macro&lt;/h2&gt;
&lt;p&gt;Since the majority of the steps are identical irrespective of what data set we use, and because we might want to use this iteretively during model building, I decided to write this up as a macro. To some degree this is required since I will be using a macro do-loop, which is only valid when embedded inside of a macro.&lt;/p&gt;
&lt;p&gt;This macro will assume we have fitted a simple linear model of the form&lt;/p&gt;
&lt;p&gt;\begin{aligned}
y_i &amp;amp;\sim \mathrm{Normal}(\mu_i, \sigma) \\&lt;br&gt;
\mu_i &amp;amp;= \beta_0 + \beta_1 x_i
\end{aligned}&lt;/p&gt;
&lt;h3 id=&#34;step-1-get-an-srs-sample&#34;&gt;Step 1: Get an SRS Sample&lt;/h3&gt;
&lt;p&gt;The first step is the simplest - selecting a subset of the posterior samples. This is easily achieved by calling 
&lt;a href=&#34;https://support.sas.com/rnd/app/stat/procedures/surveyselect.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PROC SURVEYSELECT&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc surveyselect data=&amp;amp;posterior. method=srs n=&amp;amp;n.
                  out=SRS;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-2-make-a-macro-list-of-parameters&#34;&gt;Step 2: Make a Macro List of Parameters&lt;/h3&gt;
&lt;p&gt;Next we need to generate a list of intercepts and slopes. I find it easiest to read those in PROC SQL using the &lt;code&gt;into&lt;/code&gt; operation. Additionally, we&amp;rsquo;ll also collect the $x$- and $y$-ranges of our data. This will be used to make sure our plot is centered on the scatter-plot values of our source data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc sql noprint;
  select beta0, beta1 
  into :intercepts separated by &#39; &#39;,
       :slopes  separated by &#39; &#39;
  from SRS;

  select min(x), max(x),
         min(y), max(y)
  into :minx, :maxx,
       :miny, :maxy
  from &amp;amp;ds.;
quit;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-3-macro-loop-to-add-lines-to-a-scatter-plot&#34;&gt;Step 3: Macro-Loop to Add Lines to a Scatter Plot&lt;/h3&gt;
&lt;p&gt;Now all the parts have been assembled and you can call PROC SGPLOT. We use the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/grstatproc/p1lcbd3lhs3t3bn1jk6d8sjt2yqx.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scatter&lt;/a&gt; statement to create a scatter plot of the source data. Then we use a do-loop to iteratively paste different 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/grstatproc/n1h6n82pw2uqo6n10avwmph63r7o.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lineparm&lt;/a&gt; statements corresponding to our different samples into the SGPLOT statement. Lastly, use the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/grstatproc/n0n6uml63c6h8dn16phbd1arm9g9.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;xaxis&lt;/a&gt; and 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/grstatproc/n0n6uml63c6h8dn16phbd1arm9g9.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;yaxis&lt;/a&gt; statements to focus the graph on the scatter plot data, and not forcing the x-intercepts of the different fitted lines into&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc sgplot data=&amp;amp;ds. noautolegend;
  scatter x=x y=y;
  %do i = 1 %to &amp;amp;n.;
    lineparm x=0 y=%scan(&amp;amp;intercepts, &amp;amp;i, &#39; &#39;) 
         slope=%scan(&amp;amp;slopes, &amp;amp;i, &#39; &#39;) / transparency=0.7;
  %end;

  xaxis min=&amp;amp;minx. max=&amp;amp;maxx.;
  yaxis min=&amp;amp;miny. max=&amp;amp;maxy.;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;calling-the-macro&#34;&gt;Calling the Macro&lt;/h2&gt;
&lt;p&gt;And that&amp;rsquo;s it. Assuming we declared the macro as follows&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;%macro sample_regression(ds=, posterior=, n=);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we can now call it.&lt;/p&gt;
&lt;p&gt;As a particular example, let&amp;rsquo;s run PROC MCMC&amp;rsquo;s getting started example 1 straight from 
&lt;a href=&#34;https://github.com/sassoftware/doc-supplement-statug/blob/main/Examples/m-n/mcmcgs1.sas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;filename url gs1 &#39;https://raw.githubusercontent.com/sassoftware/doc-supplement-statug/main/Examples/m-n/mcmcgs1.sas&#39;;
%include gs1;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we predict weight based on height in the &lt;code&gt;sashelp.class&lt;/code&gt; data set. The posterior samples are available as &lt;code&gt;work.classout&lt;/code&gt;. We&amp;rsquo;ll want to rename the height and weight variables to $x$ and $y$ in order to work with the chosen macro names. This is easily accomplished by using the &lt;code&gt;rename&lt;/code&gt; statement in the macro call itself. We&amp;rsquo;ll call it with $n=15$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;%sample_regression(ds=sashelp.class(rename=(Height=x Weight=y)),
                  posterior=work.classout,
                  n=15);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will produce the following output:&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-05-sample-regression-lines/class-regression-sample_hu8718c9b6d2106aec942f1d2ab5f5552b_53162_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-05-sample-regression-lines/class-regression-sample_hu8718c9b6d2106aec942f1d2ab5f5552b_53162_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;With slight modifications you can also use this macro to help you refine your priors. By using the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;model general(0);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;statement in PROC MCMC in lieu of a regular model you will get estimates of the prior parameters. See the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/statug/statug_mcmc_examples01.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; for examples.&lt;/p&gt;
&lt;!-- 
```sas sample-regression.sas
/*
 * This code was generated for the blog post &#34;Sampling Regression Lines&#34;
 * available at dmsenter89.github.io/post/23-05-sample-regression-lines/
 *
 * Author: Michael Senter, PhD
 */

&lt;&lt;&lt;macro-first-line&gt;&gt;&gt;
&lt;&lt;&lt;srs&gt;&gt;&gt;

&lt;&lt;&lt;parameter-list&gt;&gt;&gt;

&lt;&lt;&lt;lineparm&gt;&gt;&gt;
%mend sample_regression;

&lt;&lt;&lt;mcmc-gs1&gt;&gt;&gt;

&lt;&lt;&lt;macro-call&gt;&gt;&gt;
```
--&gt;
</description>
    </item>
    
    <item>
      <title>Simple Regression With PROC MCMC</title>
      <link>https://dmsenter89.github.io/post/23-05-simple-regression-with-proc-mcmc/</link>
      <pubDate>Tue, 02 May 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-05-simple-regression-with-proc-mcmc/</guid>
      <description>&lt;p&gt;In this post I&amp;rsquo;ll show how to fit simple linear and logistic regression models using the 
&lt;a href=&#34;https://support.sas.com/rnd/app/stat/procedures/mcmc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MCMC&lt;/a&gt; procedure in SAS. Note that the point of this post is to show how the mathematical model is translated into PROC MCMC syntax and not to discuss the method itself. I will include links to relevant sections in Johnson, Ott, and Dogucu (2022) if you&amp;rsquo;d like to read more about Bayesian modeling.&lt;/p&gt;
&lt;h2 id=&#34;the-mcmc-statement&#34;&gt;The MCMC Statement&lt;/h2&gt;
&lt;p&gt;The basic 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_mcmc_syntax.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;syntax&lt;/a&gt; for MCMC is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-nohighlight&#34;&gt;proc mcmc &amp;lt;options&amp;gt;;
    parms parameter &amp;lt;=&amp;gt; number &amp;lt;/options&amp;gt;;
    prior parameter ~ distribution;
    programming statements;
    model varaiable ~ distribution &amp;lt;/options&amp;gt;; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This covers the basic components of most Bayesian models - the model itself (&lt;code&gt;model&lt;/code&gt;), the parameters that need to be fit (&lt;code&gt;parms&lt;/code&gt;), and their prior distribution (&lt;code&gt;prior&lt;/code&gt;). Note that PROC MCMC requires you to always specify your priors, unlike some Bayesian modeling software that will default to some diffuse priors when they are omitted from the problem statement.&lt;/p&gt;
&lt;p&gt;The most common options you&amp;rsquo;ll use in the MCMC statement will be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data=&lt;/code&gt; the name of the input data set.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;outpost=&lt;/code&gt; the name of the output data set for posterior samples of parameters.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nbi=&lt;/code&gt; the number of burn-in iterations.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nmc=&lt;/code&gt; the number of MCMC iterations, excluding the burn-in iterations.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;seed=&lt;/code&gt; specify a random seed for the the simulation.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;thin=&lt;/code&gt; specify the thinning rate; see 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_introbayes_sect016.htm#statug.introbayes.bayesburnin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The names and function calls for the included distributions are described in the documentation on the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_mcmc_syntax09.htm#statug_mcmc002612&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MODEL statement&lt;/a&gt;. Their density definitions are documented 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_mcmc_details17.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;a-simple-linear-model&#34;&gt;A Simple Linear Model&lt;/h2&gt;
&lt;p&gt;A basic linear model looks something like this:&lt;/p&gt;
&lt;p&gt;\begin{aligned}
y_i &amp;amp;\sim \mathrm{Normal}(\mu_i, \sigma) \\&lt;br&gt;
\mu_i &amp;amp;= \beta_0 + \beta_1 x_i
\end{aligned}&lt;/p&gt;
&lt;p&gt;which will need to be combined with priors for $\beta_0$, $\beta_1$, and $\sigma$. Assume we have a data set &lt;code&gt;work.mydata&lt;/code&gt; that contains two variables: our predictor &lt;code&gt;x&lt;/code&gt; and our measured variable &lt;code&gt;y&lt;/code&gt;. Assume we use the above model together with the following priors:&lt;/p&gt;
&lt;p&gt;\begin{aligned}
\beta_0 &amp;amp;\sim \mathrm{Normal}(0, 10) \\&lt;br&gt;
\beta_1 &amp;amp;\sim \mathrm{Normal}(0, 10) \\&lt;br&gt;
\sigma &amp;amp;\sim \mathrm{Uniform}(0,50).
\end{aligned}&lt;/p&gt;
&lt;p&gt;Translating this into PROC MCMC is straightforward. Even though we can specify the statements in any order, it is common to define the model &amp;ldquo;upside down&amp;rdquo; so that each line contains only variables that have already been defined. This is for convenience, so you don&amp;rsquo;t forget to specify something before hitting &amp;ldquo;run.&amp;rdquo;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mcmc data=mydata
      outpost=posterior /* posterior sim */
      nmc=2000; /* # of data points in posterior */
    /* define the parameters. Optionally give an initial value */
    parms beta0 0 beta1 1;
    parms sigma; /* no initial value - mcmc finds its own */
    
    * define your priors;
    prior beta: ~ normal(mean=0, sd=10);
    prior sigma ~ uniform(0, 50);
    
    * define the mean and the model;
    mu = beta0 + beta1*x;
    model y ~ normal(mu, sd=sigma);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more info on simple regression, check out 
&lt;a href=&#34;https://www.bayesrulesbook.com/chapter-9.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chapters 9-11&lt;/a&gt; in Bayes Rules!&lt;/p&gt;
&lt;h2 id=&#34;a-simple-logistic-model&#34;&gt;A Simple Logistic Model&lt;/h2&gt;
&lt;p&gt;A basic logistic model will look as follows:&lt;/p&gt;
&lt;p&gt;\begin{aligned}
y_i &amp;amp;\sim \mathrm{Binomial}(n_i, p_i) \\&lt;br&gt;
\mathrm{logit}(p_i) &amp;amp;= \beta_0 + \beta_1 x_i
\end{aligned}&lt;/p&gt;
&lt;p&gt;combined with appropriate priors for $\beta_0$, $\beta_1$. Here $y_i$ is a positive integer response, $n_i$ is a count, and $x_i$ is still our predictor. In many medical studies we are interested in the special case where $y_i \in \{0,1\}$ so that the model becomes&lt;/p&gt;
&lt;p&gt;\begin{aligned}
y_i &amp;amp;\sim \mathrm{Bern}(p_i) \\&lt;br&gt;
\mathrm{logit}(p_i) &amp;amp;= \beta_0 + \beta_1 x_i.
\end{aligned}&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s assume a diffuse prior like this:&lt;/p&gt;
&lt;p&gt;\begin{aligned}
\beta_0 &amp;amp;\sim \mathrm{Normal}(0, 100) \\&lt;br&gt;
\beta_1 &amp;amp;\sim \mathrm{Normal}(0, 100).
\end{aligned}&lt;/p&gt;
&lt;p&gt;Then we can translate to PROC MCMC as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mcmc data=mydata
      outpost=posterior /* posterior sim */
      nmc=2000; /* # of data points in posterior */
    parms (beta0 beta1) 0;
    prior beta: ~ normal(0, sd=100);

    /* now define the logistic part: */
    p = logistic(beta0 + beta1*x);
    model y ~ bern(p);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Often we are not so much directly interested in the $\beta$ coefficients, but in the odds $e^{\beta_0}$ and the multiplicative change in odds $e^{\beta_1}$. While these values can be calculated and analyzed from the &lt;code&gt;outpost&lt;/code&gt; data set, we can use the &lt;code&gt;nodata&lt;/code&gt; block (delimited by &lt;code&gt;beginnodata&lt;/code&gt; and &lt;code&gt;endnodata&lt;/code&gt; statements) to directly calculate these values in our simulation. The amended procedure reads like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mcmc data=mydata
      outpost=posterior
      nmc=2000
      monitor=(odds modds);
    parms (beta0 beta1) 0;
    prior beta: ~ normal(0, sd=100);

    beginnodata;
      odds = exp(beta0);
      modds = exp(beta1);
    endnodata;

    p = logistic(beta0 + beta1*x);
    model y ~ bern(p);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See 
&lt;a href=&#34;https://www.bayesrulesbook.com/chapter-13.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chapter 13&lt;/a&gt; in Bayes Rules! for more info on logistic regression models.&lt;/p&gt;
&lt;h2 id=&#34;adding-a-random-effect&#34;&gt;Adding a Random-Effect&lt;/h2&gt;
&lt;p&gt;Random-effects, also known as hierarchical modeling, looks at group structures in the data seta nd models group-specific effects. In a clinical setting, this might be the study site.&lt;/p&gt;
&lt;p&gt;As a simple example, assume we have a data set &lt;code&gt;ht&lt;/code&gt; containing the height (&lt;code&gt;h&lt;/code&gt;) and sex (&lt;code&gt;s&lt;/code&gt;) of a population sample. Assume we are interested in modeling the distribution of height in our data set. We know that on average males are taller than females (mean 167 cm vs 156 cm based on NHANES 2006). We could build a model similar to this:&lt;/p&gt;
&lt;p&gt;\begin{aligned}
h_i &amp;amp;\sim \mathrm{Normal}(\mu_i, \sigma) \\&lt;br&gt;
\mu_i &amp;amp;= \alpha_{\mathrm{sex[i]}} \\&lt;br&gt;
\alpha_j &amp;amp;\sim \mathrm{Normal}(160, 20) \quad \text{for } j=1,2 \\&lt;br&gt;
\sigma &amp;amp;\sim \mathrm{Uniform}(0,50).
\end{aligned}&lt;/p&gt;
&lt;p&gt;In SAS, the random effect is specified with the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_mcmc_gettingstarted03.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;random&lt;/a&gt; statement. We specify the categories with the &lt;code&gt;subject&lt;/code&gt; keyword in the random statement. SAS will then automatically create the necessary number of parameters for the random effect. Our model translates to the following MCMC call:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mcmc data=ht
      outpost=posterior
      nmc=2000;
    parms sigma 5 sigmaA 6;
    prior sigma: ~ uniform(0,50);

    random alpha ~ normal(160, sd=sigmaA) subject=s monitor=(alpha);
    mu = alpha;
    model h ~ normal(mu, sd=sigma);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Assuming $s\in\{1,2\}$ this will cause SAS to create two alpha variables for the two levels of &lt;code&gt;s&lt;/code&gt;: &lt;code&gt;alpha_1&lt;/code&gt; and &lt;code&gt;alpha_2&lt;/code&gt;. Had &lt;code&gt;s&lt;/code&gt; been a character variable, say with values m and f, then SAS would have created &lt;code&gt;alpha_m&lt;/code&gt; and &lt;code&gt;alpha_f&lt;/code&gt; instead.&lt;/p&gt;
&lt;p&gt;For more information on this, check out 
&lt;a href=&#34;https://www.bayesrulesbook.com/chapter-15.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;unit IV&lt;/a&gt; of Bayes Rules!&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;Johnson, A. A., Ott, M. Q., and Dogucu, M. (2022), &lt;em&gt;Bayes Rules!: An Introduction to Applied Bayesian Modeling&lt;/em&gt;, Boca Raton, FL: CRC Press, DOI: 
&lt;a href=&#34;https://doi.org/10.1201/9780429288340&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.1201/9780429288340&lt;/a&gt;. Available online at 
&lt;a href=&#34;https://www.bayesrulesbook.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.bayesrulesbook.com&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Loading Several XPT Files From a URL</title>
      <link>https://dmsenter89.github.io/post/23-04-loading-several-xpt-files-from-a-url/</link>
      <pubDate>Mon, 24 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-04-loading-several-xpt-files-from-a-url/</guid>
      <description>&lt;p&gt;The SAS Transport File Format (XPORT) is an open file format maintained by SAS for exchanging datasets. Its use is mandated by the FDA for data set submission for new drug or device applications and the CDC uses this format to distribute public data. For details regrading this format, see 
&lt;a href=&#34;https://www.loc.gov/preservation/digital/formats/fdd/fdd000464.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this Library of Congress page&lt;/a&gt;. This post will explore how to read several of these files into a SAS session with the URL filename statement using the National Health and Nutrition Examination Survey, or 
&lt;a href=&#34;https://www.cdc.gov/nchs/nhanes/index.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NHANES&lt;/a&gt;, as an example.&lt;/p&gt;
&lt;h2 id=&#34;loading-a-single-xpt-file&#34;&gt;Loading a Single XPT File&lt;/h2&gt;
&lt;p&gt;By far the easiest way to read an XPT file is to use the &lt;code&gt;XPT2LOC&lt;/code&gt; autocall macro if it is available on your SAS installation. As an example, this snippet would load the demographics table from the 2017-2018 NHANES data set into the work library:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;filename demo &amp;quot;/data/Nhanes/2017-2018/DEMO_J.XPT&amp;quot;;
%XPT2LOC(filespec=demo, libref=work);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This macro correctly resolves the name of the data set, and it would be available as &lt;code&gt;work.demo_j&lt;/code&gt; now.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; See the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/movefile/p1tp8gighlgeifn173i6kzw2w3bu.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; for more details on this macro.&lt;/p&gt;
&lt;p&gt;If we cannot or do not want to use this macro, we&amp;rsquo;ll have to assign a LIBREF to the XPT file. This might seem weird at first, because you typically will only find a single data set in an XPT file. But if you consider that the file standard allows for multiple data sets to reside in the same XPT file, it makes sense. Using the LIBREF, we can achieve the same result as above using this snippet:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;filename xpt url &amp;quot;https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT&amp;quot;;
libname xpt xport;

proc copy in=xpt out=work; run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;loading-multiple-xpt-files-with-a-macro&#34;&gt;Loading Multiple XPT Files with a Macro&lt;/h2&gt;
&lt;p&gt;This is all fine if you only need to load one or two files that way, but becomes tedious (and repetitive) if you have to load many data sets this way. Ignoring the restricted data sets for a minute, NHANES contains many data sets spread across five domains:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Domain&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;# of Data Sets&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Demographics Data&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dietary Data&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Examination Data&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Laboratory Data&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;51&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Questionnaire Data&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;TOTAL&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Even if you only need a subset of this, you&amp;rsquo;ll find yourself wanting to shortcut having to type out all the repetitive information. This is where a macro call comes in handy.&lt;/p&gt;
&lt;p&gt;A great trick for this is to use a codebook like data set that you can iterate over. Here is a minimal example using four data sets from NHANES:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;/* create a location to hold saved data */
libname nhanes &#39;~/data/NHANES&#39;;

data nhanes.datasets;
    length df $10. dfname $100.;
    input df $ dfname $;
    infile datalines dsd;
datalines;
DEMO_J,Demographic Variables and Sample Weights
BPX_J,Blood Pressure
BMX_J,Body Measures
OHXDEN_J,Oral Health - Dentition
;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can either create this data set yourself or use a webscraping tool to make it for you. Wrapping the autocall macro or the PROC COPY into a macro is straightforward:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;%macro load_data(name=);
  /* allow bad SSL; this is due to an issue with cdc.gov */
  options set=SSLREQCERT=&amp;quot;allow&amp;quot;;

  /* set up for import */
  filename xpt url &amp;quot;https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/&amp;amp;name..XPT&amp;quot;;
  libname xpt xport;

  proc copy in=xpt out=nhanes; run;

  /* make sure to clear libname &amp;amp; filename for next macro call */
  filename xpt; libname xpt;
%mend;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only question now is how to trigger this macro for each data set listed in &lt;code&gt;nhanes.datasets&lt;/code&gt;. That&amp;rsquo;s where the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/lefunctionsref/p1blnvlvciwgs9n0zcilud6d6ei9.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CALL EXECUTE&lt;/a&gt; routine comes in. It allows us to invoke a macro for each line in the source data set while giving us access to the variables in the source data. Since this is executed as part of a data step, you can use more fine grained control by having if/else conditions, where clauses, etc. In our example, we&amp;rsquo;d use this data step:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data _NULL_;
    set nhanes.datasets;
    call execute(&#39;%load_data(name=&#39;||df||&#39;);&#39;);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After running our script, the folder specified by &lt;code&gt;libname nhanes&lt;/code&gt; will contain both our &amp;ldquo;codebook&amp;rdquo; of data sets, as well as all of the data sets listed in the file.&lt;/p&gt;
&lt;!-- 
```sas nhanes_load.sas
/*
 * This code was generated for the blog post &#34;Loading Several XPT Files From a URL&#34;
 * available at dmsenter89.github.io/post/23-04-loading-several-xpt-files-from-a-url/
 *
 * Author: Michael Senter, PhD
 */

options dlcreatedir;

&lt;&lt;&lt;codebook&gt;&gt;&gt;

&lt;&lt;&lt;macro&gt;&gt;&gt;

&lt;&lt;&lt;iteration&gt;&gt;&gt;
```

--&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Note that using this macro requires you to first download the file for processing. You can do this easily with a 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/v_037/lestmtsglobal/p05r9vhhqbhfzun1qo9mw64s4700.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TEMP filename&lt;/a&gt; statement. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>PROC MI Added to SASPy</title>
      <link>https://dmsenter89.github.io/post/23-02-proc-mi-added-to-saspy/</link>
      <pubDate>Mon, 06 Feb 2023 14:45:00 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/23-02-proc-mi-added-to-saspy/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m excited to announce that the new 
&lt;a href=&#34;https://github.com/sassoftware/saspy/releases/tag/v4.6.0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAPy v4.6.0&lt;/a&gt; release includes a pull request of mine that adds 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/statug/15.2/statug_mi_toc.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PROC MI&lt;/a&gt; to the SAS/STAT procedures directly exposed in SASPy. This procedure allows you to analyze missing data patterns and create imputations for missing data.&lt;/p&gt;
&lt;h2 id=&#34;syntax&#34;&gt;Syntax&lt;/h2&gt;
&lt;p&gt;PROC MI is accessed via the &lt;code&gt;mi&lt;/code&gt; function that has been added to the &lt;code&gt;SASstat&lt;/code&gt; class. Like other procedures, the SAS statements in MI are called as keyword arguments to the function whose name matches the SAS 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/statug/15.2/statug_mi_syntax.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;syntax&lt;/a&gt;:&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;PROC MI options;
  BY variables;
  CLASS variables;
  EM &amp;lt;options&amp;gt;;
  FCS &amp;lt;options&amp;gt;;
  FREQ variable;
  MCMC &amp;lt;options&amp;gt;;
  MNAR options;
  MONOTONE &amp;lt;options&amp;gt;;
  TRANSFORM transform (variables&amp;lt;/ options&amp;gt;) &amp;lt;…transform (variables&amp;lt;/ options&amp;gt;)&amp;gt;;
  VAR variables;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the corresponding function signature in Python:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def mi(self, data: (&#39;SASdata&#39;, str) = None,
        by: (str, list) = None,
        cls: (str, list) = None,
        em: str = None,
        fcs: str = None,
        freq: str = None,
        mcmc: str = None,
        mnar: str = None,
        monotone: str = None,
        transform: str = None,
        var: str = None,
        procopts: str = None,
        stmtpassthrough: str = None,
        **kwargs: dict) -&amp;gt; &#39;SASresults&#39;:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Statements  like &lt;code&gt;EM&lt;/code&gt; or &lt;code&gt;MCMC&lt;/code&gt;, which can stand alone in SAS, are called with an empty string argument in Python.&lt;/p&gt;
&lt;h2 id=&#34;basic-example&#34;&gt;Basic Example&lt;/h2&gt;
&lt;!-- 
```python saspy_mi.py
#!/usr/bin/env python3
#
# Example: how to access PROC MI with SASPy. To accompany
#   dmsenter89.github.io/post/23-02-proc-mi-added-to-saspy
# 
# Author: Michael Senter, PhD

import saspy


# starting the SAS Session
&lt;&lt;&lt;session&gt;&gt;&gt;

&lt;&lt;&lt;procmi&gt;&gt;&gt;

&lt;&lt;&lt;sasdata&gt;&gt;&gt;


# ending the SAS session 
sas.endsas()
```
--&gt;
&lt;p&gt;To use the new MI functionality, make sure you have updated to the newest SASPy release. In addition to starting  a SAS Session as per usual, you will also want to enable access to the SAS/STAT procedures:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sas = saspy.SASsession()  # loads a session using your default profile
stat = sas.sasstat()      # gives access to SAS/STAT procedures 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once these session objects are loaded, you can start using the mi function with &lt;code&gt;stat.mi&lt;/code&gt;. The simplest possible call is to invoke MI with a built-in data set and all defaults as &lt;code&gt;stat.mi(data=&#39;sashelp.heart&#39;)&lt;/code&gt;. For best results, store the output in a SASResults object. From there you can access the SAS log associated with the function call (&lt;code&gt;LOG&lt;/code&gt;) as well as all ODS Output using the ODS 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/statug/15.2/statug_mi_details82.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;table names&lt;/a&gt; in all caps. The default uses the EM method with 25 imputations.&lt;/p&gt;
&lt;p&gt;A more realistic use might look something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ods = stat.mi(data=&#39;sashelp.heart&#39;, em=&amp;quot;outem=outem&amp;quot;,
              var=&amp;quot;Cholesterol Height Smoking Weight&amp;quot;,
              procopts=&amp;quot;simple nimpute=20 out=imp&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is equivalent to running&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mi data=sashelp.heart simple nimpute=20 out=imp;
    em outem=outem;
    var Cholesterol Height Smoking Weight;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in SAS. This call uses the EM procedure to impute values for the cholesterol, height, smoking, and weight variables. The &lt;code&gt;simple&lt;/code&gt; option displays univariate statistics and correlations. The &lt;code&gt;outem&lt;/code&gt; option saves a data set containing the computed MLE to &lt;code&gt;work.outem&lt;/code&gt;. The imputed data sets are saved to &lt;code&gt;work.imp&lt;/code&gt;, which contains the additional variable &lt;code&gt;_IMPUTATION_&lt;/code&gt; with the imputation number. This can be used as a &lt;code&gt;by&lt;/code&gt; variable in other procedures, and the results can later be pooled using PROC MIANALYZE.&lt;/p&gt;
&lt;p&gt;The resulting &lt;code&gt;ods&lt;/code&gt; object for our example exposes the following ODS outputs to your Python instance, in addition to the log:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;CORR&#39;, &#39;EMESTIMATES&#39;, &#39;EMINITESTIMATES&#39;, &#39;EMPOSTESTIMATES&#39;, &#39;MISSPATTERN&#39;, &#39;MODELINFO&#39;, &#39;PARAMETERESTIMATES&#39;, &#39;UNIVARIATE&#39;, &#39;VARIANCEINFO&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See the SAS documentation for details. To use the imputed data with Python tools, create a SAS data object. We&amp;rsquo;ll also print the first few entries so we can see what it looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;imputed = sas.sasdata(table=&amp;quot;imp&amp;quot;, libref=&amp;quot;work&amp;quot;)
imputed.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;One exception is the SAS &lt;code&gt;class&lt;/code&gt; statement, which is implemented as &lt;code&gt;cls&lt;/code&gt; due to &lt;code&gt;class&lt;/code&gt; being a reserved keyword in Python. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Missing Data Mechanisms</title>
      <link>https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/</link>
      <pubDate>Tue, 03 Jan 2023 10:00:00 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/</guid>
      <description>&lt;p&gt;Understanding whether a variable&amp;rsquo;s missingness from a dataset is related to the underlying value of the data is a key concept in the field of missing data analysis. We distinguish three broad categories: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). In his book &lt;em&gt;Statistical Rethinking&lt;/em&gt;, McElreath&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; gives an amusing example to illustrate this concept: he considers variants of a dog eating homework and how the dog chooses - if at all - to eat the homework. The examples he give show substantial shifts in observed values, which make for a good illustration of the types of problems you might encounter. A lecture corresponding to the  example from the book can be found on 
&lt;a href=&#34;https://www.youtube.com/watch?v=oMiSb8GKR0o&amp;amp;list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN&amp;amp;index=18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube&lt;/a&gt;. In this post, I will first briefly review the different missing data mechanisms before implementing McElreath&amp;rsquo;s examples in SAS.&lt;/p&gt;
&lt;h3 id=&#34;overview-of-missing-data-mechanisms&#34;&gt;Overview of Missing Data Mechanisms&lt;/h3&gt;
&lt;p&gt;My presentation here follows van Buuren&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Let $Y$ be a $n \times p$ matrix representing a sample of $p$ variables for $n$ units of the sample and $R$ be a corresponding $n \times p$ indicator matrix, so that&lt;/p&gt;
&lt;p&gt;$$r_{i,j} = \begin{cases} 1 &amp;amp; y_{i,j} \text{ is observed} \\ 0 &amp;amp; y_{i,j} \text{ not observed.}\end{cases} $$&lt;/p&gt;
&lt;p&gt;We denote the observed data by $Y_\text{obs}$ and the missing data that $Y_\text{miss}$ so that $Y=(Y_\text{obs},Y_\text{miss})$.&lt;/p&gt;
&lt;p&gt;We distinguish three main categories for how the distribution of $R$ may depend on $Y$. This relationship is described as the &lt;em&gt;missing data model&lt;/em&gt;. Let $\psi$ contain the parameters of this model. The general expression of the missing data model is $\mathrm{Pr}(R|Y_\text{obs}, Y_\text{miss}, \psi)$, where $\psi$ consists of the parameters of the missing data model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Missing Completely at Random (MCAR).&lt;/strong&gt; This implies that the cause of the missing data is unrelated to the data itself. In this case,&lt;/p&gt;
&lt;p&gt;$$ \mathrm{Pr}(R=0| Y_\text{obs}, Y_\text{miss}, \psi) = \mathrm{Pr}(R=0|\psi).$$&lt;/p&gt;
&lt;p&gt;This is the ideal case, but unfortunately rare in practice. Many researchers implicitly assume this when using methods such as list-wise deletion, otherwise known as complete case analysis, which can produce unbiased estimates of sample means if the data are MCAR, although the reported standard error will be too large.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Missing at Random (MAR).&lt;/strong&gt; Missingness is the same within groups defined by the observed data, so that&lt;/p&gt;
&lt;p&gt;$$ \mathrm{Pr}(R=0| Y_\text{obs}, Y_\text{miss}, \psi) = \mathrm{Pr}(R=0|Y_\text{obs},\psi).$$&lt;/p&gt;
&lt;p&gt;This is a often a more reasonable assumption in practice and the starting point for modern missing data methods.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Missing not at Random (MNAR).&lt;/strong&gt; If neither the MCAR or MAR assumptions hold, then we may find that missingness depends on the missing data itself, in which case there is no simplification and
$$ \mathrm{Pr}(R=0| Y_\text{obs}, Y_\text{miss}, \psi) = \mathrm{Pr}(R=0| Y_\text{obs}, Y_\text{miss}, \psi).$$&lt;/p&gt;
&lt;p&gt;As you can imagine, this is the most tricky case to deal with.&lt;/p&gt;
&lt;h3 id=&#34;dogs-eating-homework&#34;&gt;Dogs Eating Homework&lt;/h3&gt;
&lt;p&gt;Consider dogs (D) eating students&#39; homework. Each student&amp;rsquo;s homework score (H) is graded on a 10-point scale and each student&amp;rsquo;s score varies in proportion to how much they study (S). We assume the amount of time they study is normally distributed. A binomial is used to generate homework scores from the normed time spent studying. McElreath uses the following code to simulate the full data set:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;N &amp;lt;- 100
S &amp;lt;- rnorm( N )
H &amp;lt;- rbinom( N, size=10, inv_logit(S) )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;inv_logit(x) = exp(x)+(1+exp(x))&lt;/code&gt;, the definition used by the &lt;code&gt;LOGISTIC&lt;/code&gt; function in SAS. With a data step, this can be represented in SAS as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data full;
    DO i=1 to 100;
        S=RAND(&#39;NORM&#39;);
        H=RAND(&#39;BINO&#39;, LOGISTIC(S), 10);
        output;
    END;
    label S=&#39;Amount of Studying&#39; H=&#39;Homework Score&#39;;
    drop i;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get closer in form to the R code by using PROC IML, but that&amp;rsquo;s a story for a different post.&lt;/p&gt;
&lt;p&gt;Say we are interested in estimating the relationship between $S$ and $H$. In our example, we assume that $H$ is not directly observable. Instead, $H^*$ is observed - a subset of the full data set $H$ with some homework values missing. We can now look at &lt;em&gt;why&lt;/em&gt; some of those values are missing. Specifically, in McElreath&amp;rsquo;s example each student has a dog $D$ and sometimes the dog eats the homework. But here we can again ask, why is the dog eating the homework? McElreath uses  
&lt;a href=&#34;https://en.wikipedia.org/wiki/Directed_acyclic_graph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;directed acyclic graphs&lt;/a&gt; (DAGs) to represent different missing data models, reproduced below. As we will see, these are some intuitive examples for our three missing data mechanism categories.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-directed-acyclic-graphs-corresponding-to-mcelreaths-examples-of-missing-data-models-s-represents-the-amount-of-time-spent-studying-which-in-turn-influences-the-homework-score-h-which-is-only-partially-observed-indicated-by-the-circle-alas-dogs-d-eat-some-of-the-homework-the-actually-observed-scores---those-not-eaten---are-indicated-by-h-adapted-from-figure-154-in-_statistical-rehinking_&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/dag_hu9d4aa2d4b53fa51c30ec3b81dc4e20f1_14343_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;The directed acyclic graphs corresponding to McElreath&amp;amp;rsquo;s examples of missing data models. $S$ represents the amount of time spent studying, which in turn influences the homework score $H$, which is only partially observed (indicated by the circle). Alas, dogs $D$ eat some of the homework. The actually observed scores - those not eaten - are indicated by $H^*$. Adapted from figure 15.4 in &amp;lt;em&amp;gt;Statistical Rehinking&amp;lt;/em&amp;gt;.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/dag_hu9d4aa2d4b53fa51c30ec3b81dc4e20f1_14343_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;665&#34; height=&#34;179&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    The directed acyclic graphs corresponding to McElreath&amp;rsquo;s examples of missing data models. $S$ represents the amount of time spent studying, which in turn influences the homework score $H$, which is only partially observed (indicated by the circle). Alas, dogs $D$ eat some of the homework. The actually observed scores - those not eaten - are indicated by $H^*$. Adapted from figure 15.4 in &lt;em&gt;Statistical Rehinking&lt;/em&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;missing-completely-at-random-mcar&#34;&gt;Missing Completely At Random (MCAR)&lt;/h4&gt;
&lt;p&gt;In the first example, the dogs eat homework completely at random. This is the most basic and benign case, and corresponds to DAG 1) in Figure 1. McElreath&amp;rsquo;s R code is given by&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;D &amp;lt;- rbern( N ) 
Hm &amp;lt;- H  # H*, but * is not a valid char for varnames in R
Hm[D==1] &amp;lt;- NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can implement this in SAS by using the &lt;code&gt;RAND&lt;/code&gt; function with the Bernoulli argument in an if/else clause:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;if RAND(&#39;BERN&#39;, 0.5) then Hm = .;
    else Hm = H;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This causes about half of our data to be hidden, but not in a biased way.&lt;/p&gt;
&lt;h4 id=&#34;missing-at-random-mar&#34;&gt;Missing at Random (MAR)&lt;/h4&gt;
&lt;p&gt;In the second example, we assume the amount of time a student spends studying decreases the amount of time they have to play with and exercise their dog. This, in turn, influences whether the homework gets eaten. Or, as McElreath puts it, the &amp;ldquo;dog eats conditional on the cause of homework.&amp;rdquo; In his particular example, the homework is eaten whenever a student spends more time studying than the average $S=0$. This corresponds to DAG 2) in Figure 1.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;D &amp;lt;- ifelse( S&amp;gt;0 , 1 , 0 )
Hm &amp;lt;- H
Hm[D==1] &amp;lt;-NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In SAS:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;if S&amp;gt;0 then Hm = .;
    else Hm = H;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;missing-not-at-random-mnar&#34;&gt;Missing not at Random (MNAR)&lt;/h4&gt;
&lt;p&gt;In this case, we have some correspondence between the missing variable&amp;rsquo;s value and whether or not it is missing from the data set. Here, the &amp;ldquo;dog eats conditional on the homework itself.&amp;rdquo; Suppose that dogs prefer to eat bad homework. In such a case, the value of $H$ is directly related to whether or not $H$ is observed in the particular unit or not. His example R code is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;# dogs prefer bad homework
D &amp;lt;- ifelse( H&amp;lt;5 , 1 , 0 )
Hm &amp;lt;- H
Hm[D==1] &amp;lt;- NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And in SAS:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;if H&amp;lt;5 then Hm = .;
    else Hm = H;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-full-sas-code&#34;&gt;The Full SAS Code&lt;/h3&gt;
&lt;p&gt;We can now build a SAS data set that contains a full copy of the original data set, together with our various examples of missing data mechanisms. I have added a seed to the data step for reproducibility.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data full;
    CALL streaminit( 451 ); 
    
    LABEL
        Type = &#39;Missing Data Mechanism&#39;
        S = &#39;Amount of Studying&#39;
        H = &#39;Homework Score&#39;
        Hm = &#39;Observed Homework Score&#39;
    ;

    DO i=1 to 100;
        TYPE = &#39;FULL&#39;;
        S = RAND(&#39;NORM&#39;);
        H = RAND(&#39;BINO&#39;, LOGISTIC(S), 10);
        Hm = H;
        output;
        
        /* Example 1) MCAR */
        TYPE = &#39;MCAR&#39;;
        if RAND(&#39;BERN&#39;, 0.5) then Hm = .;
            else Hm = H;
        output;
        
        /* Example 2) MAR */
        TYPE = &#39;MAR&#39;;
        if S&amp;gt;0 then Hm = .;
            else Hm = H;
        output;
        
        /* Example 3) MNAR */
        TYPE = &#39;MNAR&#39;;
        if H&amp;lt;5 then Hm = .;
            else Hm = H;
        output;
    
    END;
    
    drop i;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may want to run a &lt;code&gt;PROC SORT&lt;/code&gt; or &lt;code&gt;PROC SQL&lt;/code&gt; afterwards to group the different categories together, as they will be alternating in this data set.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;





  
  











&lt;figure id=&#34;figure-boxplot-of-our-example-data-note-that-the-mcar-data-looks-very-similar-to-the-original-data-set-unlike-the-mar-and-mnar-versions&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/mdm_boxplot_hu7d089ca360b9b11aaf1a482082b6d8dc_11177_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Boxplot of our example data. Note that the MCAR data looks very similar to the original data set, unlike the MAR and MNAR versions.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/mdm_boxplot_hu7d089ca360b9b11aaf1a482082b6d8dc_11177_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Boxplot of our example data. Note that the MCAR data looks very similar to the original data set, unlike the MAR and MNAR versions.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We can see that MCAR leads to minimal bias in our example data, while both the MAR and MNAR variations lead to substantial differences in observed vs actual homework scores for our synthetic population. For a more subtle example, see section 2.2.4 in van Buuren,&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; available 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-idconcepts.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;online here&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;R. McElreath, &lt;em&gt;Statistical Rethinking&lt;/em&gt;, 2nd ed, Chapman and Hall/CRC, Boca Raton, FL, 2020. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;S. van Buuren, &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt;, 2nd ed, Chapman and Hall/CRC, Boca Raton, FL, 2019. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>CSV2DS</title>
      <link>https://dmsenter89.github.io/post/22-11-csv2ds/</link>
      <pubDate>Wed, 23 Nov 2022 16:43:28 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/22-11-csv2ds/</guid>
      <description>&lt;p&gt;Creating a minimum working example (MWE) is a relatively frequent task. It is no problem to share an MWE for a feature in SAS because a large number of example data sets are shipped and installed by default. But sometimes you need an MWE because you are having trouble accomplishing a particular task with particular input data. At that point, you will need to share the data or a subset thereof together with your code. In SAS Forums, the preferred way to do this is with a datastep using a datalines/cards statement. Writing these by hand can be tedious since the data source is not typically a datalines statement to begin with. I have previously seen a SAS macro that can be used to generate a datalines statement from a SAS data set, but can&amp;rsquo;t seem to locate it at the moment. The data source I personally encounter the most often in my work is either in CSV or Excel formats. Since the latter can easily be exported to CSV, I decided to write a program that generates a SAS data step given a CSV file.&lt;/p&gt;
&lt;p&gt;For the implementation language I chose to use 
&lt;a href=&#34;https://go.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Go&lt;/a&gt;. I started learning about Go back in May when I implemented a 
&lt;a href=&#34;https://dmsenter89.github.io/post/22-05-go-wordle/&#34;&gt;simple CLI version of Wordle&lt;/a&gt;. Since then I have increasingly used Go to write various small tools at work. It has been a very enjoyable language to write in and distribution via GitHub is easy. If you have the Go toolchain installed, you can get the latest copy of csv2ds using&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;go install github.com/dmsenter89/csv2ds@latest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tool is very simple to use. Give it a CSV file or list of CSV files and it will generate a data step for each file using the CSV&amp;rsquo;s base name as the data set name. To ensure compatibility, variable names and the data set name are processed to be compatible with SAS&#39; naming scheme. The tool will attempt to guess if a particular column is numeric or not. If a column is determined to not be numeric, the longest cell will be used to set that variable&amp;rsquo;s length via a length statement to prevent truncation.&lt;/p&gt;
&lt;p&gt;I often work with the 
&lt;a href=&#34;https://csvkit.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;csvkit&lt;/a&gt; suite of command-line tools. It&amp;rsquo;s a wonderful collection of Python programs that can import data into CSV, generate basic column statistics, and use grep and SQL to extract data from a CSV file, amongst other things. This collection is designed to allow you to pipe the output from one as input to the next. Consider 
&lt;a href=&#34;https://csvkit.readthedocs.io/en/latest/tutorial/2_examining_the_data.html#csvsort-order-matters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this example&lt;/a&gt;. Csvcut is used to extract only certain columns from the file data.csv. Then csvgrep is used to subset to use only the data pertaining to one particular county. Then the data is sorted by the total_cost variable and displayed. I wanted my tool to be compatible with this suite, so if &lt;code&gt;-&lt;/code&gt; is passed as the filename, csv2ds will read the contents of STDIN instead. Changing the above csvkit example by replacing csvlook with my tool will generate the corresponding SAS data set:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;csvcut -c county,item_name,total_cost data.csv | csvgrep -c county -m LANCASTER | csvsort -c total_cost -r | csv2ds -
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point csv2ds is quite simple, but sufficient for my needs. Some minor intervention may be needed to make the data step template work for your data. Informats like DOLLAR are not recognized as numeric and minor edits would need to be made to the produced template.&lt;/p&gt;
&lt;p&gt;Checkout my new tool over on 
&lt;a href=&#34;https://github.com/dmsenter89/csv2ds&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SAS Markdown for Reproducibility</title>
      <link>https://dmsenter89.github.io/post/22-11-sas-markdown-for-reproducibility/</link>
      <pubDate>Fri, 11 Nov 2022 15:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/22-11-sas-markdown-for-reproducibility/</guid>
      <description>&lt;p&gt;One of the coolest packages for R is 
&lt;a href=&#34;https://yihui.org/knitr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;knitr&lt;/a&gt;. Essentially, it allows you to combine explanatory writing, such as a paper or blog post, directly with your analysis code in a Markdown document. When the target document
is compiled (&amp;lsquo;knitted&amp;rsquo;), the R code in the document is run and the results inserted into the final document. The target document could
be an HTML or a PDF file, for example. This is great for many reasons. You have a regular report you want to run, but the data updates?
Just re-knit and your entire report is updated. No more separate running of the code followed by copying the results into whatever
software you use to build the report itself. This makes it not just less cumbersome, but less error prone. It also improves reproducibility.
Somebody wants to see your work, perhaps because they are unsure of your results or they want to extend your work? You can share the
markdown file and the other party can see exactly what code was used to generate what part of your report or paper.&lt;/p&gt;
&lt;p&gt;While knitr is certainly not the first package that allows for this workflow, and also not the only one, I have found it to be the most consistent and easy to use.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Luckily, knitr supports 
&lt;a href=&#34;https://yihui.org/knitr/demo/engines/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a variety&lt;/a&gt; of 
&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/language-engines.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;languages&lt;/a&gt;, including 
&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/eng-sas.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS&lt;/a&gt;. And you can even mix and match multiple languages in 
&lt;a href=&#34;https://github.com/yihui/knitr-examples/blob/master/106-polyglot.Rmd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;one document&lt;/a&gt;.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;You might think that this sounds similar to Jupyter notebooks. While that is true, and there is a 
&lt;a href=&#34;https://github.com/sassoftware/sas_kernel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter kernel for SAS&lt;/a&gt; as well, knitr has some advantages over Jupyter for report-generation. Without additional tools, you have the option to execute but not display the code that generates your results, making a cleaner report. You can also elect to only show part of the code, with manual setup code running behind the scenes without being printed to the report itself. Additionally, the entire document is executed linearly. That means that if you update a code chunk towards the beginning of your document, it affects the code chunks following it, while in Jupyter you easily get in the habit of executing the chunks independently which can lead to inconsistencies if you don&amp;rsquo;t pay attention to the cell numbers.&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;In this post, I&amp;rsquo;ll demonstrate the basics of setting up a reproducible report using the SAS engine in knitr.&lt;/p&gt;
&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;
&lt;p&gt;Perhaps the easiest way to get started for beginners is to use RStudio and Anaconda. With that you can create a sample R Markdown document (&lt;code&gt;File -&amp;gt; New File -&amp;gt; R Markdown&lt;/code&gt;). Press the &lt;code&gt;knit&lt;/code&gt; button. If any packages required by knitr are missing, RStudio will install them for you. This way you can be sure that all the R parts are set up correctly. Additionally, I recommend installing the 
&lt;a href=&#34;https://github.com/Hemken/SASmarkdown&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SASmarkdown&lt;/a&gt; package with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# from CRAN:
install.packages(&amp;quot;SASmarkdown&amp;quot;)
# from GitHub: 
devtools::install_github(&amp;quot;Hemken/SASmarkdown&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once install is complete, load the package (&lt;code&gt;library(SASmarkdown)&lt;/code&gt;) and check the output. If you see a message that SAS was found, you are good to go. If not, you will either need to add SAS to your PATH or simply provide the path to SAS as an option in your document (see below).&lt;/p&gt;
&lt;h2 id=&#34;a-basic-markdown-file&#34;&gt;A Basic Markdown File&lt;/h2&gt;
&lt;p&gt;The important thing is to load the SASMarkdown package in your document. I recommend making a setup chunk at the very top of your document and setting include to FALSE.
That way the setup chunk is executed, but not printed to your final document.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r setup, include=FALSE}
library(SASmarkdown)
# if SAS is not in your path, define it manually:
saspath &amp;lt;- &amp;quot;C:/Program Files/SASHome/SASFoundation/9.4/sas.exe&amp;quot;
knitr::opts_chunk$set(engine=&amp;quot;sashtml&amp;quot;, engine.path=saspath)
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With that, we&amp;rsquo;re ready to run a basic SAS chunk using just the SAS option. This produces the typewriter-style output that is familiar from Enterprise Guide for example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;```{sas example1}
proc print data=sashelp.class; run;
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to take advantage of the modern HTML output that is standard in SAS Studio, we use the &lt;code&gt;sashtml&lt;/code&gt; engine instead:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;```{sashtml example2}
/* if you want, you can set an ODS style for HTML output: */
ods html style=journal;
proc print data=sashelp.class; run;
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want graphical output, for example from SGPLOT, you&amp;rsquo;ll need to use the &lt;code&gt;sashtml&lt;/code&gt; engine. To get the default blue look from SAS Studio, use the HTMLBLUE style:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;```{sashtml example3}
ods html style=HTMLBLUE;
proc sgplot data=sashelp.cars;
  scatter x=EngineSize y=MPG_CITY;
run;
```
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;some-additional-comments&#34;&gt;Some Additional Comments&lt;/h2&gt;
&lt;p&gt;The first thing that is important to note is that each chunk is processed &lt;em&gt;separately&lt;/em&gt;. That means each chunk should be written so as to be capable of being executed independent of the others. It is possible to get around this using the &lt;code&gt;collectcode=TRUE&lt;/code&gt; chunk option. This chunk will then subsequently be executed prior to the code from a following chunk. So for example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;```{sashtml save1, collectcode=TRUE}
data sample;
  set sashelp.class;
run;
```
  And now use it again:
```{sashtml save2}
proc means data=sample; run;
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is particularly useful for libnames and setting the preferred ODS style, so you don&amp;rsquo;t have to keep doing it again in each cell.&lt;/p&gt;
&lt;p&gt;The other thing to note is that knitr for SAS works best with HTML output. It can use SAS styles and produce output looking like what
you would expect running in SAS Studio. If you want PDF output, you can get nicer output using 
&lt;a href=&#34;https://support.sas.com/rnd/base/ods/odsmarkup/latex.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX Tagsets for ODS&lt;/a&gt; and the 
&lt;a href=&#34;https://support.sas.com/rnd/app/papers/statrep.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StatRep System&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;knitr itself was based on Sweave, but uses Markdown instead of LaTeX code. Other languages have similar packages, for
example 
&lt;a href=&#34;https://mpastell.com/pweave/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pweave&lt;/a&gt; for Python or 
&lt;a href=&#34;https://docs.juliahub.com/Weave/9EzOc/0.9.4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weave&lt;/a&gt; for Julia. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The chunks from different languages do not have access to each other&amp;rsquo;s data. To move data between the different engines,
more setup work is needed. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;If you code in Julia, there is an interesting new reactive notebook called 
&lt;a href=&#34;https://github.com/fonsp/Pluto.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pluto&lt;/a&gt; that
promises to always keep your cells in sync, while being geared towards a Jupyter-style workflow. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Loading Zillow Housing Data in SAS</title>
      <link>https://dmsenter89.github.io/post/22-08-zillow-data/</link>
      <pubDate>Mon, 01 Aug 2022 17:21:38 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-08-zillow-data/</guid>
      <description>&lt;p&gt;Zillow is a well-known website widely used by those searching for a home or curious to find out
the value of their current home. What you may not know is that Zillow has a dedicated research page.
To make their website work optimally, they churn through tons of data on the American housing market.
They share insights they gleaned via 
&lt;a href=&#34;https://www.zillow.com/research/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;zillow.com/research&lt;/a&gt;. If you
visit their research  website you&amp;rsquo;ll notice they have a data page where you can download some really
cool data sets for your own research. They even have an API with which you can load data directly, but
you&amp;rsquo;ll have to register for access. In this post, we&amp;rsquo;ll look at how to load the CSV files that are
available for direct download into SAS for analysis.&lt;/p&gt;
&lt;p&gt;The CSV files can be downloaded 
&lt;a href=&#34;https://www.zillow.com/research/data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. In the example below,
I&amp;rsquo;m working with the Zillow Home Value Index file for all homes, seasonally adjusted at the ZIP code level.
Tha file is fairly large. It has data going from January 2000 through June 2022 in more than 27,000 rows of data
and about 280 columns. Below is an image of the beginning of this file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;zhvi_data_preview.png&#34; alt=&#34;&#34; title=&#34;The beginning of the of the ZHVI &#39;flagship&#39; data file.&#34;&gt;&lt;/p&gt;
&lt;p&gt;When working with large CSV files, I find it useful to get a feel for it in the CLI with

&lt;a href=&#34;https://csvkit.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;csvkit&lt;/a&gt;. This is especially important when importing
with a SAS data step, because we need to know the number of columns and their order, amongst other things,
for our code. To get an overview of the total number of columns and their contents, run&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;csvcut -n Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is fairly long, so you may prefer piping to a pager. I don&amp;rsquo;t need all the different identifiers
in the file, so I&amp;rsquo;m going to exclude those I won&amp;rsquo;t need and put them into a separate, smaller CSV.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ignore these four columns which I won&#39;t need
csvcut -C RegionID,SizeRank,RegionType,StateName Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv &amp;gt; Zip_zhvi_small.csv
# alternatively, also cut down on date columns to only 2022 for debugging 
csvcut -C RegionID,SizeRank,RegionType,StateName,10-273 Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv &amp;gt; Zip_zhvi_small.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also reduce the file size by using &lt;code&gt;csvgrep&lt;/code&gt; to filter any of the columns. For example, if we only wanted
the data for North Carolina we could run &lt;code&gt;csvgrep -c State -m NC&lt;/code&gt; in the pipe.&lt;/p&gt;
&lt;p&gt;For SAS, we need to know the maximum length of string columns so we can allocate the appropriate length to the
corresponding SAS variables. This is easily done with the csvstat tool:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;csvcut -c Metro,City,CountyName Zip_zhvi_small.csv | csvstat --len
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also specify the list of columns in csvstat directly, but in my experience that tends to be slower.&lt;/p&gt;
&lt;p&gt;Alright, now we have everything we need to start on our DATA step! We start with the attribute statement.
One problem with importing this file is that everyhing is in wide format, with the dates used as headers.
We will get around this shortly. I have seen people use transpose etc for similar problems online, but this
is unnecessary if we feel comfortable with the DATA step. We&amp;rsquo;ll start by naming the identifying columns
just as in the CSV file. For the date columns, we will use a numeric range prefixed by date (&lt;code&gt;date1-date270&lt;/code&gt;).
You can use csvcut to find the exact number of date columns you have. We will also allocate the same number of
columns for the ZHVI values, so we&amp;rsquo;ll need to add a &lt;code&gt;val1-val270&lt;/code&gt;. This and the date variable are temporary
and will be dropped later, in favor of the &lt;code&gt;Date&lt;/code&gt; and &lt;code&gt;ZHVI&lt;/code&gt; variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;attrib 
    ZIP           informat=best12.    format=z5.
    State         informat=$2.
    City          informat=$30.
    Metro         informat=$42.
    CountyName    informat=$29.
    date1-date270 informat=YYMMDD10.  format=DATE9.
    val1-val270   informat=best16.
    Date                              format=Date9.
    ZHVI                              format=Dollar16.
  ;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we will allocate an array to hold &lt;em&gt;all&lt;/em&gt; of the date and ZHVI values during the processing of each row.
Since the date column won&amp;rsquo;t change, we&amp;rsquo;ll tell SAS to retain its values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;   retain date1-date270;
   array d(270) date1-date270;
   array v(270) val1-val270;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is where the magic happens now. You may not know it, but you are not limited to a single INPUT statement
in a DATA step. We use this and start by reading in only the first row. Because we use an OUTPUT
statement later, this reading of row 1 will be processed, but not saved into the output data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;if _n_ = 1 then do;
  input ZIP $ State $ City $ Metro $ CountyName $ date1-date270;
  PUT _ALL_; /* if you want to see what that looks like */
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this if clause, the date1 through date270 variables will be populated, and because we used a retain
statement earlier, these values remain available to us during the processing of every other row. You can
probably guess where this is going now: we will process each row, and then OUTPUT one line per date which
we have access to now thanks to our array and the retain statement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;input ZIP $ State $ City $ Metro $ CountyName $ val1-val270;
do i=1 to 270;
  Date  = d(i); /* look up date for column i */
  ZHVI =  v(i); /* use the corresponding i-th value for ZHVI */
  OUTPUT;       /* This output creates one line per date column */
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the end of your data step, don&amp;rsquo;t forget to&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;drop i date1-date270 val1-val270;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;so those variables don&amp;rsquo;t clutter your data set. And that&amp;rsquo;s it! You now
have the data set loaded and available in SAS.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;SAS_data_set.png&#34; alt=&#34;&#34; title=&#34;The beginning of the resulting SAS data set.&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The INDSNAME Option in SAS</title>
      <link>https://dmsenter89.github.io/post/22-04-sas-indsname-option/</link>
      <pubDate>Wed, 20 Apr 2022 11:42:02 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-04-sas-indsname-option/</guid>
      <description>&lt;p&gt;I frequently find myself needing to concatenate data sets but also wanting to be able to distinguish
which row came from which data set originally. Introductory SAS courses tend to teach the &lt;code&gt;in&lt;/code&gt; keyword,
for a workflow similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data Concat1;
set data1(in = ds0)  
    data2(in = ds1);
if ds0 then source = &amp;quot;data1&amp;quot;;
else if ds1 then source = &amp;quot;data2&amp;quot;;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With more than two input data sets, this can get unwieldy and repetitive. In an old 
&lt;a href=&#34;https://blogs.sas.com/content/iml/2015/08/03/indsname-option.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;
on Rick Wicklin&amp;rsquo;s DO LOOP, a better method is introduced - the &lt;code&gt;indsname&lt;/code&gt; option. Using this method, the above code looks much nicer:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data Concat2;
set data1-data2 indsname = source;  /* the INDSNAME= option is on the SET statement */
libref = scan(source,1,&#39;.&#39;);        /* extract the libref */
dsname = scan(source,2,&#39;.&#39;);        /* extract the data set name */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As long as your input data sets are reasonably named, you&amp;rsquo;ll now have access to all the information needed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Working with the Census API Directly from SAS</title>
      <link>https://dmsenter89.github.io/post/22-04-census-api-with-sas/</link>
      <pubDate>Wed, 13 Apr 2022 08:27:35 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-04-census-api-with-sas/</guid>
      <description>&lt;p&gt;In a previous 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;post&lt;/a&gt;, I have shown how to connect to the Census API and load data
with Python. In this post, I will do the same using SAS instead. Before we get started, two important links
from last time: a guide to the API can be found 
&lt;a href=&#34;https://www.census.gov/data/developers/guidance/api-user-guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and a list of the
available data sets can be accessed 
&lt;a href=&#34;https://api.census.gov/data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;picking-the-data&#34;&gt;Picking the Data&lt;/h2&gt;
&lt;p&gt;For this post, I&amp;rsquo;ll use the same data as last time. There we used the 2018 American Community Survey 1-Year Detailed Table
and asked for three variables - total population, household income, and median monthly cost for Alamance and Orange
counties in North Carolina (FIPS codes 37001 and 37135). The variable names are not very intuitive, so I highly recommend starting
your code with a comment section that includes a markdown-style table of the variables that you want to use. Here is
an example table for our data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;B01003_001E&lt;/td&gt;
&lt;td&gt;Total Population&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B19001_001E&lt;/td&gt;
&lt;td&gt;Household Income (12 Month)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B25105_001E&lt;/td&gt;
&lt;td&gt;Median Monthly Housing Cost&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;building-the-query&#34;&gt;Building the Query&lt;/h2&gt;
&lt;p&gt;The next step is to build the query. Like last time, the API consists of a base
URL that points us to the data set we are looking for, a list of the variables
we want to request, and a description of the geography for which we want to
request those variables. Just like last time, I&amp;rsquo;ll build the query using several
macros for flexibility purposes. Note that since &lt;code&gt;&amp;amp;&lt;/code&gt; has a special meaning in SAS,
we need to use &lt;code&gt;%str(&amp;amp;)&lt;/code&gt; when referring to it to avoid having the log clobbered with
warnings about unresolved macros.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;%let baseurl=https://api.census.gov/data/2018/acs/acs1;
%let varlist=NAME,B01003_001E,B19001_001E,B25105_001E;
%let geolist=for=county:001,135%str(&amp;amp;)in=state:37;
%let fullurl=&amp;amp;baseurl.?get=&amp;amp;varlist.%str(&amp;amp;)&amp;amp;geolist.;
%put &amp;amp;=fullurl;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Your log should now show the full query URL:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FULLURL=https://api.census.gov/data/2018/acs/acs1?get=NAME,B01003_001E,B19001_001E,B25105_001E&amp;amp;for=county:001,135&amp;amp;in=state:37
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;making-the-api-request&#34;&gt;Making the API Request&lt;/h2&gt;
&lt;p&gt;The API call is achieved with a simple PROC HTTP call using a temporary file to hold the response from the server.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;filename response temp;

proc http url=&amp;quot;&amp;amp;fullurl.&amp;quot; method=&amp;quot;GET&amp;quot; out=response;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;handling-the-json-response&#34;&gt;Handling the JSON Response&lt;/h2&gt;
&lt;p&gt;We read the JSON response by utilizing the

&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lestmtsglobal/n1jfdetszx99ban1rl4zll6tej7j.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LIBNAME JSON Engine&lt;/a&gt;
in SAS.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;libname manual JSON fileref=response;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now run &lt;code&gt;proc datasets lib=manual; quit;&lt;/code&gt;. You&amp;rsquo;ll see two data sets that were created: ALLDATA which contains the whole JSON file&amp;rsquo;s contents
in a single data set, and ROOT which is a data set of all the root-level data. The latter one is the one we want. Here&amp;rsquo;s what the
first few observations in each look like:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-first-few-observations-in-the-automatically-created-data-sets&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/auto_datasets_hu58f00ccbf67d7d2f36e2c3ae0591a33b_44045_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;First few observations in the automatically created data sets.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/auto_datasets_hu58f00ccbf67d7d2f36e2c3ae0591a33b_44045_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1073&#34; height=&#34;490&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    First few observations in the automatically created data sets.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Just like with Python, all columns are treated as character variables at first. Because of the way the Census API is structured,
the first row consists of headers, which SAS didn&amp;rsquo;t use. This is something we&amp;rsquo;ll need to fix. At this point we have two main routes we can use to fix
these issues - we can manually create a new data set from ROOT with PROC SQL and address the issues in that way, or we can take
advantage of SAS&#39; JSON map feature to define how we want to load the JSON when the LIBNAME statement is executed. There are good use cases for each,
so I will show both methods.&lt;/p&gt;
&lt;h3 id=&#34;cleaning-up-via-proc-sql&#34;&gt;Cleaning up via PROC SQL&lt;/h3&gt;
&lt;p&gt;Using PROC SQL, you can rename all the character variables you want to keep. To change from character to numeric,
you&amp;rsquo;ll use the &lt;code&gt;input&lt;/code&gt; function. You can then assign formats and labels as desired. To get rid of the first row,
you can just add a conditional &lt;code&gt;having ordinal_root ne 1&lt;/code&gt; to avoid loading that line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;proc sql;
	create table census as
	select
		element1 as Name,
		input(element2, best12.) as B01003_001E format=COMMA12.  label=&#39;Total Population&#39;,
		input(element3, best12.) as B19001_001E format=DOLLAR12. label=&#39;Household Income (12 Month)&#39;,
		input(element4, best12.) as B25105_001E format=DOLLAR12. label=&#39;Median Monthly Housing Cost&#39;,
		element5 as state,
		element6 as county
	from manual.root
	having ordinal_root ne 1;
quit;
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-result-from-the-proc-sql-method&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/CensusData_SQL_hu13384c4c5502c575dbc7b9e51c49ebcb_20401_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Result from the PROC SQL method.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/CensusData_SQL_hu13384c4c5502c575dbc7b9e51c49ebcb_20401_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1022&#34; height=&#34;124&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Result from the PROC SQL method.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;A benefit of this method is that as you fix the input table, you can already begin to work with
it thanks to the &lt;code&gt;calculated&lt;/code&gt; keyword in PROC SQL. Say we weren&amp;rsquo;t actually interested in housing cost and
household income, but instead would like to know what percent of their annual income a household spends on
housing in a given county. We could just add a new variable to our PROC SQL call and build our table like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;proc sql;
	create table census as
	select
		element1 as Name,
		input(element2, best12.) as B01003_001E format=COMMA12. label=&#39;Total Population&#39;,
		input(element3, best12.) as B19001_001E format=DOLLAR12. label=&#39;Household Income (12 Month)&#39;,
		input(element4, best12.) as B25105_001E format=DOLLAR12. label=&#39;Median Monthly Housing Cost&#39;,
		/* Now calculate what we want from the new columns: */
		12*(calculated B25105_001E)/calculated B19001_001E as HousingCostPCT format=PERCENT10.2,
		element5 as state,
		element6 as county
	from manual.root
	having ordinal_root ne 1;
quit;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;using-a-json-map&#34;&gt;Using a JSON MAP&lt;/h3&gt;
&lt;p&gt;Alternatively, we could change the way SAS reads the JSON data by editing the JSON map it uses to decode
the JSON file. The first step is to ask SAS to create a map for us to edit:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;filename automap &amp;quot;sas.map&amp;quot;;
libname autodata JSON fileref=response map=automap automap=create;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The map will look something like this:





  
  











&lt;figure id=&#34;figure-beginning-of-the-automatically-created-json-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_hu793d98e625aa154a8d9815a25890d65f_22860_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Beginning of the automatically created JSON map.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_hu793d98e625aa154a8d9815a25890d65f_22860_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;460&#34; height=&#34;426&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Beginning of the automatically created JSON map.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Note that this is also a JSON file which you can edit in a text editor. With this map, you can change the names
of the data sets and variables, assign labels and formats, and also re-format incoming data. Variables and data sets
you don&amp;rsquo;t want to read can simply be deleted from the map. Here&amp;rsquo;s the beginning of my edited file:





  
  











&lt;figure id=&#34;figure-beginning-of-my-edited-json-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_EDITED_hua840053fe8872e6ad1fe911a8f4abee6_59487_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Beginning of my edited JSON map.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_EDITED_hua840053fe8872e6ad1fe911a8f4abee6_59487_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;475&#34; height=&#34;575&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Beginning of my edited JSON map.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Since the first row of observations in the JSON are actually a header and non-numeric, I add &lt;code&gt;?&lt;/code&gt; prior to the
specified informat. This prevents errors in the log and simply replaces non-matching variables with missing values.
We can now reload the JSON using our custom map by dropping the &lt;code&gt;automap=create&lt;/code&gt; option from the LIBNAME statement:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;libname autodata JSON fileref=response map=automap;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I now print the resulting data set, the header row is still there, but replaced by missing values in numeric
columns:





  
  











&lt;figure id=&#34;figure-the-data-set-as-a-result-of-the-edited-json-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/MAP_RESULT_hua2828dd6a29f70dd2548d0bd22c856b1_25610_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;The data set as a result of the edited JSON map.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/MAP_RESULT_hua2828dd6a29f70dd2548d0bd22c856b1_25610_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1026&#34; height=&#34;165&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The data set as a result of the edited JSON map.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This means we&amp;rsquo;ll need to additionally drop this row in a separate step using a delete statement either in
a PROC SQL or DATA step.&lt;/p&gt;
&lt;p&gt;Whichever method you choose, you now can access data via an API call from SAS. Happy exploring!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cleaning up a Date String with RegEx in SAS</title>
      <link>https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/</link>
      <pubDate>Wed, 29 Sep 2021 13:41:36 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/</guid>
      <description>&lt;p&gt;Sometimes we have to deal with manually entered data, which means there is a good chance that the data needs to be cleaned for consistency due to the
inevitable errors that creep in when typing in data, not to speak of any inconsistencies between individuals entering data.&lt;/p&gt;
&lt;p&gt;In my particular case, I was recently dealing with a data set that included
manually calculated ages that had been entered as a complete string
of the number of years, months, and days of an individual. Such a string
is not particularly useful for analysis and I wanted to have the age as
a numeric variable instead. Regular expressions can help out a lot in this
type of situation. In this post, we will look at a few representative examples
of the type of entries I&amp;rsquo;ve encountered and how to read them using RegEx in SAS.&lt;/p&gt;
&lt;h2 id=&#34;lets-look-at-the-data&#34;&gt;Let&amp;rsquo;s Look at the Data&lt;/h2&gt;





  
  











&lt;figure id=&#34;figure-what-were-starting-from&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/RAW_DS_hu83735545411e40d8d73c711ad73aa038_21697_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;What we&amp;amp;rsquo;re starting from.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/RAW_DS_hu83735545411e40d8d73c711ad73aa038_21697_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;508&#34; height=&#34;250&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    What we&amp;rsquo;re starting from.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;If we look at our sample data, we notice a few things. The data is consistently
ordered from largest to smallest, in the order of year, month, and day.
For some lines, only the year variable is available. In all cases, the string
starts with two digits.&lt;/p&gt;
&lt;p&gt;Separation of the time units is inconsistent; occasionally they are separated
by commas, sometimes by hyphens, and in some cases by spaces alone. The terms
indicating the units are spelled and capitalized inconsistently as well. There
are some abbreviations and occasionally the plural &amp;rsquo;s&#39; in days is wrapped in
parentheses.&lt;/p&gt;
&lt;p&gt;If you want to follow along, you can create the sample data with the
following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data raw;
    infile datalines delimiter = &#39;,&#39; MISSOVER DSD;
    attrib
        ID     informat=best32. format=1.
        STR_AGE informat=$500.   format=$500. label=&#39;Age String&#39;
        VAR1   informat=best32. format=1.;
    input ID STR_AGE $ VAR1;

    datalines;
    1,&amp;quot;62 Years, 5 Months, 8 Days&amp;quot;,1
    2,43 Yrs. -2 Months -4 Day(s), 2
    3,33 years * months 24 days, 1
    4,58,1
    5,&amp;quot;47 Yrs. -11 Months -27 Day(s)&amp;quot;,2
    ;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-regex-patterns&#34;&gt;The RegEx Patterns&lt;/h2&gt;
&lt;p&gt;We will use a total of three regex patterns, one for each of the time units:
year, month, day.  SAS uses Pearl regex and the function &lt;code&gt;prxparse&lt;/code&gt; to define
the regex patterns that are supposed to be searched for.&lt;/p&gt;
&lt;p&gt;For the year variable, we need to match the first two digits in our string.
Therefore, the correct call is &lt;code&gt;prxparse(&#39;/^(\d{2}).*/&#39;)&lt;/code&gt;. Note that the
&lt;code&gt;(&lt;/code&gt; and &lt;code&gt;)&lt;/code&gt; delimit the capture group.&lt;/p&gt;
&lt;p&gt;The month and day regex patterns are very similar. For the months, we want to
lazy-match the until we hit between one or two digits followed by
an &amp;rsquo;m&#39; and some number of other characters. We use the &lt;code&gt;i&lt;/code&gt; flag since
we cannot guarantee capitalization: &lt;code&gt;prxparse(&#39;/.*?(\d{1,2}).M.*/i&#39;)&lt;/code&gt;.
The day pattern is nearly identical: &lt;code&gt;prxparse(&#39;/.*?(\d{1,2}).D\D*$/i&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can extract our matches using the &lt;code&gt;prxposn&lt;/code&gt; function. We use the
&lt;code&gt;prxmatch&lt;/code&gt; function to check if we actually have a match:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;/* match into strings */
if prxmatch(year_rxid, STR_AGE)  then year_dig_str = prxposn(year_rxid,1,STR_AGE);
if prxmatch(month_rxid, STR_AGE) then month_dig_str = prxposn(month_rxid,1,STR_AGE);
if prxmatch(day_rxid, STR_AGE)   then day_dig_str = prxposn(day_rxid,1, STR_AGE);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The extracted strings can then be converted to numeric variables using
the &lt;code&gt;input&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;The last step is the calculation of the age from the three components.
Since not all three time units are specified for every row, we cannot use
the standard arithmetic of &lt;code&gt;years + months + days&lt;/code&gt;, because the missing
values would propagate. We need to use the &lt;code&gt;sum&lt;/code&gt; function instead.&lt;/p&gt;
&lt;p&gt;Putting it all together, we get the correct output:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-result&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/FIXED_DS_hu37f587f0a5cd00ab598beb8689b70b5f_34165_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;The Result&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/FIXED_DS_hu37f587f0a5cd00ab598beb8689b70b5f_34165_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;867&#34; height=&#34;251&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Result
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;complete-code&#34;&gt;Complete Code&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data fixed;
    set raw;
    
   /* define the regex patterns */
   year_rxid  = prxparse(&#39;/^(\d{2}).*/&#39;);
   month_rxid = prxparse(&#39;/.*?(\d{1,2}).M.*/i&#39;);
   day_rxid   = prxparse(&#39;/.*?(\d{1,2}).D\D*$/i&#39;);   /* match 2 digits followed by D and non-digit chars  */
  
   /* make sure we have enough space to store the extraction */
   length year_dig_str month_dig_str day_dig_str $4;
   
   /* match into strings */
   /* match into strings */
   if prxmatch(year_rxid, STR_AGE)  then year_dig_str = prxposn(year_rxid,1,STR_AGE);
   if prxmatch(month_rxid, STR_AGE) then month_dig_str = prxposn(month_rxid,1,STR_AGE);
   if prxmatch(day_rxid, STR_AGE)   then day_dig_str = prxposn(day_rxid,1, STR_AGE);
   
   /* use input to convert str -&amp;gt; numeric */
   years  = input(year_dig_str, ? 12.);
   months = input(month_dig_str, ? 12.);
   days   = input(day_dig_str, ? 12.);
   
   /* Use SUM function when calculating age
    to avoid missing values propagating  */
   age = sum(years,months/12,days/365.25);
   
   /* get rid of temporary variables */ 
   drop month_rxid month_dig_str year_rxid year_dig_str day_rxid day_dig_str;
   run;
   
proc print data=fixed; run;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>From Proc Import to a Data Step with Regex</title>
      <link>https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/</link>
      <pubDate>Thu, 29 Jul 2021 08:46:10 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/</guid>
      <description>&lt;p&gt;I find myself needing to import CSV files with a relatively large number of columns. In many cases, &lt;code&gt;proc import&lt;/code&gt; works surprisingly well in giving me what I want. But sometimes, I need to do some work while reading in the file and it would be nice to just use a data step to do so, but I don&amp;rsquo;t want to type it in by hand. That&amp;rsquo;s when a combination of &lt;code&gt;proc import&lt;/code&gt; and some regex substitution can come in handy.&lt;/p&gt;
&lt;p&gt;For the first step, run a &lt;code&gt;proc import&lt;/code&gt;, like this sample code that is provided by SAS Studio when you double click on a CSV file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;FILENAME REFFILE &#39;/path/to/file/data.csv&#39;;

PROC IMPORT DATAFILE=REFFILE
    DBMS=CSV
    OUT=WORK.IMPORT;
    GETNAMES=YES;
RUN;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you run this code, you will see that SAS generates a complete data step for you. This is what the beginning of one looks like:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-sample-log-output&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/log_hu417e5750fec5319adb043ca92305efb0_26856_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Sample log output.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/log_hu417e5750fec5319adb043ca92305efb0_26856_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;797&#34; height=&#34;461&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample log output.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;There will be be two lines for each variable, one giving the &lt;code&gt;informat&lt;/code&gt; and one giving the &lt;code&gt;format&lt;/code&gt; that SAS decided on. This will be followed by an &lt;code&gt;input&lt;/code&gt; statement. You can copy that from the log into a text editor such as VSCode, but unfortunately the line numbering of the LOG will carry over. One convenient way of fixing this is to use regex search-and-replace. Each line starts with a space followed by 1-3 digits, followed by a variable number of spaces until the next word. To capture this I use &lt;code&gt;^\s\d{1,3}\s+&lt;/code&gt; as my search term and replace with nothing. This will left align the whole data step, but this can be adjusted later.&lt;/p&gt;
&lt;p&gt;At this point the data step can be saved as a SAS file or copied back over to the file you are working within SAS Studio, but I like to do one more adjustment. I really like using the &lt;code&gt;attrib&lt;/code&gt; statement, 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lestmtsref/n1wxb7p9jkxycin16lz2db7idbnt.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see documentation&lt;/a&gt;, because it allows me to see the informat, format, and label of a variable all in one place. So I use regex to re-arrange my informat statement into the beginnings of an attribute statement. Use the search term &lt;code&gt;informat\s([^\s]+)\s([^\s]+)\s+;&lt;/code&gt; to capture each informat line and create two capture groups - the variable name as group 1 and the informat as group 2. If you use the replace code &lt;code&gt;$1 informat=$2 format=$2&lt;/code&gt;, you will see the beginnings of an attribute statement. In this replacement scheme, each informat matches each format. This is fine for date and character variables, but you may want to adjust the display format for some of your numeric variables.&lt;/p&gt;
&lt;p&gt;To clean this up, get rid of the format lines (you can search for &lt;code&gt;^format.+\n&lt;/code&gt; and replace with an empty replace to delete them), add the &lt;code&gt;attrib&lt;/code&gt; statement below the &lt;code&gt;infile&lt;/code&gt; and make sure to end the block of attributes with a semicolon, and indent your code as desired.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-sample-data-step-view&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/code_snip_hub5c78044ade6674b35a08b503d783f3d_19917_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Sample data step view.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/code_snip_hub5c78044ade6674b35a08b503d783f3d_19917_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;843&#34; height=&#34;190&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample data step view.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;And there you have it! The beginning of a nicely formatted data step that you can start to work with.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making INPUT and LABEL Statements with AWK</title>
      <link>https://dmsenter89.github.io/post/21-07-awk-for-sas/</link>
      <pubDate>Tue, 06 Jul 2021 10:38:27 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-07-awk-for-sas/</guid>
      <description>&lt;p&gt;I am currently working with a database provided by the North Carolina Department of Public Safety
that consists of several fixed-width files. Each of these has an associated codebook that gives the
internal variable name, a label of the variable, its data type, as well as the start column and
the length of the fields for each column. To import the data sets into SAS, I could copy and paste
part of that data into my INPUT and LABEL statements, but that gets tedious pretty fast when dealing
with dozens of lines. And since I have multiple data sets like that, I didn&amp;rsquo;t really want to do it that way.
In this post I show how a simple command-line script can be written to deal with this problem.&lt;/p&gt;
&lt;h2 id=&#34;introducing-awk&#34;&gt;Introducing AWK&lt;/h2&gt;
&lt;p&gt;Here are the first few lines of one of these files:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CMDORNUM      OFFENDER NC DOC ID NUMBER          CHAR      1       7     
CMCLBRTH      OFFENDER BIRTH DATE                DATE      8       10    
CMCLSEX       OFFENDER GENDER CODE               CHAR      18      30    
CMCLRACE      OFFENDER RACE CODE                 CHAR      48      30    
CMCLHITE      OFFENDER HEIGHT (IN INCHES)        CHAR      78      2     
CMWEIGHT      OFFENDER WEIGHT (IN LBS)           CHAR      80      3     
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the data is tabular and separated by multiple spaces. Linux programs often deal
with column data and a tool is available for manipulating column-based data on the command-line:
AWK, a program that can be used for complex text manipulation from the command-line. Some useful
tutorials on AWK in general are available at 
&lt;a href=&#34;https://www.grymoire.com/Unix/Awk.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;grymoire.com&lt;/a&gt;
and at 
&lt;a href=&#34;https://www.tutorialspoint.com/awk/index.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorialspoint&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For our purposes, we want to know about the &lt;code&gt;print&lt;/code&gt; and &lt;code&gt;printf&lt;/code&gt; commands for AWK. To illustrate
how this works, make a simple list of three lines with each term separated by a space:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cat &amp;lt;&amp;lt; EOF &amp;gt; list.txt
1 one apple pie
2 two orange cake
3 three banana shake
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To print the whole file, you&amp;rsquo;d use the print statement: &lt;code&gt;awk &#39;{print}&#39; list.txt&lt;/code&gt;. But I could do that with
&lt;code&gt;cat&lt;/code&gt;, so what&amp;rsquo;s the point? Well, what if I only want &lt;em&gt;one&lt;/em&gt; of the columns? By default, &lt;code&gt;$n&lt;/code&gt; refers to the
&lt;em&gt;n&lt;/em&gt;th column in AWK. So to print only the fruits I could write &lt;code&gt;awk &#39;{print $3}&#39; list.txt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Multiple columns can be printed by listing multiple columns separated by a comma:
&lt;code&gt;awk &#39;{print $2,$3}&#39; list.txt&lt;/code&gt;. Note that if you omit the comma the two columns get concatenated into
a single column.&lt;/p&gt;
&lt;p&gt;If additional formatting is required, we can use the &lt;code&gt;printf&lt;/code&gt; command. So to create a hyphenated
fruit and food-item column, we could use &lt;code&gt;awk &#39;{printf &amp;quot;%s-%s\n&amp;quot;, $3, $4}&#39; list.txt&lt;/code&gt;. Note that we
have to indicate the end-of line or else everything will be printed into a single line of text.&lt;/p&gt;
&lt;p&gt;Now we almost have all of the skills to create the label and input statements in SAS! Let&amp;rsquo;s create
a comma-delimited list for practice:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; list.txt
1,one,apple pie
2,two,orange cake
3,three,banana shake
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;-F&lt;/code&gt; flag is used to tell AWK to use a different column separator. So to print the
third column, we&amp;rsquo;d use &lt;code&gt;awk -F &#39;,&#39; &#39;{print $3}&#39; list.txt&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;making-the-sas-statements&#34;&gt;Making the SAS statements&lt;/h2&gt;
&lt;p&gt;Now we know everything we need to know about AWK to create code we want. First we note that
our coding file uses multiple spaces as column separators as opposed to single spaces. If
each item was a single word, this wouldn&amp;rsquo;t be a problem. Unfortunately, our second column
reads &amp;ldquo;OFFENDER NC DOC ID NUMBER&amp;rdquo; which would be split into five columns by default. So we
will need to use the column separator flag as &lt;code&gt;-F &#39;[[:space:]][[:space:]]+&#39;&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-label-statement&#34;&gt;The LABEL Statement&lt;/h3&gt;
&lt;p&gt;A SAS label has the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/v_011/lestmtsref/n1r8ub0jx34xfsn1ppcjfe0u16pc.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;general form&lt;/a&gt;
&lt;code&gt;LABEL variable-1=label-1&amp;lt;...variable-n=label-n&amp;gt;;&lt;/code&gt;, so for example&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;label score1=&amp;quot;Grade on April 1 Test&amp;quot;  
      score2=&amp;quot;Grade on May 1 Test&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is a valid label statement. In our file the variable names are given in column 1
and the appropriate labels in column 2. So an AWK script to print the appropriate
labels can be written like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;awk -F &#39;[[:space:]][[:space:]]+&#39; &#39;{printf &amp;quot;\t%s=\&amp;quot;%s\&amp;quot;\n&amp;quot;, $1, $2}&#39; FILE.DAT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what everything looks like given our code:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;label.PNG&#34; alt=&#34;Sample Code returned by AWK.&#34; title=&#34;Sample Code returned by AWK.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-input-statement&#34;&gt;The INPUT STATEMENT&lt;/h3&gt;
&lt;p&gt;The INPUT statement can be made in a similar way, it just requires some minor tweaking as
INPUT can be a bit more complex to handle a variety of data, see the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lestmtsref/n0oaql83drile0n141pdacojq97s.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;.
In our case we are dealing with a fixed-width record. The fourth column gives the starting column
of the data and the fifth gives us the width of that field. The third gives us the data type.
The majority of ours are character, so it seems easiest to just have the AWK script print each
line as though it were a character together with a SAS comment giving the name and &amp;ldquo;official&amp;rdquo; data
type. Then the few lines that need adjustment can be manually adjusted. The corresponding code would
look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;awk -F &#39;[[:space:]][[:space:]]+&#39; &#39;{printf &amp;quot;\t@%s %s $%s. /*%s - %s*/\n&amp;quot;,$4, $1, $5, $3, $2}&#39; FILE.DAT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what is returned by our code (highlighted part has been manually edited):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;input.PNG&#34; alt=&#34;Sample Code returned by AWK.&#34; title=&#34;Sample Code returned by AWK.&#34;&gt;&lt;/p&gt;
&lt;p&gt;I hope you all find this useful and that it will save you some typing!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SASPy Video Tutorial</title>
      <link>https://dmsenter89.github.io/post/21-06-youtube-tutorial/</link>
      <pubDate>Tue, 29 Jun 2021 10:57:05 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-06-youtube-tutorial/</guid>
      <description>&lt;p&gt;I have been using both SAS and Python extensively for a while now. With each having great features, it was very useful to combine my
skills in both languages by seamlessly moving between SAS and Python in
a single notebook. In the video below, fellow SAS intern Ariel Chien and I show how easy it is to connect the SAS and Python kernels using the open-source SASPy package together with SAS OnDemand for Academics.
I hope you will also find that this adds to your workflow!&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/6mcsbeKwSqM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The Jupyter notebook from the video can be viewed 
&lt;a href=&#34;https://github.com/sascommunities/sas-howto-tutorials/tree/master/sastopython&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;on GitHub&lt;/a&gt;. For installation instructions, check out the 
&lt;a href=&#34;https://github.com/sassoftware/saspy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SASPy GitHub page&lt;/a&gt;. Configuration for SASPy to connect to ODA can be found 
&lt;a href=&#34;https://support.sas.com/ondemand/saspy.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;at this support page&lt;/a&gt;. For more information on SAS OnDemand for Academics, 
&lt;a href=&#34;https://www.sas.com/en_us/software/on-demand-for-academics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Census 2020 Population Estimates Updated</title>
      <link>https://dmsenter89.github.io/post/21-06-covid-county-incidence/</link>
      <pubDate>Wed, 09 Jun 2021 16:00:34 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-06-covid-county-incidence/</guid>
      <description>&lt;p&gt;The Census Bureau has updated its population estimates for 2020 with county level data. This means any
projects that have had to rely on the 2019 estimates can now switch to the 2020 estimates.&lt;/p&gt;
&lt;p&gt;This is particularly useful for those of us who have been trying to track the development of COVID-19. The
average incidence rates are typically rescaled to new cases per 100,000 people. Previous graphs and maps I
have created used the 2019 estimates. I have now updated my code for mapping North Carolina developments to
use the 2020 estimates.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-county-level-data-for-north-carolina-using-the-nyt-covid-data-set-date-set-to-june-8-2021&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-06-covid-county-incidence/nc_avg_incidence_08jun2021_hu8396d2a41a978826522d96cfe881f35d_68326_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;County level data for North Carolina using the NYT COVID data set. Date set to June 8, 2021.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-06-covid-county-incidence/nc_avg_incidence_08jun2021_hu8396d2a41a978826522d96cfe881f35d_68326_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    County level data for North Carolina using the NYT COVID data set. Date set to June 8, 2021.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Below this post is my code for loading the necessary data using SAS.
Note that I&amp;rsquo;m using a macro called &lt;code&gt;mystate&lt;/code&gt; that can be set to the statecode abbreviation of your choice.
The conditional &lt;code&gt;County ne 0&lt;/code&gt; is in the code because the county level CSV includes both the county data as
well as the totals for each state.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;
filename popdat url &#39;https://www2.census.gov/programs-surveys/popest/datasets/2010-2020/counties/totals/co-est2020-alldata.csv&#39;;

data censusdata;
	infile POPDAT delimiter=&#39;,&#39; MISSOVER DSD lrecl=32767 firstobs=2;
	informat SUMLEV REGION DIVISION State County best32.
                         STNAME $20. CTYNAME $35. 
		CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020 best32.;
	format SUMLEV REGION DIVISION STATE best32. COUNTY 5. STNAME $20. CTYNAME $35. 
		CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020 
		COMMA12. StateCode $2.;
	input SUMLEV REGION DIVISION STATE COUNTY STNAME $ CTYNAME $
                        CENSUS2010POP ESTIMATESBASE2010 
		POPESTIMATE2010-POPESTIMATE2020;

	if (State ne 0) and (State ne 72) then
		do;
			FIPS=put(State, Z2.);
			Statecode=fipstate(FIPS);

			if Statecode eq &amp;amp;mystate and County ne 0 then
				output;
		end;
	keep STNAME CTYNAME County FIPS Statecode Popestimate2020;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The media release can be 
&lt;a href=&#34;https://www.census.gov/newsroom/press-releases/2021/2020-vintage-population-estimates.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;viewed here&lt;/a&gt;. The county-level data set can be downloaded 
&lt;a href=&#34;https://www.census.gov/programs-surveys/popest/technical-documentation/research/evaluation-estimates/2020-evaluation-estimates/2010s-counties-total.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;at this page&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Git with SAS Studio</title>
      <link>https://dmsenter89.github.io/post/21-01-git-with-sas-studio/</link>
      <pubDate>Mon, 11 Jan 2021 14:42:50 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/21-01-git-with-sas-studio/</guid>
      <description>&lt;p&gt;Git is a widely used version control system that allows users to track their software
development in both public and private repositories. It is also increasingly used to store
data in text formats, see for example the 
&lt;a href=&#34;https://github.com/nytimes/covid-19-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New York Times COVID-19 data set&lt;/a&gt;.
This post will briefly demonstrate how to clone and pull updates from a GitHub repository
using the git functions that are built into SAS Studio.&lt;/p&gt;
&lt;p&gt;Git functionality has been built into SAS Studio for a little while, so there are actually
two slightly different iterations of the git functions. The examples in this post will use the versions
compatible with SAS Studio 3.8, which is the current version available at SAS OnDemand for Academics.
All git functions use the same prefix. In older versions such as SAS Studio 3.8 the prefix is &lt;code&gt;gitfn_&lt;/code&gt;,
which is followed by a git command such as &amp;ldquo;clone&amp;rdquo; or &amp;ldquo;pull&amp;rdquo;. In SAS Studio 5, the prefix has been
simplified to just &lt;code&gt;git_&lt;/code&gt;. Most git functions have the same name between the&lt;br&gt;
two versions, so that the only difference is the prefix. A complete table of the old and new
versions of the git functions is available 
&lt;a href=&#34;https://go.documentation.sas.com/?cdcId=pgmsascdc&amp;amp;cdcVersion=9.4_3.5&amp;amp;docsetId=lefunctionsref&amp;amp;docsetTarget=n1mlc3f9w9zh9fn13qswiq6hrta0.htm&amp;amp;locale=en#p0evl64wd2dljrn1l43t739qtwba&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in the documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We use the git functions by calling them in an otherwise empty DATA step. In other words, we use the
format&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
    /* use your git functions here */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;cloning-a-repo&#34;&gt;Cloning a Repo&lt;/h2&gt;
&lt;p&gt;To clone a repo from github we use &lt;code&gt;gitfn_clone&lt;/code&gt;. It takes two arguments -
the URL of the repository of interest and the path to an &lt;em&gt;empty&lt;/em&gt; folder. You can
have SAS create the folder for you by using &lt;code&gt;OPTIONS DLCREATEDIR&lt;/code&gt;. The basic
syntax for the clone is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
    rc = gitfn_clone (
     &amp;quot;&amp;amp;repoURL.&amp;quot;,    /* URL to repo */
     &amp;quot;&amp;amp;targetDIR.&amp;quot;); /* folder to put repo in */
    put rc=;         /* equals 0 if successful */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It doesn&amp;rsquo;t matter if the URL you use ends in &amp;ldquo;.git&amp;rdquo; or not. In other words, the
following two macros would both work the same:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;%LET repoURL=https://github.com/nytimes/covid-19-data;
/* works the same as */
%LET repoURL=https://github.com/nytimes/covid-19-data.git;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use password based authentication to pull in private repositories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
    rc = gitfn_clone (
     &amp;quot;&amp;amp;repoURL.&amp;quot;,   
     &amp;quot;&amp;amp;targetDIR.&amp;quot;,
     &amp;quot;&amp;amp;githubUSER.&amp;quot;,   /* your GitHub username */
     &amp;quot;&amp;amp;githubPASSW.&amp;quot;); /* your GitHub password */
    put rc=;         /* equals 0 if successful */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; GitHub is &lt;em&gt;deprecating&lt;/em&gt; 
&lt;a href=&#34;https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;password-based authentication&lt;/a&gt;; you will need to switch to OAuth authentication or SSH keys
if you are not already using them. To access a repository using an SSH key, use the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;                             
 rc = gitfn_clone(
  &amp;quot;&amp;amp;repoURL.&amp;quot;,
  &amp;quot;&amp;amp;targetDIR.&amp;quot;,
  &amp;quot;&amp;amp;sshUSER.&amp;quot;,
  &amp;quot;&amp;amp;sshPASSW.&amp;quot;,
  &amp;quot;&amp;amp;sshPUBkey.&amp;quot;,
  &amp;quot;&amp;amp;sshPRIVkey.&amp;quot;);
 put rc=;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;pull-ing-in-updates&#34;&gt;Pull-ing in Updates&lt;/h2&gt;
&lt;p&gt;It is just as easy to pull in updates to a local repository by using
&lt;code&gt;gitfn_pull(&amp;quot;&amp;amp;repoDIR.&amp;quot;)&lt;/code&gt;. This also works with SSH keys for private
repositories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
 rc = gitfn_pull(
  &amp;quot;&amp;amp;repoDIR.&amp;quot;,
  &amp;quot;&amp;amp;sshUSER.&amp;quot;,
  &amp;quot;&amp;amp;sshPASSW.&amp;quot;,
  &amp;quot;&amp;amp;sshPUBkey.&amp;quot;,
  &amp;quot;&amp;amp;sshPRIVkey.&amp;quot;);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;other-functions&#34;&gt;Other Functions&lt;/h2&gt;
&lt;p&gt;SAS also offers other built-in functions, such as &lt;code&gt;_diff&lt;/code&gt;, &lt;code&gt;_status&lt;/code&gt;, &lt;code&gt;_push&lt;/code&gt;,
&lt;code&gt;_commit&lt;/code&gt;, and others. For a complete list, see the SAS documentation 
&lt;a href=&#34;https://go.documentation.sas.com/?cdcId=pgmsascdc&amp;amp;cdcVersion=9.4_3.5&amp;amp;docsetId=lefunctionsref&amp;amp;docsetTarget=n1mlc3f9w9zh9fn13qswiq6hrta0.htm&amp;amp;locale=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
