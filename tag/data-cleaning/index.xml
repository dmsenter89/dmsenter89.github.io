<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data-cleaning | Michael&#39;s Site</title>
    <link>https://dmsenter89.github.io/tag/data-cleaning/</link>
      <atom:link href="https://dmsenter89.github.io/tag/data-cleaning/index.xml" rel="self" type="application/rss+xml" />
    <description>data-cleaning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 29 Sep 2021 13:41:36 -0400</lastBuildDate>
    <image>
      <url>https://dmsenter89.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>data-cleaning</title>
      <link>https://dmsenter89.github.io/tag/data-cleaning/</link>
    </image>
    
    <item>
      <title>Cleaning up a Date String with RegEx in SAS</title>
      <link>https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/</link>
      <pubDate>Wed, 29 Sep 2021 13:41:36 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/</guid>
      <description>&lt;p&gt;Sometimes we have to deal with manually entered data, which means there is a good chance that the data needs to be cleaned for consistency due to the
inevitable errors that creep in when typing in data, not to speak of any inconsistencies between individuals entering data.&lt;/p&gt;
&lt;p&gt;In my particular case, I was recently dealing with a data set that included
manually calculated ages that had been entered as a complete string
of the number of years, months, and days of an individual. Such a string
is not particularly useful for analysis and I wanted to have the age as
a numeric variable instead. Regular expressions can help out a lot in this
type of situation. In this post, we will look at a few representative examples
of the type of entries I&amp;rsquo;ve encountered and how to read them using RegEx in SAS.&lt;/p&gt;
&lt;h2 id=&#34;lets-look-at-the-data&#34;&gt;Let&amp;rsquo;s Look at the Data&lt;/h2&gt;





  
  











&lt;figure id=&#34;figure-what-were-starting-from&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/RAW_DS_hu83735545411e40d8d73c711ad73aa038_21697_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;What we&amp;amp;rsquo;re starting from.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/RAW_DS_hu83735545411e40d8d73c711ad73aa038_21697_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;508&#34; height=&#34;250&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    What we&amp;rsquo;re starting from.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;If we look at our sample data, we notice a few things. The data is consistently
ordered from largest to smallest, in the order of year, month, and day.
For some lines, only the year variable is available. In all cases, the string
starts with two digits.&lt;/p&gt;
&lt;p&gt;Separation of the time units is inconsistent; occasionally they are separated
by commas, sometimes by hyphens, and in some cases by spaces alone. The terms
indicating the units are spelled and capitalized inconsistently as well. There
are some abbreviations and occasionally the plural &amp;rsquo;s&#39; in days is wrapped in
parentheses.&lt;/p&gt;
&lt;p&gt;If you want to follow along, you can create the sample data with the
following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data raw;
    infile datalines delimiter = &#39;,&#39; MISSOVER DSD;
    attrib
        ID     informat=best32. format=1.
        STR_AGE informat=$500.   format=$500. label=&#39;Age String&#39;
        VAR1   informat=best32. format=1.;
    input ID STR_AGE $ VAR1;

    datalines;
    1,&amp;quot;62 Years, 5 Months, 8 Days&amp;quot;,1
    2,43 Yrs. -2 Months -4 Day(s), 2
    3,33 years * months 24 days, 1
    4,58,1
    5,&amp;quot;47 Yrs. -11 Months -27 Day(s)&amp;quot;,2
    ;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-regex-patterns&#34;&gt;The RegEx Patterns&lt;/h2&gt;
&lt;p&gt;We will use a total of three regex patterns, one for each of the time units:
year, month, day.  SAS uses Pearl regex and the function &lt;code&gt;prxparse&lt;/code&gt; to define
the regex patterns that are supposed to be searched for.&lt;/p&gt;
&lt;p&gt;For the year variable, we need to match the first two digits in our string.
Therefore, the correct call is &lt;code&gt;prxparse(&#39;/^(\d{2}).*/&#39;)&lt;/code&gt;. Note that the
&lt;code&gt;(&lt;/code&gt; and &lt;code&gt;)&lt;/code&gt; delimit the capture group.&lt;/p&gt;
&lt;p&gt;The month and day regex patterns are very similar. For the months, we want to
lazy-match the until we hit between one or two digits followed by
an &amp;rsquo;m&#39; and some number of other characters. We use the &lt;code&gt;i&lt;/code&gt; flag since
we cannot guarantee capitalization: &lt;code&gt;prxparse(&#39;/.*?(\d{1,2}).M.*/i&#39;)&lt;/code&gt;.
The day pattern is nearly identical: &lt;code&gt;prxparse(&#39;/.*?(\d{1,2}).D\D*$/i&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can extract our matches using the &lt;code&gt;prxposn&lt;/code&gt; function. We use the
&lt;code&gt;prxmatch&lt;/code&gt; function to check if we actually have a match:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;/* match into strings */
if prxmatch(year_rxid, STR_AGE)  then year_dig_str = prxposn(year_rxid,1,STR_AGE);
if prxmatch(month_rxid, STR_AGE) then month_dig_str = prxposn(month_rxid,1,STR_AGE);
if prxmatch(day_rxid, STR_AGE)   then day_dig_str = prxposn(day_rxid,1, STR_AGE);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The extracted strings can then be converted to numeric variables using
the &lt;code&gt;input&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;The last step is the calculation of the age from the three components.
Since not all three time units are specified for every row, we cannot use
the standard arithmetic of &lt;code&gt;years + months + days&lt;/code&gt;, because the missing
values would propagate. We need to use the &lt;code&gt;sum&lt;/code&gt; function instead.&lt;/p&gt;
&lt;p&gt;Putting it all together, we get the correct output:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-result&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/FIXED_DS_hu37f587f0a5cd00ab598beb8689b70b5f_34165_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;The Result&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/FIXED_DS_hu37f587f0a5cd00ab598beb8689b70b5f_34165_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;867&#34; height=&#34;251&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Result
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;complete-code&#34;&gt;Complete Code&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data fixed;
    set raw;
    
   /* define the regex patterns */
   year_rxid  = prxparse(&#39;/^(\d{2}).*/&#39;);
   month_rxid = prxparse(&#39;/.*?(\d{1,2}).M.*/i&#39;);
   day_rxid   = prxparse(&#39;/.*?(\d{1,2}).D\D*$/i&#39;);   /* match 2 digits followed by D and non-digit chars  */
  
   /* make sure we have enough space to store the extraction */
   length year_dig_str month_dig_str day_dig_str $4;
   
   /* match into strings */
   /* match into strings */
   if prxmatch(year_rxid, STR_AGE)  then year_dig_str = prxposn(year_rxid,1,STR_AGE);
   if prxmatch(month_rxid, STR_AGE) then month_dig_str = prxposn(month_rxid,1,STR_AGE);
   if prxmatch(day_rxid, STR_AGE)   then day_dig_str = prxposn(day_rxid,1, STR_AGE);
   
   /* use input to convert str -&amp;gt; numeric */
   years  = input(year_dig_str, ? 12.);
   months = input(month_dig_str, ? 12.);
   days   = input(day_dig_str, ? 12.);
   
   /* Use SUM function when calculating age
    to avoid missing values propagating  */
   age = sum(years,months/12,days/365.25);
   
   /* get rid of temporary variables */ 
   drop month_rxid month_dig_str year_rxid year_dig_str day_rxid day_dig_str;
   run;
   
proc print data=fixed; run;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>North Carolina Housing Data</title>
      <link>https://dmsenter89.github.io/post/20-11-north-carolina-housing/</link>
      <pubDate>Fri, 06 Nov 2020 10:10:01 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/20-11-north-carolina-housing/</guid>
      <description>&lt;p&gt;A popular beginners machine learning problem is the prediction of housing prices. A frequently used data set for this purpose uses housing prices in California along some additional  gathered through the 1990 Census. One such data set is available 
&lt;a href=&#34;https://www.kaggle.com/camnugent/california-housing-prices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; at Kaggle. Unfortunately, that data set is rather old. And I live in North Carolina, not California! So I figured I might as well create a new housing data set, but this time with more up-to-date information and using North Carolina as the state to be analyzed. One thing that may be interesting about North Carlina as compared to California is the position of major populations centers. In California, major population centers are near the beach, while major population centers in North Carolina are in the interior of the state. Both large citites and proximity to the beach tend to correlate with higher housing prices. In California, unlike in North Carolina, both of these go together.&lt;/p&gt;
&lt;p&gt;This post will describe the Kaggle data set with California housing prices and then walk you through how the relevant data can be acquired from the Census Bureau. I&amp;rsquo;ll also show how to clean the data. For those who just want to explore the complete data set, I have made it available for download &lt;a href=&#34;https://dmsenter89.github.io/files/NC_Housing_Prices_2018.csv&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#the-source-data-set&#34;&gt;The Source Data Set&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acquiring-the-census-data-set&#34;&gt;Acquiring the Census Data Set&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#census-variables&#34;&gt;Census Variables&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#geography-considerations&#34;&gt;Geography Considerations&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acquiring-location-data&#34;&gt;Acquiring Location Data&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-merge-with-geoid-matching&#34;&gt;Data Merge with GEOID Matching&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-cleaning&#34;&gt;Data Cleaning&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;the-source-data-set&#34;&gt;The Source Data Set&lt;/h2&gt;
&lt;p&gt;The geographic unit of the Kaggle data set is the Census block group, which means we will have several thousand data points for our analysis. For a good big-picture overview of Census geography divisions, see this 
&lt;a href=&#34;https://pitt.libguides.com/uscensus/understandinggeography&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt; from the University of Pittsburgh library. The data set&amp;rsquo;s ten columns contain geographic, housing, and Census information that can be broken down as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;geographic information
&lt;ul&gt;
&lt;li&gt;longitude&lt;/li&gt;
&lt;li&gt;latitude&lt;/li&gt;
&lt;li&gt;ocean proximity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;housing information
&lt;ul&gt;
&lt;li&gt;median age of homes&lt;/li&gt;
&lt;li&gt;median value of homes&lt;/li&gt;
&lt;li&gt;total number of rooms in area&lt;/li&gt;
&lt;li&gt;total number of bedrooms in the area&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Census information
&lt;ul&gt;
&lt;li&gt;population&lt;/li&gt;
&lt;li&gt;number of households&lt;/li&gt;
&lt;li&gt;median income&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most of these exist directly in the Census API data that we have 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;covered previously&lt;/a&gt;. The ocean proximity variable is a categorical giving approximate distance from the beach. My data set will not include this last categorical variable.&lt;/p&gt;
&lt;h2 id=&#34;acquiring-the-census-data-set&#34;&gt;Acquiring the Census Data Set&lt;/h2&gt;
&lt;h3 id=&#34;census-variables&#34;&gt;Census Variables&lt;/h3&gt;
&lt;p&gt;The first, and most time consuming aspect, is to figure out where the data we want is located. We know that the US has a decennial census, so accurate information is available every ten years at every level of geography that the Census Bureau tracks. Since it is currently a census year 2020 and the newest information hasn&amp;rsquo;t been tabulated yet, that means the last census count that is available is from 2010. While this is 20 years more current than the California set from 1990, it still seems a bit outdated. Luckily, since the introduction of the American Community Survey (ACS) we have annually updated information available - but not for every level of geography. Only the 5-year ACS average gives us census block-level information for the whole state, making it comparable to the Kaggle data set. The most recent of these is the 2018.&lt;/p&gt;
&lt;p&gt;I start by creating a data dictionary from the 
&lt;a href=&#34;https://api.census.gov/data/2018/acs/acs5/groups.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;groups&lt;/a&gt; and 
&lt;a href=&#34;https://api.census.gov/data/2018/acs/acs5/variables.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;variables&lt;/a&gt; pages of the &amp;ldquo;American Community Survey: 1-Year Estimates: Detailed Tables 5-Year&amp;rdquo; data set. Note that median home age is not directly available. Instead, we will use the median year structures were built to calculate the median home age. Our data dictionary also does not include any data for the longitude and latitude of each row. We will get that data separately.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dictionary = {
    &#39;B01001_001E&#39; : &amp;quot;population&amp;quot;,
    &#39;B11001_001E&#39; : &amp;quot;households&amp;quot;,
    &#39;B19013_001E&#39; : &amp;quot;median_income&amp;quot;, 
    &#39;B25077_001E&#39; : &amp;quot;median_house_value&amp;quot;,
    &#39;B25035_001E&#39; : &amp;quot;median_year_structure_built&amp;quot;,
    &#39;B25041_001E&#39; : &amp;quot;total_bedrooms&amp;quot;,
    &#39;B25017_001E&#39; : &amp;quot;total_rooms&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;geography-considerations&#34;&gt;Geography Considerations&lt;/h3&gt;
&lt;p&gt;The next step is figuring out exactly what level of geography we want. Our data set goes down to the Census block level at its most granular. Unfortunately, the Census API won&amp;rsquo;t let us pull the data for all the Census blocks in a state at once. Census tracts on the other hand can be acquired in one go. If we were to shortcut and use only tract data, this would be a pretty quick API call build:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;primary_geo = &amp;quot;tract:*&amp;quot;
secondary_geo = &amp;quot;state:37&amp;quot;
query = base_URL + &amp;quot;?get=&amp;quot; + &amp;quot;,&amp;quot;.join(data_dictionary.keys()) + f&amp;quot;&amp;amp;for={primary_geo}&amp;amp;in={secondary_geo}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But let&amp;rsquo;s try and do it for the Census blocks instead. This will require us to build a sequence of API calls that loops over a larger geographic area, say the different counties in the state, and pull in the respective census block data for that geographic unit. While the FIPS codes for the state counties are sorted alphabetically, they are not contiguous. A full listing of North Carolina county FIPS codes is availalbe 
&lt;a href=&#34;https://www.lib.ncsu.edu/gis/countyfips&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;from NCSU here&lt;/a&gt;. It appears to be that the county FIPS codes are three digits long, starting at &lt;code&gt;001&lt;/code&gt; and go up to &lt;code&gt;199&lt;/code&gt; in increments of 2, meaning only odd numbers are in the county set. So it looks like we will be using &lt;code&gt;range(1,200,2)&lt;/code&gt; with zero-padding to create the list of county FIPS codes. So we could use a loop similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vars_requested = &amp;quot;,&amp;quot;.join(data_dictionary.keys())

for i in range(1,200,2):
    geo_request = f&amp;quot;for=block%20group:*&amp;amp;in=state:37%20county:{i:03}&amp;quot;
    query = base_URL + f&amp;quot;?get={vars_requested}&amp;amp;{geo_request}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While practicing to write the appropriate API call, you may find it useful to give it frequent, quick tests using curl. If you are using Jupyter or IPython, you can use &lt;code&gt;!curl &amp;quot;{query}&amp;quot;&lt;/code&gt; to test your API query. Don&amp;rsquo;t forget the quotation marks, since the ampersand has special meaning in the shell. It may be helpful to test the output of your call at the county or city level with that reported on the 
&lt;a href=&#34;https://www.census.gov/quickfacts/fact/table/US/PST045219&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Census Quickfacts page&lt;/a&gt;, if your variable is listed there. This can help make sure you are pulling the data you actually want.&lt;/p&gt;
&lt;p&gt;Now that we have figured out the loop necessary for creation of the API calls, we can put everything together and create a list of Pandas DataFrames which we then concatenate to create our master list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import requests

# create the base-URL
host_name = &amp;quot;https://api.census.gov/data&amp;quot;
year = &amp;quot;2018&amp;quot;
dataset_name = &amp;quot;acs/acs5&amp;quot;
base_URL = f&amp;quot;{host_name}/{year}/{dataset_name}&amp;quot;

# build the api calls as a list
query_vars = base_URL + &amp;quot;?get=&amp;quot; + &amp;quot;,&amp;quot;.join(list(data_dictionary.keys()) + [&amp;quot;NAME&amp;quot;,&amp;quot;GEO_ID&amp;quot;])
api_calls = [query_vars + f&amp;quot;&amp;amp;for=block%20group:*&amp;amp;in=state:37%20county:{i:03}&amp;quot; for i in range(1,200,2) ]

# running the API calls will take a moment
rjson_list = [requests.get(call).json() for call in api_calls]

# create the data frame by concatenation
df_list = [pd.DataFrame(data[1:], columns=data[0]) for data in rjson_list]
df = pd.concat(df_list, ignore_index=True)

# save the raw output to disk
df.to_csv(&amp;quot;raw_census.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we have the data set! We do still have to address the issue of our values all being imported as strings as mentioned in my 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;Census API post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;acquiring-location-data&#34;&gt;Acquiring Location Data&lt;/h2&gt;
&lt;p&gt;As mentioned above, we are still missing information regarding the latitude and longitude of the different block groups. The Census Bureau makes a lot of geographically coded data available on its 
&lt;a href=&#34;https://tigerweb.geo.census.gov/tigerwebmain/TIGERweb_main.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TIGERweb&lt;/a&gt; page. You can interact with it both using a REST API and its web-interface. A page with map information exists 
&lt;a href=&#34;https://tigerweb.geo.census.gov/arcgis/rest/services/Generalized_ACS2018/Tracts_Blocks/MapServer/4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Dealing with shapefiles and the TIGERweb API can get a little complicated. Luckily, I know someone with expertise in GIS and shapefiles so we will be using a CSV file of the geographic data we need courtesy of 
&lt;a href=&#34;https://www.linkedin.com/in/summer-faircloth-652797137&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Summer Faircloth&lt;/a&gt;, a GIS intern at the North Carolina Department of Transportation. She downloaded the TIGER/Line Shapefiles for the 20189 ACS 
&lt;a href=&#34;https://catalog.data.gov/dataset/tiger-line-shapefile-2018-state-north-carolina-current-block-group-state-based&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Block Groups&lt;/a&gt; and 
&lt;a href=&#34;https://catalog.data.gov/dataset/tiger-line-shapefile-2018-state-north-carolina-current-census-tract-state-based&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Census Tracts&lt;/a&gt; and joined the data sets in ArcMap, from where she exported our CSV file, which is now &lt;a href=&#34;https://dmsenter89.github.io/files/BlockGroup_Tract2018.csv&#34; target=&#34;_blank&#34;&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t need all of the columns in the CSV file, so we will limit the import to the parts we need with the &lt;code&gt;usecols&lt;/code&gt; keyword.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&amp;quot;raw_census.csv&amp;quot;, dtype={})
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;shapedata = pd.read_csv(&amp;quot;BlockGroup_Tract2018.csv&amp;quot;, 
                        dtype={&amp;quot;GEOID&amp;quot;: str},
                        usecols=[&#39;GEOID&#39;,&#39;NAMELSAD&#39;,&#39;INTPTLAT&#39;,&#39;INTPTLON&#39;,&#39;NAMELSAD_1&#39;] )

shapedata = shapedata.rename(columns={&#39;INTPTLAT&#39; : &#39;latitude&#39;, &#39;INTPTLON&#39; : &#39;longitude&#39; })
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-merge-with-geoid-matching&#34;&gt;Data Merge with GEOID Matching&lt;/h2&gt;
&lt;p&gt;At this stage we have two data frames - the first consists of all the Census information sans the geographic coordinates of the block groups, and a second data set containing the block groups&#39; location. Both data sets contain a GEOID column that can be used for merging. The GEOID returned by the Census API includes additional information to the regular FIPS code based GEOID used in the TIGERweb system. For example, &amp;ldquo;1500000US370010204005&amp;rdquo; in the census data set is actually GEOID &amp;ldquo;370010204005&amp;rdquo; for purposes of the TIGERweb data set. We&amp;rsquo;ll use a string split to make our GEO_ID variable from the Census API compatible with the FIPS code based GEOID from the TIGERweb service.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;GEO_ID&amp;quot;] = df[&amp;quot;GEO_ID&amp;quot;].str.split(&#39;US&#39;).str[1]

df = df.merge(shapedata, left_on=&#39;GEO_ID&#39;, right_on=&amp;quot;GEOID&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-cleaning&#34;&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;Now that our data set has been assembled, we can work on cleaning up the merged data set. We have the following tasks left:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;convert column data types to numeric&lt;/li&gt;
&lt;li&gt;drop unnecessary columns&lt;/li&gt;
&lt;li&gt;rename columns&lt;/li&gt;
&lt;li&gt;handle missing values&lt;/li&gt;
&lt;li&gt;calculate median age of homes&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for col in data_dictionary.keys():
    if col not in [&amp;quot;NAME&amp;quot;, &amp;quot;GEO_ID&amp;quot;]:
        df[col] = pd.to_numeric(df[col])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To indicate missing values, the Census API returns a value of &amp;ldquo;-666666666&amp;rdquo; in numeric columns. As all of our variables - except for longitude - ought to be positive, we can use the &lt;code&gt;mask&lt;/code&gt; function to convert all negative values to missing. We&amp;rsquo;ll start by filtering out the string columns that are no longer necessary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# filter down to our numerical columns
keeps = list(data_dictionary.keys()) +[&amp;quot;latitude&amp;quot;, &amp;quot;longitude&amp;quot;]
df = df.filter(items=keeps)

# replace vals &amp;lt; 0 with missing
k = df.loc[:, df.columns != &#39;longitude&#39;]
k = k.mask(k &amp;lt; 0)
df.loc[:, df.columns != &#39;longitude&#39;] = k
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the missing values have been handled, we can go ahead and calculate our median home age.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.rename(columns=data_dictionary, inplace=True)
df[&amp;quot;housing_median_age&amp;quot;] = 2018 - df[&amp;quot;median_year_structure_built&amp;quot;]
df.drop(columns=&amp;quot;median_year_structure_built&amp;quot;, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we&amp;rsquo;re done! We will save our output data set to disk for future analysis in a different post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.to_csv(&amp;quot;NC_Housing_Prices_2018.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
