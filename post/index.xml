<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Michael&#39;s Site</title>
    <link>https://dmsenter89.github.io/post/</link>
      <atom:link href="https://dmsenter89.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 23 Dec 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dmsenter89.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://dmsenter89.github.io/post/</link>
    </image>
    
    <item>
      <title>Remote Hosted, Local Jupyter?!</title>
      <link>https://dmsenter89.github.io/post/24/12-remote-hosted-local-jupyter/</link>
      <pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/24/12-remote-hosted-local-jupyter/</guid>
      <description>&lt;p&gt;If you visit the 
&lt;a href=&#34;https://jupyter.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Jupyter website&lt;/a&gt; you&amp;rsquo;ll
encounter a bunch of &amp;ldquo;try it in your browser&amp;rdquo; buttons. If you&amp;rsquo;ve used Jupyter
for a decade or so like me, you probably have also been ignoring these buttons.
And if you have clicked on them, you might have been lead to a

&lt;a href=&#34;https://mybinder.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mybinder.org&lt;/a&gt;. Don&amp;rsquo;t get me wrong, mybinder is cool. It
creates a docker image that remote-hosts a live environment so that you can
share your interactive notebooks on the web. Cool stuff. But I just found
something better.&lt;/p&gt;
&lt;p&gt;Have you ever heard of 
&lt;a href=&#34;https://webassembly.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WebAssembly&lt;/a&gt;, or wasm as it&amp;rsquo;s
abbreviated? The elevator pitch for WebAssenmbly can be summarized as binary
format for a locally runnin, sandboxed JavaScript VM. The idea being that
computation can essentially be off-loaded from a remote server to the machine
running the browser that&amp;rsquo;s viewing the website. And you can now probably guess
where this is going&amp;hellip;&lt;/p&gt;
&lt;p&gt;Thanks to wasm, there&amp;rsquo;s now an online 
&lt;a href=&#34;https://jupyter.org/try-jupyter/lab/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JupyterLab
instance&lt;/a&gt; featuring a Python kernel and a
SQLite kernel. The Python kernel is powered by

&lt;a href=&#34;https://pyodide.org/en/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pyodide&lt;/a&gt;, a WASM port of CPython. While it&amp;rsquo;s
not 1-1 feature complete, it&amp;rsquo;s quite impressive already. Solve an ODE with
SciPy? Data analytics with Pandas? Visualization with Matplotlib? Fancy plots
with Bokeh? This instance has got you covered. The governing 
&lt;a href=&#34;https://github.com/jupyterlite&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JupyterLite
project&lt;/a&gt; is open-source and even includes
instructions on 
&lt;a href=&#34;https://jupyterlite.readthedocs.io/en/latest/quickstart/deploy.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;how to deploy JupyterLite on GitHub
Pages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;What&amp;rsquo;s wild to me is that yes, you can run this on a GitHub pages instance.
Because you don&amp;rsquo;t actually need a backend, only a static site server, since all
computations happen locally. You access your local files when you upload from a
local VM, so even though you&amp;rsquo;re &amp;ldquo;uploading&amp;rdquo; you don&amp;rsquo;t actually need to share
your data with the remote server. It&amp;rsquo;s quite impressive. Again, not everything
works 100% yet, but you can get surprisingly far with this setup. You can even
get a taste of it without leaving &lt;em&gt;this blog&lt;/em&gt; because you can embed a REPL
provided by JupyterLite&amp;rsquo;s demo instance as an iframe. Feel free to play with the
REPL below and checkout the JupyterLite project. It&amp;rsquo;s really cool.&lt;/p&gt;
&lt;iframe
  src=&#34;https://jupyterlite.github.io/demo/repl/index.html?kernel=python&amp;toolbar=1&#34;
  width=&#34;100%&#34;
  height=&#34;100%&#34;
&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Lotteries and Pascal&#39;s Mugging</title>
      <link>https://dmsenter89.github.io/post/24/12-lotteries-and-pascals-mugging/</link>
      <pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/24/12-lotteries-and-pascals-mugging/</guid>
      <description>&lt;p&gt;Most have heard of 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Pascal%27s_wager&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pascal&amp;rsquo;s wager&lt;/a&gt;, but have you heard of the thought experiment known as 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Pascal%27s_mugging&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pascal&amp;rsquo;s mugging&lt;/a&gt;? The mugging attempts to reframe the essence of the wager argument using only finite values, thereby getting around some standard objections to the wager argument.&lt;/p&gt;
&lt;h2 id=&#34;pascals-mugging&#34;&gt;Pascal&amp;rsquo;s Mugging&lt;/h2&gt;
&lt;p&gt;It appears the term was coined in a 
&lt;a href=&#34;https://web.archive.org/web/20241213091126/https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt; by Eliezer Yudkowsky and framed in terms of potential risks posed by AI tasked with solving a problem. Nick Bostrom retells the mugging as 
&lt;a href=&#34;https://doi.org/10.1093/analys/anp062&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a conversation&lt;/a&gt; between Pascal and a hypothetical extra-dimensional mugger. The second part of &amp;ldquo;deal&amp;rdquo; in the paper is a bit extreme, but the initial part &amp;ndash; offering a fixed cost of money &lt;em&gt;now&lt;/em&gt; for a low probability payoff &lt;em&gt;tomorrow&lt;/em&gt; &amp;ndash; reminded me a bit of one of my most frequently discussed posts from 2022 on 
&lt;a href=&#34;../../22-09-lottery/&#34;&gt;whether it makes sense to play lottery&lt;/a&gt;. To be fair, that was two posts rolled into one &amp;ndash; the first part is about how point estimates can be misleading, particularly for skewed distributions, and the second consisted of a mini-benchmark using a simple example calculation to make that point.&lt;/p&gt;
&lt;p&gt;The beginning of Bostrom&amp;rsquo;s version goes something like this: a mugger approaches Pascal and asks for his wallet. Unfortunately, the mugger forgot his weapon so Pascal is disinclined to acquiesce to his request. To still get the wallet, the mugger offers Pascal a deal: give the mugger the wallet anyways, valued at $x$ USD, and the next day the mugger will return and pay Pascal $N x$ USD in return. As the story progresses, $N$ gets larger. We obviously don&amp;rsquo;t just believe the mugger, so there is some (small) probability $p$ that the mugger will return with the promised reward. The idea behind the experiment is that if $N$ grows sufficiently large then for any non-zero $p$ the expected value of paying the mugger becomes positive. It then veers off talking about other utility issues to make the payout better, but this early part of the conversation is essentially equivalent to a lottery game. Pay for the ticket now in the hopes that come game night the right numbers show up and you&amp;rsquo;re rich.&lt;/p&gt;
&lt;p&gt;As stated, it would appear that it is reasonable for Pascal to pay the mugger and &amp;ndash; for a sufficiently large jackpot &amp;ndash; to play the lottery, even though intuitively it strikes us as the &amp;ldquo;wrong answer&amp;rdquo; given the low probability of winning. This got me thinking: can we steelman the case for playing the lottery by ignoring the magnitude of the win?&lt;/p&gt;
&lt;h2 id=&#34;lets-crunch-some-numbers&#34;&gt;Let&amp;rsquo;s Crunch Some Numbers&lt;/h2&gt;
&lt;p&gt;If we ignore the question of the exact magnitude of the lottery win, we can divide the event space into three possibilities &amp;ndash; we are either worse off (cost of buying ticket), win back the ticket cost only, or win more money than the ticket cost so that we have a net gain from playing. Working off of the published odds again as a shortcut, we wind up with the following probabilities:&lt;/p&gt;
&lt;p&gt;$$
\begin{cases} Pr(\text{loss}) = 24/25 \\&lt;br&gt;
Pr(\text{even}) = 1/38 \\&lt;br&gt;
Pr(\text{gain}) = 13/950
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll postulate that people don&amp;rsquo;t really care about just breaking even on the ticket, so from a decision point of view it probably makes sense to reduce this to a binomial problem: I either win or loose on each ticket, where loosing includes the case of breaking even. People usually buy a handful of tickets, call it $n$. So now I can write a random variable representing my number of winning tickets as&lt;/p&gt;
&lt;p&gt;$$
X \sim \mathrm{Bin}\left(n, \frac{13}{950} \right).
$$&lt;/p&gt;
&lt;p&gt;We already showed that within any reasonable lifetime, there is a vanishingly small chance you&amp;rsquo;ll become rich from playing the lottery. To steelman, we&amp;rsquo;ll say that the utility of playing the lottery comes not from the money won but from the fun of playing.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll start by asserting that winning is more fun than loosing. So how often can we loose and still have fun? We definitely don&amp;rsquo;t want to &lt;em&gt;always&lt;/em&gt; loose, so $x&amp;gt;0$ is required. Winning more often than we loose is unlikely by design of the lottery so that can be our upper bound. Our realistic expectation should then be something like $0 &amp;lt; x \leq \left \lfloor{n/2}\right \rfloor$. If $x$ is in this window, I&amp;rsquo;ll call it a &amp;ldquo;Good Game&amp;rdquo; of lottery.&lt;/p&gt;
&lt;p&gt;Realistically, you&amp;rsquo;re not going to buy 100+ lottery tickets. Let&amp;rsquo;s say an average person likely wouldn&amp;rsquo;t buy more than 20 tickets, which still feels like plenty. So how likely is it you&amp;rsquo;ll have a good game, given as a function of the number of lottery tickets purchased? Let&amp;rsquo;s do a quick DATA step and find out.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data lottery;
  p=13/950;

  do N=2 to 20;
    gg=cdf(&#39;Binomial&#39;, floor(N/2), p, N) - cdf(&#39;Binomial&#39;, 0, p, N);
    output;
  end;
; run;

proc sgplot data=lottery;
  scatter x=N y=gg;
  xaxis label=&#39;Number of Tickets&#39;;
  yaxis label=&#39;Probability of a Good Game&#39;;
run;
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/24/12-lotteries-and-pascals-mugging/gg_hu8a67ac3c7eccf0e3fa506f4b6959062b_12022_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/24/12-lotteries-and-pascals-mugging/gg_hu8a67ac3c7eccf0e3fa506f4b6959062b_12022_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;So in the best case scenario, where we buy 20 tickets, we only have a chance of approximately 0.24 of having a good time. In other words, even with the generous assumption you&amp;rsquo;d be happy to loose 19 games if you win 1 you&amp;rsquo;d still be disappointed most of the time. Given my experience, I&amp;rsquo;d say many people will probably purchase fewer than 20 tickets. Perhaps 2 to 6. In such cases, you&amp;rsquo;re still bound to be disappointed. I&amp;rsquo;d say even with this steelmanning, it doesn&amp;rsquo;t make sense. Instead of buying a lottery ticket, perhaps buy a coffee and a donut for similar cost but with a guaranteed happiness payoff.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Data Don&#39;t Speak for Themselves</title>
      <link>https://dmsenter89.github.io/post/24/11-the-data-dont-speak-for-themselves/</link>
      <pubDate>Mon, 25 Nov 2024 14:02:54 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/24/11-the-data-dont-speak-for-themselves/</guid>
      <description>&lt;p&gt;Ever heard someone say they were &amp;ldquo;letting the data speak for itself?&amp;rdquo; I
often encounter this phrase on the internet by someone claiming
not to be interpreting the data, but merely relaying facts. I don&amp;rsquo;t
believe that&amp;rsquo;s actually true in the sense that it&amp;rsquo;s typically meant.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a good minimal example I&amp;rsquo;ve used in a math modeling class before.
I got this table from slides in an epidemiology course where it was
presented in the context of racial disparities in health care. It shows rates
of infant mortality for two time points in two different racial groups
in the US. Here&amp;rsquo;s a reproduction of the table:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Year&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;White (Non-Hispanic)&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Black (Non-Hispanic)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1950&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;26.8&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;43.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1998&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;13.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This represents the number of infants who died per 1,000 live births, so lower is better. The first thing to note and credit is
that the numbers have been plummeting in both columns, a testament to
our improvements in infant care.&lt;/p&gt;
&lt;p&gt;Now, if all I&amp;rsquo;m trying to do is to say that there is a disparity because
the numbers don&amp;rsquo;t match, that&amp;rsquo;s true. But it also trivial and
uninteresting. I&amp;rsquo;m usually much more interested in patterns and trends.
Given that we have two different time points, it is reasonable to ask a
question like &amp;ldquo;are the disparities getting better or worse?&amp;rdquo; As it turns out, and
we did this exercise in class, the question happens to be a bit vague, so it&amp;rsquo;s&lt;br&gt;
easy to come up with different answers both in direction and
magnitude of the disparity. I&amp;rsquo;ll group a couple of examples each by
outcome.&lt;/p&gt;
&lt;p&gt;Say I want to see the data as saying the racial disparity is getting
bigger, i.e. the gaps are widening in some sense. To measure the
disparity, the US Office of Minority Health uses the ratio of the
non-Hispanic Black to the non-Hispanic White rates. That would give us
$\frac{43.9}{26.8} \approx 1.63$ for 1950 versus of
$\frac{13.8}{6} = 2.3$ for 1998. This would tell a story of the gap
widening. This would imply the gap has grown by about 40%.&lt;/p&gt;
&lt;p&gt;Alternatively, we could construct the relative percentage difference
between the Black and White rates, which would give
$\frac{43.9 - 26.8}{26.8} \approx 0.64$ for 1950 vs
$\frac{13.8 - 6}{6} = 1.3$ for 1998. This also tells a story of
disparities growing, but it appears even more alarming than using the
previous method &amp;ndash; the 1998 difference is about twice that of 1950!&lt;/p&gt;
&lt;p&gt;If I want to take the opposite route and see the disparities as
improving, I could compare the rates of improvement in the infant mortality rates.
I have two data points for each racial category, so I can fit a line to each and
compare the slopes. That would give us
$\frac{6 - 26.8}{1998 - 1950} \approx - 0.43$ for Whites and
$\frac{13.8 - 43.9}{1998 - 1950} \approx - 0.63$ for Blacks. If I choose
this metric, improvements have been substantial. The rates have gone down
for Blacks about 50% faster than for Whites.&lt;/p&gt;
&lt;p&gt;We could also work with the number of deaths more directly. In 1950, there were
$43.9 - 26.8 = 17.1$ excess infant deaths per 1,000 live births amongst
Blacks compared to Whites, while in 1998 there were only
$13.8 - 6 = 7.8$ excess deaths per 1,000 live births. This metric could
also be framed as a success &amp;ndash; thanks to improvements in disparities, we
have nearly 10 fewer Black infant deaths per 1,000 live births than we
would have had the disparities of the 1950s persisted.&lt;/p&gt;
&lt;p&gt;Anybody wanting to opine on the matter could calculate any of these
measures and claim they are fairly representing the data as they see
them. All of these choices reflect what in statistics would be called an
&lt;em&gt;estimand&lt;/em&gt; &amp;ndash; a particular, mathematically defined answer to a
question that we seek to infer from the data using an estimator. Each
estimand is related to the scientific question being asked, but makes it
more precise. One issue that arises is that because the scientific
question of interest is by necessity a bit vague, at least when offered
initially, there are multiple valid routes to go about answering it.
This leads to what Gelman termed the 
&lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Garden of Forking
Paths&amp;rdquo;&lt;/a&gt;
and the different possible answers we can get to our question from a
data set.&lt;/p&gt;
&lt;p&gt;Another issue to consider is proper conditioning on factors playing a
role in the outcome of interest. A relatable illustration of this is the
famous 
&lt;a href=&#34;https://www.pewresearch.org/social-trends/2023/03/01/the-enduring-grip-of-the-gender-pay-gap/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;80 cents on the
dollar&amp;rdquo;&lt;/a&gt;
line about the US gender pay gap. This is an overall measure of group
differences often raised in a political context. While this represents true
observable group differences in wages, it is often used to imply wage
discrimination. Because that&amp;rsquo;s a more interesting question from a policy perspective.
But wage discrimination is usually thought of as two
individuals with the same background characteristics, say qualifications
and personality traits indicative of productivity, and differing in only
one aspect of interest, say race, gender, or sexual orientation, having
meaningfully different wages. So to be able to infer discrimination, we
have to also collect and analyze the necessary covariates that can
reflect differences in qualifications and ability. Wages have high
variability and different industries have very different pay-bands which
are often stratified to some degree by educational achievement. And we
know there are large gender gaps in fields of study. Here&amp;rsquo;s an
interesting link to an overview of this issue by

&lt;a href=&#34;https://www.bankrate.com/loans/student-loans/top-paying-college-majors-gender-gap/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bankrate.&lt;/a&gt;
Out of the six majors listed with a median salary of $100,000 or more,
only one was even close to even in enrollment by gender. Without taking
differences such as these (and others) into account, the noted group difference
in and of itself is not particularly enlightening.&lt;/p&gt;
&lt;p&gt;Back to our example on infant mortality. Since this is a medical issue,
we can look at whether there are established 
&lt;a href=&#34;https://www.cdc.gov/maternal-infant-health/infant-mortality/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;risk
factors&lt;/a&gt;
for it whose distribution might differ between groups. Here is a list of
the top 5 risk factors identified by the CDC:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;birth defects,&lt;/li&gt;
&lt;li&gt;preterm birth and low birth weight,&lt;/li&gt;
&lt;li&gt;sudden infant death syndrome,&lt;/li&gt;
&lt;li&gt;unintentional injuries, and&lt;/li&gt;
&lt;li&gt;maternal pregnancy complications.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since each of these factors are believed to affect the infant mortality
rate, it would be appropriate to include them in an analysis. After all, it&amp;rsquo;s
not a given that they do not differ by racial groups and variation in
risk factors may explain some of the variation in the observed
difference in infant mortality rates. For example: Blacks have about

&lt;a href=&#34;https://www.marchofdimes.org/peristats/data?reg=99&amp;amp;top=4&amp;amp;stop=45&amp;amp;lev=1&amp;amp;slev=1&amp;amp;obj=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;twice the rate of low birth weight
infants&lt;/a&gt;
compared to Whites. Among pregnancy complications, 
&lt;a href=&#34;https://doi.org/10.1001/jama.2017.3439&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;preeclampsia is more
common among Blacks&lt;/a&gt; than
Whites. For other risk factors, like gestational diabetes, the 
&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/10.1111/j.1365-3016.2010.01140.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rates
are
similar.&lt;/a&gt;
Including these risk factors in an analysis could increase or decrease
the observed disparity, but would simultaneously provide a richer
picture and ultimately a better answer to our scientific question. This is particularly
true if we&amp;rsquo;re asking this question from a public health perspective to guide
funding allocations in an effort to alleviate disparaties.&lt;/p&gt;
&lt;p&gt;So all this to say that I don&amp;rsquo;t believe the data speak for themselves.
Analysts use data to tell a story. And I don&amp;rsquo;t mean that maliciously. You can
sincerely tell different stories with the same data. Since there are many ways of
reasoning with and analyzing data, openness is key. And part of that
openness is clarity on the estimands we choose, and what they imply
about the specific question we&amp;rsquo;re asking. This becomes especially
important when the stakes are high, such as in regulatory review of
clinical trials or when policy decisions are at stake. We&amp;rsquo;re currently
seeing a move to more transparency here, see for example 
&lt;a href=&#34;https://www.fda.gov/regulatory-information/search-fda-guidance-documents/e9r1-statistical-principles-clinical-trials-addendum-estimands-and-sensitivity-analysis-clinical&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here for the
E9(R1)
Addendum&lt;/a&gt;
on estimands and sensitivity analysis in clinical trials. There&amp;rsquo;s also a
relatively recent 
&lt;a href=&#34;https://www.routledge.com/Estimands-Estimators-and-Sensitivity-Analysis-in-Clinical-Trials/Mallinckrodt-Molenberghs-Lipkovich-Ratitch/p/book/9781032242620?srsltid=AfmBOor0NBn9COUmntJla4NGNZKI2pSg-7OeMi4R0Qlb4vMd76iYhYDh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;book by Mallinckrodt et
al&lt;/a&gt;
offering relevant examples from the clinical setting on picking and
justifying appropriate estimands.&lt;/p&gt;
&lt;p&gt;This also shows that it pays to take a second look at published data,
something I&amp;rsquo;ve recently become more interested in. See 
&lt;a href=&#34;https://www.sensible-med.com/p/the-value-of-reanalysis-of-a-clinical&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this
post&lt;/a&gt;
by John Mandrola and the topic of reanalyzing clinical trial data and
their perhaps surprising finding: in their admittedly small sample, 35%
of the reanalysis of &lt;em&gt;existing, published trial data&lt;/em&gt; lead to different
interpretations than the originally published article.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From p-Values to Bayes Factors</title>
      <link>https://dmsenter89.github.io/post/24/11-from-p-values-to-bayes-factor/</link>
      <pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/24/11-from-p-values-to-bayes-factor/</guid>
      <description>&lt;p&gt;Most medical papers featuring statistical analysis still utilize a hypothesis
testing framework. Data is collected, an analysis is run, a &lt;em&gt;p&lt;/em&gt;-value reported,
and &amp;ndash; if it is found to be below the magic threshold of 0.05 &amp;ndash; the finding is
declared &amp;ldquo;(statistically) significant.&amp;rdquo; The authors then suggest, with varying
degrees of explicitness, that their results support their preferred hypothesis,
or &amp;ndash; in some less modest cases &amp;ndash; may go as far as claiming to have found
&amp;ldquo;proof&amp;rdquo; their preferred hypothesis. Criticism of this methodology is old, and
will not be rehashed here. Suffice it to say that the &lt;em&gt;p&lt;/em&gt;-value is typically
constructed conditional on a strawman &amp;ldquo;null hypothesis&amp;rdquo; being true, and as such
doesn&amp;rsquo;t provide direct evidence concerning any specific alternative hypothesis
the researchers are actually interested in. What we&amp;rsquo;re left with is a general
desire to say something along the lines of &amp;ldquo;Based on the data I have collected,
I now have more/less reason to believe that my preferred hypothesis is correct.&amp;rdquo;
&lt;em&gt;P&lt;/em&gt;-values aside, there exists a measure to express this idea: it is often
called the Bayes factor.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-bayes-factor-and-its-bound&#34;&gt;The Bayes Factor and Its Bound&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start with the basic idea. We can frame the problem of how likely a
particular model $M$ is given data $D$ that we have collected by using Bayes
theorem:&lt;/p&gt;
&lt;p&gt;$$\Pr(M|D) = \frac{\Pr(D|M)\Pr(M)}{\Pr(D)}$$&lt;/p&gt;
&lt;p&gt;Suppose you have two competing models as explanations for your data,
$M_{1}$ and $M_{2}$. If you want to know if the data favors one model
over another, you could simply take that ratio:&lt;/p&gt;
&lt;p&gt;$$\frac{\Pr(M_{2}|D)}{\Pr(M_{1}|D)} = \underset{\text{ Bayes Factor}}{\underbrace{\frac{\Pr(D|M_{2})}{\Pr(D|M_{1})}}} \times \underset{\text{Prior Odds}}{\underbrace{\frac{\Pr(M_{2})}{\Pr(M_{1})}}}$$&lt;/p&gt;
&lt;p&gt;Which model is in the numerator or denominator can be chosen by convenience. As
written above, a larger value of this ratio favors $M_2$ over $M_1$, while small
values of the ratio favor $M_1$ over $M_2$.&lt;/p&gt;
&lt;p&gt;Substitute in your traditional notation for a null-hypothesis $H_{0}$
and alternative hypothesis $H_{1}$ and we can compare two different
Bayes factors, $\text{BF}_{10}$ which compares the alternative
hypothesis to the null, or $\text{BF}_{01}$ which compares the null to
the the alternative. For illustrative purposes, consider the case of&lt;/p&gt;
&lt;p&gt;$$\mathrm{BF}_{10} = \frac{1}{\mathrm{BF}_{01}} = 10$$&lt;/p&gt;
&lt;p&gt;We can interpret this to say that the data favor the alternative
hypothesis 10 to 1 compared to the null. In other words, bigger values
of $ \text{ BF }_{10} $ correspond to greater evidence in favor of $H_{1}$
over $H_{0}$, whereas smaller values of $\text{BF}_{01}$ favor $H_{1}$
over $H_{0}$.&lt;/p&gt;
&lt;p&gt;In general, constructing a Bayes factor requires modeling. What we &lt;em&gt;can&lt;/em&gt;
do without modeling, however, is provide reasonable bounds for the Bayes
factor - a minimum bound for $\text{BF}_{01}$ or &amp;ndash; equivalently &amp;ndash; a
maximum bound for $\text{BF}_{10}$. Benjamin and Berger (2019) recommend
a particularly simple upper bound for the Bayes factor that can be shown
to hold in a wide variety of situations:&lt;/p&gt;
&lt;p&gt;$$\text{ BF}_{10} \leq \text{ BFB } = \frac{1}{- ep\log(p)}$$&lt;/p&gt;
&lt;p&gt;This approximation is valid for $p &amp;lt; \frac{1}{e} \approx 0.367$.&lt;/p&gt;
&lt;h2 id=&#34;adding-bayes-factors-to-sas-output&#34;&gt;Adding Bayes Factors to SAS Output&lt;/h2&gt;
&lt;p&gt;There is an easy way to start adding such Bayes Factor bounds to your
existing SAS workflow. Did you know that you can convert any ODS output
table to a SAS data set? That way you can access any reported value for
later analysis. For our purposes, this means we can access any test statistic
or &lt;em&gt;p&lt;/em&gt;-value reported by SAS and use them to calculate the appropriate Bayes
factor bounds. I&amp;rsquo;ll show two simple examples.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say I want to employ a bound for the output of PROC TTEST. I can look up
the relevant table name in the SAS/STAT User&amp;rsquo;s Guide in the &amp;ldquo;ODS Table Names&amp;rdquo;
section under the PROCs &amp;ldquo;Details&amp;rdquo; &amp;ndash; in this case, I want to use the table named
&lt;code&gt;TTests&lt;/code&gt;. I can then save this table to a SAS data set using &lt;code&gt;ods output&lt;/code&gt;. To
save typing, we&amp;rsquo;ll use the &lt;code&gt;filename&lt;/code&gt; and &lt;code&gt;include&lt;/code&gt; statements to utilize the
available code for the Getting Started example in TTest:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;filename tgs url
  &#39;https://raw.githubusercontent.com/sassoftware/doc-supplement-statug/refs/heads/main/Examples/r-z/ttegs1.sas&#39;;
ods output TTests=res; /* this line exports the TTests table to &amp;quot;res&amp;quot; */
%include tgs;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at my new data set, I can see that the &lt;em&gt;p&lt;/em&gt;-value is saved to a variable
named &lt;code&gt;Probt&lt;/code&gt;. I can now use a DATA step to calculate both the BFB and the
reciprocal of it, the minimum Bayes factor:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data bayes;
  set res;
  BFB = 1/(-CONSTANT(&#39;E&#39;)*Probt*Log(Probt));
  BFmin = 1/BFB;
run;
proc print; run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I&amp;rsquo;m omitting the &lt;code&gt;data&lt;/code&gt; keyword from the PROC PRINT call to keep things concise.
This way, it automatically uses the last data set in use. If you run these two
code snippets, you&amp;rsquo;ll get the following table:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-bayes-factor-bounds-from-our-proc-ttest-example&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/24/11-from-p-values-to-bayes-factor/ttest_hu495a71d51f4100df1b1cbf22bbde0982_13685_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;The Bayes factor bounds from our PROC TTest example.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/24/11-from-p-values-to-bayes-factor/ttest_hu495a71d51f4100df1b1cbf22bbde0982_13685_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;737&#34; height=&#34;103&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Bayes factor bounds from our PROC TTest example.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This suggests that for our particular example, the most favorable
interpretation would favor the alternative hypothesis over the null by a
rate of about 5.4-to-1.&lt;/p&gt;
&lt;p&gt;Another common source for &lt;em&gt;p&lt;/em&gt;-values is regression output. Each parameter estimate
is accompanied by a &lt;em&gt;p&lt;/em&gt;-value. We can use the same procedure as above to look
up the relevant ODS table name and use &lt;code&gt;ODS OUTPUT&lt;/code&gt; to save that table for later
use:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;ods output ParameterEstimates=ParmEst;
proc reg data=sashelp.baseball;
   id name team league;
   model logSalary = nhits nbb yrmajor crhits;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This table contains a little more detail and a pretty large BFB, so I decided to
specify which variables I want to print and added a &lt;code&gt;FORMAT&lt;/code&gt; to the &lt;code&gt;PRINT&lt;/code&gt;
call.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data bayes;
  set ParmEst;
  BFB = 1/(-CONSTANT(&#39;E&#39;)*Probt*Log(Probt));
  BFmin = 1/BFB;
run;

proc print noobs;
  var Variable Label Estimate Probt BF:;
  format BF: COMMA14.2;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing that&amp;rsquo;s neat to notice here is that the &lt;em&gt;p&lt;/em&gt;-value is printed by SAS
using a special format. Since user&amp;rsquo;s are normally not interested in the &lt;em&gt;exact&lt;/em&gt;
value when it is less than $10^{-4}$ ODS just prints &amp;ldquo;&amp;lt;.0001.&amp;rdquo; This doesn&amp;rsquo;t mean
SAS doesn&amp;rsquo;t calculate the exact &lt;em&gt;p&lt;/em&gt;-value as we can see from the data set
produced with &lt;code&gt;ODS OUTPUT&lt;/code&gt;. It stores the actual numeric value, so the BFB
computation can proceed without issues. This is what it looks like:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-bayes-factor-bounds-from-our-proc-reg-example&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/24/11-from-p-values-to-bayes-factor/reg_hu8bfa6fceface903fe712ffe709e7ead3_58561_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;The Bayes factor bounds from our PROC REG example.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/24/11-from-p-values-to-bayes-factor/reg_hu8bfa6fceface903fe712ffe709e7ead3_58561_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1291&#34; height=&#34;354&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Bayes factor bounds from our PROC REG example.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;You can try these code snippets out yourself using 
&lt;a href=&#34;https://welcome.oda.sas.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS OnDemand for Academics&lt;/a&gt;
or 
&lt;a href=&#34;https://www.sas.com/en_us/software/viya-for-learners.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Viya for Learners&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The bound used in this post is one recommendation by Benjamin and Berger (2019)
to improve scientific result reporting during this time in which we&amp;rsquo;re slowly
trying to move away from &lt;em&gt;p&lt;/em&gt;-values. To learn more about alternative bounds and
the conditions in which they hold, I would recommend the very readable overview
of the subject of by Held and Ott (2018).&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Benjamin, D. J., and Berger, J. O. (2019), &amp;ldquo;Three Recommendations for Improving
the Use of p-Values,&amp;rdquo; &lt;em&gt;The American Statistician&lt;/em&gt;, ASA Website, 73, 186–191.
&lt;a href=&#34;https://doi.org/10.1080/00031305.2018.1543135&#34;&gt;https://doi.org/10.1080/00031305.2018.1543135&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Held, L., and Ott, M. (2018), &amp;ldquo;On p-Values and Bayes Factors,&amp;rdquo; &lt;em&gt;Annual Review of
Statistics and Its Application&lt;/em&gt;, Annual Reviews, 5, 393–419.
&lt;a href=&#34;https://doi.org/https://doi.org/10.1146/annurev-statistics-031017-100307&#34;&gt;https://doi.org/https://doi.org/10.1146/annurev-statistics-031017-100307&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;This doesn&amp;rsquo;t solve all problems with hypothesis testing. In
particular, see section 7.4 in BDA3 for limitations. You may also
enjoy the critique offered at

&lt;a href=&#34;https://datacolada.org/78&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DataColada&lt;/a&gt;. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Some Thoughts on Parenting</title>
      <link>https://dmsenter89.github.io/post/24/08-parenting/</link>
      <pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/24/08-parenting/</guid>
      <description>&lt;p&gt;I have recently returned to work from my paternity leave. I really enjoyed my
time with my youngest and am grateful to SAS for providing 8 weeks of paid
paternity leave &amp;ndash; a benefit that remains uncommon in the United States. This
precious time allowed me to bond with my youngest child and navigate the dynamic
world of parenting four children whose ages span from infancy to the teenage
years.&lt;/p&gt;
&lt;p&gt;The playgrounds and family events I attended with my kids this summer provided
me not only with a lot of joy, but also an opportunity to think about how
parenting differs between my native Germany and my current home in the U.S.
Overall, I&amp;rsquo;ve found that some of the cultural norms I&amp;rsquo;ll note below make
parenting more burdensome and difficult than they have to be. I&amp;rsquo;ve noticed
myself gradually adapting to some of these attitudes, despite trying to maintain
awareness of my own ideas and ideals about parenting. But there is good reason
to believe that most parenting choices do not permanently affect our children&amp;rsquo;s
future in the way we&amp;rsquo;d like to think, unless we parent somewhat far outside the
norm of our cultural environment.&lt;/p&gt;
&lt;p&gt;One of the cultural norms I find unhelpful I mostly encounter at American
playgrounds. I have observed a pattern of near-constant supervision, with
parents actively engaging with their children almost non-stop. It seems like many parents can&amp;rsquo;t
let 90 seconds pass without saying something to their child, whether that&amp;rsquo;s
praise, a word of caution, or instructions on how to play. It&amp;rsquo;s rare to see a
parent sitting back to chat with fellow adults while their children explore
independently &amp;ndash; unless they&amp;rsquo;ve arrived as a prearranged group. This approach
often seems odd to me and the other German expatriates I&amp;rsquo;ve spoken with.&lt;/p&gt;
&lt;p&gt;This may be partially attributed to America&amp;rsquo;s obsession with child safety,
particularly fears of child abduction. Parents follow their children closely,
avoiding any loss of visual contact or distances of more than a few feet. As a
personal example of this, I can think of an incident where my about 2-year old
playfully closed a store&amp;rsquo;s glass door with me just on the other side. Although
the surrounding area was clear and we had good visibility due to the glass door
and glass walls, my wife&amp;rsquo;s anxiety kicked in. What if someone ran up to snatch
our child away? This echoes a common American concern and other American moms
could related to her feelings. As an immigrant, this level of concern strikes me
as out of proportion, especially when considering actual child abduction
statistics. According to the National Center for Missing &amp;amp; Exploited Children,
there were 
&lt;a href=&#34;https://www.missingkids.org/content/dam/missingkids/pdfs/analysis-of-nonfamily-abductions-reported-to-ncmec-2016-2020.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;only 366 reported
cases&lt;/a&gt;
of non-family abductions over the five years from 2016-2020, with over half
involving someone known to the family. In the 0-5 age group, a mere 190 cases
were reported. While each case is undoubtedly tragic, we have to weigh the
actual risks to our children against the potential drawbacks of our attempts at
minimizing said risks. Given that the risk of pediatric stranger abduction is
lower than the risk of death by 
&lt;a href=&#34;https://static1.squarespace.com/static/65789e268a44340b2eca10cd/t/6680348ed61d9c427c5395a9/1719678094526/fact.JPG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pediatric vehicular
heatstroke&lt;/a&gt;,
it seems to me it&amp;rsquo;s not worth it to impede our children&amp;rsquo;s growth toward independence and
self-confidence in public by attempts to further minimize such a remote risk.&lt;/p&gt;
&lt;p&gt;Another observation I&amp;rsquo;ve made is the tendency to treat children of various ages
as if they were in the 7-12 age range. Toddlers face unrealistic expectations
regarding emotional regulation, partiularly in public, while teens often
appear to be coddled, delaying their progression into adulthood.&lt;/p&gt;
&lt;p&gt;Overall, parenting in the U.S. appears excessively child-centric. There&amp;rsquo;s a
prevailing cultural expectation to constantly entertain children, as if boredom
is a calamity to be avoided at all costs. But children are good at
entertaining themselves when we let them. The ultimate goal of parenting should
be to raise self-sufficient adults, not perpetual playmates.&lt;/p&gt;
&lt;p&gt;A book that I&amp;rsquo;ve found helpful on these cultural differences and that explores
alternative approaches is Michaeleen Doucleff’s &amp;ldquo;Hunt, Gather, Parent.&amp;rdquo; I find
her writing style a bit annoying, but the book&amp;rsquo;s insights are incredibly
valuable. Jeremy Kun&amp;rsquo;s blog provides 
&lt;a href=&#34;https://www.jeremykun.com/2024/04/01/unusual-tips-for-parenting-toddlers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an excellent
summary&lt;/a&gt;
of the key insights. I&amp;rsquo;ve condensed them below. See Jeremy&amp;rsquo;s blog for more details.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to think about your child and your role as a parent:
&lt;ol&gt;
&lt;li&gt;Your child &amp;ldquo;has no brain&amp;rdquo; (=is predictably irrational).&lt;/li&gt;
&lt;li&gt;Your job is to teach your child to think.&lt;/li&gt;
&lt;li&gt;Your child mirrors your energy.&lt;/li&gt;
&lt;li&gt;Speech is a stimulant.&lt;/li&gt;
&lt;li&gt;Children want to be like their parents.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Actionable advice:
&lt;ol&gt;
&lt;li&gt;Say less.&lt;/li&gt;
&lt;li&gt;Quiz you child on good behavior, and focus on bad outcomes.&lt;/li&gt;
&lt;li&gt;Do your own thing, and let your kid participate (but never force them).&lt;/li&gt;
&lt;li&gt;Use monsters and stories to drive values.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bryan Caplan&amp;rsquo;s book &amp;ldquo;Selfish Reasons to Have More Kids&amp;rdquo; offers another perspective
that can help alleviate the pressures of modern American parenting. Caplan in essence
argues that many parents today engage in intensive parenting practices that the
parents don&amp;rsquo;t enjoy (and sometimes, the kids don&amp;rsquo;t either) but that have little long-term
impact on children, while overlooking simpler, more effective strategies that foster
strong family bonds. He encourages parents to focus on what really matters and to
keep our worries and fears about our children&amp;rsquo;s future in check.&lt;/p&gt;
&lt;p&gt;Here are 5 takeways from Caplan&amp;rsquo;s book:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rethink Parental Investment:&lt;/strong&gt; Caplan suggests that parents often overestimate
the influence of intensive parenting on their children&amp;rsquo;s outcomes. He
encourages a more relaxed approach that doesn&amp;rsquo;t sacrifice parental happiness
for marginal gains in child development.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Genetics Play a Major Role:&lt;/strong&gt; The book emphasizes the significant impact of
genetics over upbringing. Caplan argues that nature has a stronger hand than
nurture in many aspects of a child&amp;rsquo;s future, such as intelligence and
personality traits.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enjoy Parenting More:&lt;/strong&gt; By worrying less about optimizing every aspect of their
children&amp;rsquo;s lives, parents can enjoy the experience of parenting more,
reducing stress and increasing the overall happiness of the family.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Long-term Relationship Building:&lt;/strong&gt; Caplan advises parents to focus on
cultivating a positive, long-lasting relationship with their children, as
this has a profound and enduring impact on both the parents&#39; and children&amp;rsquo;s
well-being.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consider Having More Kids:&lt;/strong&gt; With the understanding that parenting can be less
intensive and still very successful, Caplan encourages parents to consider
the benefits of having more children, such as the joys of a larger family and
the support siblings can provide to each other throughout their lives.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both books effectively challenge the conventional approach to modern American parenting
and offer valuable insights on creating a more relaxed, independent, and I would
argue ultimately healthier approach to raising future adults.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Programming Links - July</title>
      <link>https://dmsenter89.github.io/post/24/07-programming-links/</link>
      <pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/24/07-programming-links/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;From the 
&lt;a href=&#34;https://nullprogram.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;null program&lt;/a&gt;:
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://nullprogram.com/blog/2014/10/21/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Object Oriented C&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Skeeto&amp;rsquo;s 
&lt;a href=&#34;https://nullprogram.com/blog/2023/10/08/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C coding style&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://nullprogram.com/blog/2017/08/20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Portable Makefiles&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://salykova.github.io/matmul-cpu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beating NumPy&amp;rsquo;s Matrix Multiplication&lt;/a&gt; in C.&lt;/li&gt;
&lt;li&gt;A list of 
&lt;a href=&#34;https://www.pythonmorsels.com/cli-tools/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cli-tools&lt;/a&gt; provided by Python.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Get SSH Keys From Github</title>
      <link>https://dmsenter89.github.io/post/24/04-get-ssh-keys-from-github/</link>
      <pubDate>Mon, 22 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/24/04-get-ssh-keys-from-github/</guid>
      <description>&lt;p&gt;Did you know you can get your public SSH keys via GitHub? I recently installed
Ubuntu Server to VM. During the installation process, it asked for my GitHub
username and then populated the authorized_keys file with my public keys. That
was super nifty! My new install never needed to accept password-based logins
and I didn&amp;rsquo;t have to worry about onboarding different machines&#39; keys.
I was curious how it worked and it turns out you can do this directly yourself
via cURL:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl https://github.com/${GITUSERNAME}.keys &amp;gt;&amp;gt; ~/.ssh/authorized_keys
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same thing works if you have uploaded GPG keys for signing:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl https://github.com/${GITUSERNAME}.gpg
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The GitHub API also exposes this info, but then you&amp;rsquo;ll have to process the
returning JSON if you want to use it for your keys file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl https://api.github.com/users/${GITUSERNAME}/keys
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Neat!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New MI Feature: Flux Statistics</title>
      <link>https://dmsenter89.github.io/post/24/04-new-mi-feature-flux/</link>
      <pubDate>Thu, 18 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/24/04-new-mi-feature-flux/</guid>
      <description>&lt;p&gt;The 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_050/pgmsaswn/n1l6ng10yj6s1an1v0rt9nj79ktc.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Viya 2024.04 release&lt;/a&gt;
includes a brand new MI feature: new missing data statistics. An important
choice when building an imputation model is the selection of variables to be
included. One method to help in the variable selection process is the usage of
summary statistics such as influx and outflux, as proposed by 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/missing-data-pattern.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;van
Buuren&lt;/a&gt;. In his
words: &amp;ldquo;Influx and outflux are summaries of the missing data pattern intended to
aid in the construction of imputation models. Keeping everything else constant,
variables with high influx and outflux are preferred. Realize that outflux
indicates the potential (and not actual) contribution to impute other variables&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The MI statement now supports the new FLUX option. When specified, MI produces a table
including the influx, outflux, average inbound and outbound, and FICO statistics
along with a column indicating the percent of cases for which the particular
variable has been observed. When ODS graphics are turned on, MI additionally
produces a scatter plot of the variables&#39; influx and outflux. For details,
see the new section on 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_050/statug/statug_mi_details57.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Missing Data Statistics&lt;/a&gt; in the MI chapter of the SAS/STAT User&amp;rsquo;s Guide.&lt;/p&gt;
&lt;p&gt;One thing that&amp;rsquo;s cool about this new feature for all users, not just those interested
in multiple imputation, is the fact that this new feature allows you to get a
complete overview of the percent of observed/missing cases for &lt;em&gt;all&lt;/em&gt; variables &amp;mdash;
both character and numeric! Previously, you either needed to use

&lt;a href=&#34;https://blogs.sas.com/content/iml/2011/09/19/count-the-number-of-missing-values-for-each-variable.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;separately procedures&lt;/a&gt; for character and numeric variables, or 
&lt;a href=&#34;https://www.sas.com/content/dam/SAS/en_ca/User%20Group%20Presentations/TASS/Zdeb-MissingData.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;expend some work&lt;/a&gt; to get a macro written that creates a table
of both types of variables for you.&lt;/p&gt;
&lt;p&gt;With this new feature, you can simply use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;/* optional: creates output ds with PctObs and PctMiss vars */
ods output Flux=Flux;

/* sample code using the sashelp.heart data set */
proc mi data=sashelp.heart flux
      nimpute=0
      displaypattern=nomeans;
   class _character_;
   var _all_;
   fcs;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we can include all variables in our data set with &lt;code&gt;var _all_&lt;/code&gt;. If our
data set includes character variables, we need &lt;code&gt;class _character_&lt;/code&gt; to label all
character variables as classification variables. If you are only interested in
a subset of the variables, you can of course specify them here. We use the FCS
statement to accomodate classification variables and we set &lt;code&gt;nimpute=0&lt;/code&gt; since we
don&amp;rsquo;t actually want to create imputations, just view the missing data statistics.
The &lt;code&gt;ods output&lt;/code&gt; statement is completely optional. It creates a data set with
variables PctObs and PctMiss for every variable in the analysis that you could
then further process with PROC SQL or some other method.&lt;/p&gt;
&lt;p&gt;In this example, the table will look as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;flux-table.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;For a full walkthrough of this code, see the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_050/statug/statug_mi_examples19.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;new example&lt;/a&gt;
in the MI chapter of the SAS/STATS User&amp;rsquo;s Guide.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Observations on Obstetrics and EBM</title>
      <link>https://dmsenter89.github.io/post/24/04-obs-on-obstetrics-and-ebm/</link>
      <pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/24/04-obs-on-obstetrics-and-ebm/</guid>
      <description>&lt;p&gt;We live in the age of evidence-based medicine and an increasing willingness on the part of patients to review medical guidance and actively participate in their care. This includes such personal and emotional areas like pregnancy and birth. Some popular tools to help would-be parents  include Emily Oster&amp;rsquo;s famous &amp;ldquo;Expecting Better&amp;rdquo; and 
&lt;a href=&#34;https://evidencebasedbirth.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;evidencebasedbirth.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ve had our fourth child this year and each time we go through prenatal appointments, birth, and post-partum care I encounter some differences between recommendations I&amp;rsquo;ve found in the literature and what providers say or do. It sticks out to me more than other branches of medicine I&amp;rsquo;ve encountered, but that might just be a difference in the amount of experience I&amp;rsquo;ve had versus willingness to go on Google Scholar for a lit review due to personal nature of the topic. Either way, in this post I wanted to share some notes on two particular issues that I&amp;rsquo;ve encountered and what I&amp;rsquo;ve seen in the literature on those topics. Obviously I&amp;rsquo;m not a medical provider and this is not medical advice, just a literature review by a curious statistician.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#low-amniotic-fluid-volume-afv&#34;&gt;Low Amniotic Fluid Volume (AFV)&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#measurement-techniques&#34;&gt;Measurement Techniques&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#management&#34;&gt;Management&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#continuous-efm&#34;&gt;Continuous EFM&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#concluding-thoughts&#34;&gt;Concluding Thoughts&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;

&lt;h2 id=&#34;low-amniotic-fluid-volume-afv&#34;&gt;Low Amniotic Fluid Volume (AFV)&lt;/h2&gt;
&lt;p&gt;When your little baby is &lt;em&gt;in utero&lt;/em&gt; it is surrounded by 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Amniotic_fluid&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;amniotic fluid&lt;/a&gt;. A sufficient amount of amniotic fluid is important for the baby, as it provides a cushion and allows for nutrient and oxygen flow through the umbilical cord. When amniotic fluid levels are low, baby&amp;rsquo;s ability to receive nutrients and oxygen can be hindered. This is called oligohydramnios in the literature.&lt;/p&gt;
&lt;p&gt;It is important to know that we can divide cases of low amniotic fluid volume (AFV) into two broad categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the idiopathic, that is cases where no explanation for the low fluid level is known. it happens &amp;ldquo;on its own;&amp;rdquo; this is also called isolated oligohydramnios. This is the most common.&lt;/li&gt;
&lt;li&gt;low AFV as a side effect of another pregnancy complication. This can be further divided into maternal causes, i.e. a complication arising in the mother, the fetus, or having a placental cause.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See the 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/books/NBK562326/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StatPearls&lt;/a&gt; article for relevant citations on these categories. This post focuses on idiopathic oligohydramnios, or isolated oligohydramnios (IO).&lt;/p&gt;
&lt;h3 id=&#34;measurement-techniques&#34;&gt;Measurement Techniques&lt;/h3&gt;
&lt;p&gt;How is IO diagnosed? Accurately measuring AFV is difficult. The most accurate technique is invasive and involves injecting a dye into the amniotic cavity and sampling of the amniotic fluid to determine a dilution curve. This technique is accurate but impractical, so most AFV assessments during pregnancy are done via ultrasound measurements. Two techniques have been developed: the amniotic fluid index (AFI) and the maximum vertical pocket (MVP) technique, sometimes called single deepest pocket (SDP).&lt;/p&gt;
&lt;p&gt;For MVP, a sonogropher identifies the deepest pocket of amniotic fluid not including the umbilical cord or body parts and measures the length from the 12 o&amp;rsquo;clock to the 6 o&amp;rsquo;clock position. The normal range is 2-8 cm, with &amp;lt;2 cm diagnosed as oligohydramnios and &amp;gt;8 cm as polyhydramnios. For the AFI method, the uterus is divided into 4 quadrants and the MVP is obtained in each quadrant. A sum of less than 5 cm is diagnosed as oligohydramnios 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/books/NBK562326/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Keilman and Shanks 2022)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There is a long debate on which method is better, but a Cochrane review of five trials suggested providers use MVP as opposed to AFI due to an increased number labor inductions and Ceasarean deliveries among women screened using AFI, with no concomitant improvement in perinatal outcomes 
&lt;a href=&#34;https://doi.org/10.1002/14651858.CD006593.pub2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Nabhan and Abdelmoula 2008)&lt;/a&gt;. As of 2014, ACOG followed suit and recommends the MVP method (Landon et al. 2018). But a decade later many providers continue to rely on the AFI. That was my experience with our providers in fetal-maternal medicine at Wake Forest in Winston-Salem, for example.&lt;/p&gt;
&lt;p&gt;Unfortunately, both MVP and AFI are known to be poor predictors of actual AFV. One paper by 
&lt;a href=&#34;https://doi.org/10.1002/jum.15116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hughes et al. (2020)&lt;/a&gt; for example pits the two methods against each other on women with known reference AFV. Performance of the ultrasound measurements are stratified by the true AFV classified as low, normal or high. The AUC result for both methods are disappointingly low. The AUC for low and normal volumes is estimated as being between 0.53 and 0.59. For high AFV, minor improvement is seen with the AUC around 0.62 and 0.63. As a rule of thumb, an AUC of 0.5 is roughly equivalent to randomly selecting a test result, with a minimum acceptable AUC of 0.7 being suggested for a test to have utility (Hosmer et al. 2013).&lt;/p&gt;
&lt;p&gt;A further complication is that an abnormal AFV is relatively rare. I&amp;rsquo;ve found estimates anywhere from 0.5-8.0% of pregnancies being affected. So what is the diagnostic value of an ultrasound reading indicating low AFV? The sensitivity for low AFV is quite poor, with Hughes giving point estimates of 15% and 7.69% for AFI and MVP, respectively. For specificity, Hughes gives point estimates of 97.54% and 99.08% for AFI and MVP. Plug the paper&amp;rsquo;s values into Bayes&#39; theorem and prepare to be underwhelmed.&lt;/p&gt;
&lt;h3 id=&#34;management&#34;&gt;Management&lt;/h3&gt;
&lt;p&gt;In short, we have a relatively infrequent medical condition that is looked for using a crude test with a very high false positive rate. Even if we allow for retesting after a few days, results are not necessarily &amp;ldquo;reassuring&amp;rdquo; towards the end of pregnancy due to the natural decline of AFV this far along. A study by 
&lt;a href=&#34;https://doi.org/10.1016/0002-9378%2889%2990527-9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brace and Wolf (1989)&lt;/a&gt; estimates changes in amniotic fluid volume throughout pregnancy. What is interesting is that according to their measurements, AFV peaks between 32 and 34 weeks and then starts to decline. This paper is cited and reproduced in a few standard textbooks on obstetrics, so you would imagine providers are familiar with it.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-estimated-amniotic-fluid-at-various-stages-of-pregnancies-based-on-brace-and-wolf-1989-as-reproduced-in-landon-et-al-2020-note-that-average-afv-declines-near-term-at-all-percentiles&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/24/04-obs-on-obstetrics-and-ebm/AFV-per-week_hub3b60ce5d922ffa37186bd7ecfe3650e_429559_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Estimated amniotic fluid at various stages of pregnancies. Based on Brace and Wolf (1989) as reproduced in Landon et al. (2020). Note that average AFV declines near term at all percentiles.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/24/04-obs-on-obstetrics-and-ebm/AFV-per-week_hub3b60ce5d922ffa37186bd7ecfe3650e_429559_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1734&#34; height=&#34;1267&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Estimated amniotic fluid at various stages of pregnancies. Based on Brace and Wolf (1989) as reproduced in Landon et al. (2020). Note that average AFV declines near term at all percentiles.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This creates a risk if an AFV assessment is made after 34 weeks and the result is towards the low end. If a provider waits a few days, chances are that the reading will be similar or lower than the previous one for entirely natural causes. Note also the very high variation between readings as mentioned in Landon et al. (2020). Couple that with the recent trend of interpreting the 
&lt;a href=&#34;https://doi.org/10.1056/NEJMoa1800566&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ARRIVE study&lt;/a&gt; as demonstrating that there is no to minimal risk when inducing an early-term pregnancy (cf. 
&lt;a href=&#34;https://doi.org/10.1111/jmwh.12996&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Carmichael and Snowden (2019)&lt;/a&gt; for some good thoughts, and see the abstract of ARRIVE itself for selection criteria) and we have a recipe for otherwise unnecessary medical interventions. Even before ARRIVE, medical providers routinely felt that low AFV alone is a reason for offering inductions; see for example 
&lt;a href=&#34;https://doi.org/10.1080/14767050802559103&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Schwartz et al. (2009)&lt;/a&gt; and 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/books/NBK562326&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keilman and Shanks (2022)&lt;/a&gt; and many secondary citations in other papers on this topic. But induction is not as easy of a solution as it is made out to be. It can be more uncomfortable for the mother and rates of Ceasareans may be higher (though this is a bit disputed in the literature). Despite this, I&amp;rsquo;ve repeatedly encountered a borderline reading as being interpreted as indication for an expedited induction (within a day or less), even absent other issues or stress-tests.&lt;/p&gt;
&lt;p&gt;Just to reiterate, these comments are regarding &lt;em&gt;isolated oligohydramnios&lt;/em&gt;, that is, low AFV that is not a side-effect of some condition. Interventions for IO may be thought to prevent stillbirth in part due to an otherwise missed comorbidity such as fetal growth restriction, but the literature simply does not bear out the thought of this being accurate. Even for low AFV associated with a comorbidity, a relatively recent meta-analysis suggests basing practice decisions on the relevant comorbidity, and not the AFV results 
&lt;a href=&#34;https://doi.org/10.1002/uog.15929&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Rabie et al. 20217)&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;continuous-efm&#34;&gt;Continuous EFM&lt;/h2&gt;
&lt;p&gt;A quick summary of the issue is available from the abstract to 
&lt;a href=&#34;https://doi.org/10.1016/j.bpobgyn.2020.02.004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Knupp et al. (2020)&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Continuous electronic fetal monitoring (EFM) was first introduced commercially over 50 years ago with the hope of improving perinatal outcomes during labor. However, despite the increased use of EFM, definitive improvements in perinatal outcomes have not been demonstrated. Variance in tracing interpretation and intervention has led to increased rates of cesarean and operative vaginal deliveries and perhaps increased maternal and neonatal morbidity. Since its inception, several strategies have been developed in hopes of optimizing EFM and improving these outcomes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Providers remain stubbornly committed to EFM despite the limitations and potential risks, while the promised improvements always remain somewhere beyond the horizon. Ironically, Knupp et al. continue their paper by recommending bluetooth enabled remote EFM solutions and the ever-popular &amp;ldquo;machine learning&amp;rdquo; to finally realize those longed-for improvements.&lt;/p&gt;
&lt;p&gt;It seems hard to pick any particular study showing issues with EFM, because they are so common. This makes it somewhat difficult to understand why the use of continuous EFM remains so popular. For some references, see the aforementioned paper by 
&lt;a href=&#34;https://doi.org/10.1016/j.bpobgyn.2020.02.004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Knupp&lt;/a&gt;.
For a more colorful summary of the many issues with EFM, see 
&lt;a href=&#34;https://doi.org/10.1055/s-0038-1632404&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sartwelle and Johnston (2018)&lt;/a&gt;. They seem to hit the nail on the head when showing that routine continuous EFM may be junk-science unwilling to die.&lt;/p&gt;
&lt;p&gt;ACOG guidelines on EFM are 
&lt;a href=&#34;https://www.acog.org/clinical/clinical-guidance/practice-bulletin/articles/2009/07/intrapartum-fetal-heart-rate-monitoring-nomenclature-interpretation-and-general-management-principles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hidden behind a paywall&lt;/a&gt;, but judging by the 
&lt;a href=&#34;https://www.acog.org/womens-health/faqs/fetal-heart-rate-monitoring-during-labor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;public FAQ&amp;rsquo;s&lt;/a&gt; statement on fetal heart rate monitoring, it appears as though they do not recommend routine continuous EFM. The American Academy for Nursing does not recommend routine EFM, and does not recommend using a routine EFM as part of admission since it is linked to higher rates of continued EFM utilization.&lt;/p&gt;
&lt;p&gt;It seems to me that EFM is popular because it gives providers a sense of control. Continuous feedback gives the illusion of having immediate and continued access to a fetus&#39; wellbeing. But that is precisely why this technology is problematic. Using crude tools during foetal monitoring has been shown to lead to unnecessary interventions, all of which have associated risks. During birth, patients are in a particularly vulnerable state &amp;mdash; especially after prolonged labor &amp;mdash; and any indication of risk to a fetus is likely to invoke an emotional response, as opposed to a rational or scientific one. This in turn can lead to poor outcomes that could have otherwise been avoided.&lt;/p&gt;
&lt;h2 id=&#34;concluding-thoughts&#34;&gt;Concluding Thoughts&lt;/h2&gt;
&lt;p&gt;In my personal experience, obstestrics providers seem to be focus on narrow, short-term goals using outdated or unscientific methodology. Long-term outcomes seem less of a concern, as mother and baby are soon moved to another department after birth, so any subsequent negative outcomes aren&amp;rsquo;t &amp;ldquo;their problem&amp;rdquo; anymore. I don&amp;rsquo;t mean that this is done purposefully, but rather that the compartmentalization of hospital care and evaluation metrics for providers may inadvertently encourage this type of behavior. Additionally, I&amp;rsquo;ve been surprised several times at the datedness of a provider&amp;rsquo;s knowledge of the literature on a topic or what the result of a study was. In one case that readily comes to mind, a provider heavily pushed for a scheduled Ceasarean based on the results of a study she had collaborated on. Yet, when we asked for a reference and I then looked up the study, the results actually suggested the opposite &amp;mdash; expectant management &amp;mdash; for our particular case. So it&amp;rsquo;s definitely worth to review treatment suggestions with your provider as we continue moving away from paternalistic models of care to one where patient and provider collaborate on finding the best treatment.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Brace, R. A., and Wolf, E. J. (1989), “Normal amniotic fluid volume changes throughout pregnancy,” &lt;em&gt;American Journal of Obstetrics and Gynecology&lt;/em&gt;, Elsevier BV, 161, 382–388. &lt;a href=&#34;https://doi.org/10.1016/0002-9378(89)90527-9&#34;&gt;https://doi.org/10.1016/0002-9378(89)90527-9&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Carmichael, S. L., and Snowden, J. M. (2019), “The ARRIVE Trial: Interpretation from an Epidemiologic Perspective,” &lt;em&gt;Journal of Midwifery &amp;amp; Women’s Health&lt;/em&gt;, Wiley, 64, 657–663. &lt;a href=&#34;https://doi.org/10.1111/jmwh.12996&#34;&gt;https://doi.org/10.1111/jmwh.12996&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Grobman, W. A., Rice, M. M., Reddy, U. M., Tita, A. T. N., Silver, R. M., Mallett, G., Hill, K., Thom, E. A., El-Sayed, Y. Y., Perez-Delboy, A., Rouse, D. J., Saade, G. R., Boggess, K. A., Chauhan, S. P., Iams, J. D., Chien, E. K., Casey, B. M., Gibbs, R. S., Srinivas, S. K., Swamy, G. K., Simhan, H. N., and Macones, G. A. (2018), “Labor Induction versus Expectant Management in Low-Risk Nulliparous Women,” &lt;em&gt;New England Journal of Medicine&lt;/em&gt;, 379, 513–523. &lt;a href=&#34;https://doi.org/10.1056/NEJMoa1800566&#34;&gt;https://doi.org/10.1056/NEJMoa1800566&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hughes, D. S., Magann, E. F., Whittington, J. R., Wendel, M. P., Sandlin, A. T., and Ounpraseuth, S. T. (2019), “Accuracy of the Ultrasound Estimate of the Amniotic Fluid Volume (Amniotic Fluid Index and Single Deepest Pocket) to Identify Actual Low, Normal, and High Amniotic Fluid Volumes as Determined by Quantile Regression,” &lt;em&gt;Journal of Ultrasound in Medicine&lt;/em&gt;, Wiley, 39, 373–378. &lt;a href=&#34;https://doi.org/10.1002/jum.15116&#34;&gt;https://doi.org/10.1002/jum.15116&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Keilman, C., and Shanks, A. L. (2022), “Oligohydramnios,” in StatPearls [Internet], Treasure Island, FL: StatPearls Publishing. Available at &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/books/NBK562326/&#34;&gt;https://www.ncbi.nlm.nih.gov/books/NBK562326/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Knupp, R. J., Andrews, W. W., and Tita, A. T. N. (2020), “The future of electronic fetal monitoring,” &lt;em&gt;Best Practice &amp;amp; Research Clinical Obstetrics &amp;amp; Gynaecology&lt;/em&gt;, Elsevier BV, 67, 44–52. &lt;a href=&#34;https://doi.org/10.1016/j.bpobgyn.2020.02.004&#34;&gt;https://doi.org/10.1016/j.bpobgyn.2020.02.004&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Landon, M. B., Driscoll, D. A., Jauniaux, E. R. M., Galan, H. L., Grobman, W. A., and Berghella, V. (2018), &lt;em&gt;Gabbe’s Obstetrics Essentials: Normal and Problem Pregnancies&lt;/em&gt;, Philadelphia, PA: Elsevier, p. 496.&lt;/p&gt;
&lt;p&gt;Landon, M. B., Galan, H. L., Jauniaux, E. R. M., Driscoll, D. A., Berghella, V., Grobman, W. A., Kilpatrick, S. J., and Cahill, A. G. (2020), &lt;em&gt;Gabbe’s Obstetrics: Normal and Problem Pregnancies&lt;/em&gt;, Philadelphia, PA: Saunders, p. 1280.&lt;/p&gt;
&lt;p&gt;Levin, G., Rottenstreich, A., Tsur, A., Cahan, T., Shai, D., and Meyer, R. (2020), “Isolated oligohydramnios – should induction be offered after 36 weeks?,” &lt;em&gt;The Journal of Maternal-Fetal &amp;amp; Neonatal Medicine&lt;/em&gt;, Informa UK Limited, 35, 4507–4512. &lt;a href=&#34;https://doi.org/10.1080/14767058.2020.1852546&#34;&gt;https://doi.org/10.1080/14767058.2020.1852546&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Nabhan, A. F., and Abdelmoula, Y. A. (2008), “Amniotic fluid index versus single deepest vertical pocket as a screening test for preventing adverse pregnancy outcome,” &lt;em&gt;Cochrane Database of Systematic Reviews&lt;/em&gt;, Wiley, 2010. &lt;a href=&#34;https://doi.org/10.1002/14651858.cd006593.pub2&#34;&gt;https://doi.org/10.1002/14651858.cd006593.pub2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Patrelli, T. S., Gizzo, S., Cosmi, E., Carpano, M. G., Di Gangi, S., Pedrazzi, G., Piantelli, G., and Modena, A. B. (2012), “Maternal Hydration Therapy Improves the Quantity of Amniotic Fluid and the Pregnancy Outcome in Third‐Trimester Isolated Oligohydramnios: A Controlled Randomized Institutional Trial,” &lt;em&gt;Journal of Ultrasound in Medicine&lt;/em&gt;, Wiley, 31, 239–244. &lt;a href=&#34;https://doi.org/10.7863/jum.2012.31.2.239&#34;&gt;https://doi.org/10.7863/jum.2012.31.2.239&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Rabie, N., Magann, E., Steelman, S., and Ounpraseuth, S. (2017), “Oligohydramnios in complicated and uncomplicated pregnancy: a systematic review and meta-analysis,” &lt;em&gt;Ultrasound in Obstetrics &amp;amp; Gynecology&lt;/em&gt;, Wiley, 49, 442–449. &lt;a href=&#34;https://doi.org/10.1002/uog.15929&#34;&gt;https://doi.org/10.1002/uog.15929&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Sartwelle, T., and Johnston, J. (2018), “Continuous Electronic Fetal Monitoring during Labor: A Critique and a Reply to Contemporary Proponents,” &lt;em&gt;The Surgery Journal&lt;/em&gt;, Georg Thieme Verlag KG, 04, e23–e28. &lt;a href=&#34;https://doi.org/10.1055/s-0038-1632404&#34;&gt;https://doi.org/10.1055/s-0038-1632404&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Schwartz, N., Sweeting, R., Young, B. K., Schwartz, N., Sweeting, R., and Young, B. K. (2009), “Practice patterns in the management of isolated oligohydramnios: a survey of perinatologists,” &lt;em&gt;The Journal of Maternal-Fetal &amp;amp; Neonatal Medicine&lt;/em&gt;, Informa UK Limited, 22, 357–361. &lt;a href=&#34;https://doi.org/10.1080/14767050802559103&#34;&gt;https://doi.org/10.1080/14767050802559103&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Takeaways from &#39;On the uses and abuses of regression models&#39;</title>
      <link>https://dmsenter89.github.io/post/24/03-takeaways-uses-and-abuses-of-regression/</link>
      <pubDate>Tue, 19 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/24/03-takeaways-uses-and-abuses-of-regression/</guid>
      <description>&lt;p&gt;This weekend I found an interesting new preprint by Carlin and Moreno-Betancur
on arxiv titled 
&lt;a href=&#34;https://arxiv.org/abs/2309.06668&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;On the Uses and Abuses of Regression
Models&amp;rdquo;&lt;/a&gt; so I had to check it out.&lt;/p&gt;
&lt;p&gt;The article focuses on medical literature, where regressions &amp;ndash; even in my
experience &amp;ndash; often seem done almost automatically and then interpreted
depending on the the &lt;em&gt;desired&lt;/em&gt; question as opposed to with respect to model
construction. 
&lt;a href=&#34;https://doi.org/10.1080/07388940500339167&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Garbage can&amp;rdquo;
regressions&lt;/a&gt; to find &amp;ldquo;important risk
factors&amp;rdquo; abound, as do repeat fittings  of simple models in an attempt to
describe a joint distribution. One of my favorite examples to show in class of
the issues with the latter is a 2008 paper by 
&lt;a href=&#34;https://doi.org/10.1038/oby.2008.351&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wang et al.&lt;/a&gt; that to date has been cited more than
1,000 times. The topic of the paper is an analysis of NHANES data with the aim
of predicting the prevalence of obesity in the US. They desire to describe how
different subgroups of Americans, that is the different genders and ethnicities,
fare. Instead of fitting a joint model, they fit multiple linear models. This
leads to fun results in their Table 2, where &lt;em&gt;all&lt;/em&gt; Americans of &lt;em&gt;all&lt;/em&gt; races and
ethnicities will be obese by 2048, yet &lt;em&gt;all men&lt;/em&gt; won&amp;rsquo;t be obese until 2051.
Mexican-American men fare the best, as they escape being part of &lt;em&gt;all Americans&lt;/em&gt;
somehow and won&amp;rsquo;t reach 100% prevalence until 2126.&lt;/p&gt;
&lt;p&gt;Carlin and Moreno-Betancur describe these and other issues they encountered in
the literature. One I find notable is what they call the &amp;ldquo;true model myth.&amp;rdquo;
Essentially, the idea that that the &amp;ldquo;best&amp;rdquo; fitted model represents the data
generating process, ergo the coefficients are easily interpreted in a causal
manner so we can derive practice recommendations from these models without much
discussion. That is of course not accurate.&lt;/p&gt;
&lt;p&gt;Overall, Carlin proposes a simple classification scheme for the different
purposes of a regression analysis:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;descriptive:&amp;rdquo; characterise the distribution of a feature or health outcome
in a population,&lt;/li&gt;
&lt;li&gt;&amp;ldquo;predictive:&amp;rdquo; produce a model or algorithm for predicting future values given
certain predictors,&lt;/li&gt;
&lt;li&gt;&amp;ldquo;causal:&amp;rdquo; investigate the extent to which a health outcome in some population
would be different if a particular intervention were made.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since they understand the problem with the misuse and misinterpration of
regression to (at least partially) be due to a certain vagueness with respect to
the purpose for which the regression is fit, they propose a teaching framework
centered around these types of research questions. This is in opposition of the
more &amp;ldquo;traditional&amp;rdquo; focues on the &amp;ldquo;maths&amp;rdquo; of the problem. In other words, focus
more on using standard tools to answer specific questions, than learning how to
do simple problems &amp;ldquo;by hand&amp;rdquo; and then hoping that eventually people will figure
out the hard parts of applying the theory to real life on their own.&lt;/p&gt;
&lt;p&gt;Overall, I think the paper is a worthwhile read. It reminds me of two other
books I like that take a model-centric approach to teaching regression methos &amp;ndash;
McElreath&amp;rsquo;s 
&lt;a href=&#34;https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Statistical
Rethinking&amp;rdquo;&lt;/a&gt;
and 
&lt;a href=&#34;https://avehtari.github.io/ROS-Examples/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Regression and Other
Stories&amp;rdquo;&lt;/a&gt; by Gelman, Hill,
and Vehtari. Gelman also posted about this preprint on 
&lt;a href=&#34;https://statmodeling.stat.columbia.edu/2024/03/17/on-the-uses-and-abuses-of-regression-models-a-call-for-reform-of-statistical-practice-and-teaching-wed-appreciate-your-comments/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;his
blog&lt;/a&gt;.
Check out the discussion section there for other good insights and some
additional reading material on this topic.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Calling R From SAS</title>
      <link>https://dmsenter89.github.io/post/23-12-calling-r-from-sas/</link>
      <pubDate>Fri, 22 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-12-calling-r-from-sas/</guid>
      <description>&lt;p&gt;The statistics literature is filled with example code and sample data in R. Sometimes I
find myself wanting to work through some provided sample data and compare the output from
R with SAS code. In this post, I&amp;rsquo;ll show how to connect R and SAS so that you can load and
execute R code straight from within SAS.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;In order to use this feature, you will want to have both R and SAS/IML installed on the
same computer. Make sure both SAS and R are in your path. In order to call R code from
SAS, you will need to start SAS with the &lt;code&gt;rlang&lt;/code&gt; option. You can call SAS from the command
line with the &lt;code&gt;-rlang&lt;/code&gt; option or you can add the option in your &amp;ldquo;sasv9.cfg&amp;rdquo; file.&lt;/p&gt;
&lt;p&gt;Once SAS is started, you can verify that the setup worked by running&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc options option=rlang;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The log will list &lt;code&gt;RLANG&lt;/code&gt; if the option was specified. If you forgot to add the option
prior to startup, you&amp;rsquo;ll see &lt;code&gt;NORLANG&lt;/code&gt; in the log instead.&lt;/p&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;R code can be called from within IML via a submit statement. The basic structure
is this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc iml;
  submit / R;
    /* R code her */
  endsubmit;
quit;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this we can run R code from within SAS. But the real power comes from our
ability to move data between R and SAS. The following functions are available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ExportDatasetToR(&amp;quot;libname.dsname&amp;quot;, RDataFrame);&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ExportMatrixToR(IMLMatrix, RMatrix);&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ImportDataSetFromR(r-expr, &amp;quot;libname.dsname&amp;quot;)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ImportMatrixFromR(r-expr, IMLMatrix)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Parameters can be passed to R as well, similar to how parameters can be passed
from IML to SAS PROCs.&lt;/p&gt;
&lt;p&gt;For more details, see the SAS/IML manual.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automatic Suspend in Fedora 38</title>
      <link>https://dmsenter89.github.io/post/23-12-automatic-suspend-in-fedora-38/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-12-automatic-suspend-in-fedora-38/</guid>
      <description>&lt;p&gt;For a while now I&amp;rsquo;ve recycled an old iMac running Fedora Workstation as a simple homeserver. It&amp;rsquo;s been working well in the past, but just now with the EOL of Fedora 37 did I get around to updating from Fedora 36 to Fedora 38.&lt;/p&gt;
&lt;p&gt;Unfortuantely for my use case, there is an unannounced change in Gnome&amp;rsquo;s powersettings when moving to Fedora 38 from 37. See the 
&lt;a href=&#34;https://discussion.fedoraproject.org/t/gnome-suspends-after-15-minutes-of-user-inactivity-even-on-ac-power/79801&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fedora forum&lt;/a&gt; for a description of the issue and a suggested fix. Unforunately, for me at least, the suggested fix doesn&amp;rsquo;t work and my system would still suspend, even with it disabled via the suggested commands. I had to use gsettings directly from my user account in addition to the the linked fix to get it to work and persist.&lt;/p&gt;
&lt;p&gt;Only a couple of weeks after getting this fixed, that workstation suffered a hard drive failure that caused it to become unbootable. I have sinced scrapped that workstation and bought a 
&lt;a href=&#34;https://www.bee-link.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beelink&lt;/a&gt; mini-pc for about $200. I&amp;rsquo;m running Arch Linux on it.&lt;/p&gt;
&lt;p&gt;I had previously used Arch extensively as my daily driver and for various other machines, and never had problems with. My recent Fedora experience really drove me back to committing to Arch. Does Arch take longer to install and set up than Fedora? Sure. But it&amp;rsquo;s well documented and you actually understand the system you have and what packages are on it. It&amp;rsquo;s actually a transparent system, unlike the convenience-packaged distros with all their preinstalled stuff. You see so many people online worried about Arch not being &amp;ldquo;stable,&amp;rdquo; but in over ten years with different desktops and laptops running on Arch I haven&amp;rsquo;t had any major issues. Truth be told, I&amp;rsquo;ve had fewer issues with Arch than in the past five years with Ubuntu and Fedora systems.&lt;/p&gt;
&lt;p&gt;A particular example of something nice about Arch Linux: I have a ten year old workstation running on Arch that hadn&amp;rsquo;t booted in 4 years. The only difficulty in getting it up and running again was updating the archlinux-keyring for pacman. That didn&amp;rsquo;t take long and was well-documented. Since then, it&amp;rsquo;s humming along great with more up-to-date software than recent Ubuntu releases. Updating Fedora from 36 to 38 on the other hand was much more of a hassle. I had multiple reboots, hunting for what default packages changed, what changed without being documented, etc. My older Arch system got up and running in no time after scanning through the Arch Linux news for any required manual interventions that popped up in the past few years. Arch for the win.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reproducibility by Sharing Code</title>
      <link>https://dmsenter89.github.io/post/23-09-reproducibility-by-sharing-code/</link>
      <pubDate>Tue, 12 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-09-reproducibility-by-sharing-code/</guid>
      <description>&lt;p&gt;Whenever I speak with students, I emphasize the need to share as much code and
data as is feasible to enable reproduciblity. The fact that a large amount of
research is not reproducible is a big issue that has gotten a lot of traction in
the past two decades since Ioannidis published his influental paper. Given these
issues, it is important to try to minimize other sources of variation in the
process that could lead to reproducibility problems, such as choices in how to
conduct statistical tests or how data is prepared. The many variations of the
basic research problem is something Gelman has termed the

&lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/unpublished/forking.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;garden of forking paths&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This week I came across a paper by Ostropolets et al (2023) that really
exemplifies this. The short version is this: ask 54 researchers across 9 teams
to reproduce the cohort used in awell-documented paper in their field from the
same source data set and compare the outcomes to both the original paper and a
“master implementation” that was recreated with one of the original authors of
the reference paper. All researchers had access to the same tools and data set.&lt;/p&gt;
&lt;p&gt;The result? Substantial variations in the final data set that was selected. Only
four out of ten inclusion criteria fully aligned with the reference
implementation. Note that this is just a cohort selection problem; they did not
attempt to reproduce other steps from the paper.&lt;/p&gt;
&lt;p&gt;This goes to show how important it is to share source code in order to achieve
reproduciblity. If cohort selection had been done programmatically and the
source code shared, we would have greater assurances that future teams trying to
work with this data would be able to reproduce these findings and build on them.
As the paper puts it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In this regard, if we truly aspire to reproducible science, we should not hope
that good documentation is sufficient and tolerate optional sharing of code,
but rather make code sharing a hard requirement that can be complemented by
free text descriptions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;See citation below for the paper&amp;rsquo;s DOI. Gelman shared a PDF of this paper
on

&lt;a href=&#34;https://statmodeling.stat.columbia.edu/2023/09/06/forking-paths-in-medical-research-a-study-with-9-research-teams/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;his blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In a similar vein, he 
&lt;a href=&#34;https://statmodeling.stat.columbia.edu/2023/09/10/the-authors-of-research-papers-have-no-obligation-to-share-their-data-and-code-and-i-have-no-obligation-to-believe-anything-they-write/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shared&lt;/a&gt;
a paper by Menkveld et al. (2022) in which several teams attempt to reproduce
hypothesis tests based on the same data set, again leading to substantial
variations.&lt;/p&gt;
&lt;p&gt;A solution to this issue is regular sharing of both data and code, as much as is
feasible. In medical research in particular there are questions of
confidentiality that we need to be concerned with, but this shouldn’t stop us
from making de-identified data available either via a supplement or having them
saved and ready should a third-party request them. This of course requires some
coordination amongst the study authors. Too often I have seen researchers not be
organized enough to be able to recover the original data set or steps in
creating an analysis they themselves did when this was needed some years after
the original study. My advice for this is to keep all information to a
particular paper in one folder as you work on it, preferably together with the
paper draft themselves so it will be obvious to future-you what paper those source
files go with. When the study is done, just archive that folder and keep a cold
copy on an external drive as well as on a storage server. This is often provided
by the university free of charge. If multiple studies use the same data set,
you&amp;rsquo;ll end up with multiple copies of that data using this method, but in my
opinion that&amp;rsquo;s not a problem - storage is cheap these days, particularly cold
storage. And if there are concerns about data privacy with your university&amp;rsquo;s
storage, you can always encrpyt the archive prior to uploaded. GPG is your
friend here.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Ioannidis, J. P. A. (2005), &amp;ldquo;Why Most Published Research Findings Are False,&amp;rdquo;
&lt;em&gt;PLOS Medicine&lt;/em&gt;, 2(8):e123, DOI: 
&lt;a href=&#34;https://doi.org/10.1371/journal.pmed.0020124&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.1371/journal.pmed.0020124&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Menkveld, A. J. et al (2022), &amp;ldquo;Non-standard errors&amp;rdquo;, available online at 
&lt;a href=&#34;https://wrap.warwick.ac.uk/176566/1/WRAP-non-standard-errors-Kozhan-2023.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://wrap.warwick.ac.uk/176566/1/WRAP-non-standard-errors-Kozhan-2023.pd&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ostropolets, A. et al. (2023), &amp;ldquo;Reproducible variability: assessing investigator discordance across 9 research teams attempting to reproduce the same observational study,&amp;rdquo; &lt;em&gt;Journal of the American Medical Informatics Association&lt;/em&gt;, Oxford University Press (OUP), 30, 859–868. DOI:

&lt;a href=&#34;https://doi.org/10.1093/jamia/ocad009&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.1093/jamia/ocad009&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some Basic SQL Joins</title>
      <link>https://dmsenter89.github.io/post/23-09-basic-sql-joins/</link>
      <pubDate>Tue, 05 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-09-basic-sql-joins/</guid>
      <description>&lt;p&gt;A non-technical friend recently asked me for help with a merge problem. They had two separate data pulls of electronic medical records based on specific study parameters. The set of people in the database who fit the study parameters changed in between the data pulls, for example by having people age into our out of a study, or by having new diagnoses added to their records that cause them to either be newly included or excluded. Let&amp;rsquo;s call the older data set A and the newer data set B. The goal was to get all those entries from B that don&amp;rsquo;t also show up in A. The data sets were pulled by a staff data scientist at that company who, despite their title, said they couldn&amp;rsquo;t figure out how to remove those entries from B that were already in A. Barring any special circumstances, this is a fairly standard problem so let&amp;rsquo;s look at a couple of tools we could use to solve it.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with some made-up sample data:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-minimal-sample-data-for-demonstration-purposes&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/sample-data_huec327b38ea9381c4b4e186ea675e12fc_20885_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Minimal sample data for demonstration purposes.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/sample-data_huec327b38ea9381c4b4e186ea675e12fc_20885_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;474&#34; height=&#34;322&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Minimal sample data for demonstration purposes.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- ```shell
$ bat *.csv
───────┬────────────────────────────────────────────────────────────────────
       │ File: A.csv
───────┼────────────────────────────────────────────────────────────────────
   1   │ MRN,Weight,Chol_Status
   2   │ 23356,140,
   3   │ 74592,,Desirable
   4   │ 79602,139,High
───────┴────────────────────────────────────────────────────────────────────
───────┬────────────────────────────────────────────────────────────────────
       │ File: B.csv
───────┼────────────────────────────────────────────────────────────────────
   1   │ MRN,Weight,Chol_Status
   2   │ 64836,129,High
   3   │ 79602,139,High
   4   │ 2466,127,Borderline
───────┴────────────────────────────────────────────────────────────────────
``` --&gt;
&lt;p&gt;The MRN here stands for &amp;ldquo;medical record number,&amp;rdquo; a common unique identifier present in clinical data sets. Each of our data sets has three rows, but only one row is shared between both - that associated with MRN 79602. We could theoretically merge on multiple columns or coalesce data if we think some missing fields might have been updated in the meantime, but for purposes of this example we&amp;rsquo;ll keep it simple and just merge on the MRN.&lt;/p&gt;
&lt;h2 id=&#34;sql-merge-types&#34;&gt;SQL Merge Types&lt;/h2&gt;
&lt;p&gt;There are four basic types of merge: left join, right join, outer join, and inner join. There&amp;rsquo;s also the cross join but that one shows up less frequently in my experience. A picture speaks a thousand words, so here&amp;rsquo;s a Venn diagram illustrating the idea behind these joins.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-standard-sql-joins&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/sql-joins_hu519d137be5f72d83cc0fb24e7060e243_236726_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Standard SQL Joins.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/sql-joins_hu519d137be5f72d83cc0fb24e7060e243_236726_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2593&#34; height=&#34;1814&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Standard SQL Joins.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In our case, we actually want left/right &amp;ldquo;inner&amp;rdquo; or &amp;ldquo;exclusive&amp;rdquo; joins, like this:&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/exclusive-joins60_hu03ae2ec37b24cdee5a828b5b640777a8_45542_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-09-basic-sql-joins/exclusive-joins60_hu03ae2ec37b24cdee5a828b5b640777a8_45542_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;377&#34; height=&#34;514&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;implementations&#34;&gt;Implementations&lt;/h2&gt;
&lt;p&gt;I figured I would go over three basic tools: SAS, SQL, and Pandas.&lt;/p&gt;
&lt;h3 id=&#34;only-in-a&#34;&gt;Only in A&lt;/h3&gt;
&lt;p&gt;For starters, we want all entries in $A$ that are not also in $B$. In set notation that is the set denoted $A-B$ (sometimes $A\backslash B$).
Merges like this is what SQL excels at, so let&amp;rsquo;s see the SQL statment first:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table res1 as
  select A.* from
    A left join B 
    on A.MRN=B.MRN
    where B.MRN is NULL;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should run in any typical SQL implementation, including PROC SQL in SAS and SQLite3. We expect the following table as output:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;MRN&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;th&gt;Chol_Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;23356&lt;/td&gt;
&lt;td&gt;140&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;74592&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Desirable&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To do a left outer join instead, we would just omit the &lt;code&gt;where&lt;/code&gt; clause. We could do the same with a data step merge statement, but unlike SQL this would assume our input data sets are sorted by the merge key:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data res1;
  merge A (IN = X) B (IN=Y);
  by MRN;
  If X and not Y;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pandas&#39; merge statement allows for the creation of an indicator variable, similar to the &lt;code&gt;in&lt;/code&gt; keyword used in the SAS data step merge. That indicator will tell us if the particular row is present in both the left and the right tables (value &lt;code&gt;both&lt;/code&gt;), or just in one of them (values &lt;code&gt;left_only&lt;/code&gt; and &lt;code&gt;right_only&lt;/code&gt;). We can then query on that indicator variable to subset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;res1 = (pd.merge(A, B, how=&#39;outer&#39;, indicator=True)
            .query(&#39;_merge==&amp;quot;left_only&amp;quot;&#39;)
            .drop(&#39;_merge&#39;, axis=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;only-in-b&#34;&gt;Only in B&lt;/h3&gt;
&lt;p&gt;Same idea, but reversed: $B-A$. The implementation is identical except that we are using a right join instead:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table res2 as
  select B.* from
  A right join B
  on A.MRN=B.MRN
  where A.MRN is NULL;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Expected output:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;MRN&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;th&gt;Chol_Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;64836&lt;/td&gt;
&lt;td&gt;129&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2466&lt;/td&gt;
&lt;td&gt;127&lt;/td&gt;
&lt;td&gt;Borderline&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is interesting to note that SQLite, at least as of 3.37.2, still doesn&amp;rsquo;t support right joins, so if you&amp;rsquo;re using that you&amp;rsquo;ll just want to use the left join method above but switch the &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; around. The data step implementation is also straight forward:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data res2;
  merge A (IN = X) B (IN=Y);
  by MRN;
  If Y and not X;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;as is the Pandas version:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;res2 = (pd.merge(A, B, how=&#39;outer&#39;, indicator=True)
            .query(&#39;_merge==&amp;quot;right_only&amp;quot;&#39;)
            .drop(&#39;_merge&#39;, axis=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;whats-in-common&#34;&gt;What&amp;rsquo;s in common?&lt;/h3&gt;
&lt;p&gt;Finally, you  might be curious to see which rows both data sets have in common, that is $A \cap B$. That&amp;rsquo;s a simple inner join:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create table res3 as
  select A.* from 
  A inner join B
  on A.MRN=B.MRN;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Expected output:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;MRN&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;th&gt;Chol_Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;79602&lt;/td&gt;
&lt;td&gt;139&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In SAS:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data res3;
  merge A (IN = X) B (IN=Y);
  by MRN;
  If X and Y;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and in Pandas:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;res3 = (pd.merge(A, B, how=&#39;outer&#39;, indicator=True)
            .query(&#39;_merge==&amp;quot;both&amp;quot;&#39;)
            .drop(&#39;_merge&#39;, axis=1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it. All that&amp;rsquo;s left to do is to save the data in a format your customer or colleagues can work with and we&amp;rsquo;re done.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Univariate Missing Data with PROC MI</title>
      <link>https://dmsenter89.github.io/post/23-08-univariate-mi/</link>
      <pubDate>Sun, 13 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-08-univariate-mi/</guid>
      <description>&lt;p&gt;In Chapter 3 of van Buuren&amp;rsquo;s &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt; a variety of methods for imputing univariate missing data are presented. This post will summarize these techniques and show how to implement them in SAS.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#preliminaries&#34;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#imputing-under-a-normal-linear-model&#34;&gt;Imputing under a Normal Linear Model&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#regression-imputation&#34;&gt;Regression Imputation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#stochastic-regression-imputation&#34;&gt;Stochastic Regression Imputation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#bayesianbootstrap-multiple-imputation&#34;&gt;Bayesian/Bootstrap Multiple Imputation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#what-if-my-data-are-non-normal&#34;&gt;What if my data are non-normal?&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#predictive-mean-matching&#34;&gt;Predictive Mean Matching&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#classification-and-regression-trees&#34;&gt;Classification and Regression Trees&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-propensity-score-method&#34;&gt;The Propensity Score Method&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#categorical-and-count-data&#34;&gt;Categorical and Count Data&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#the-logistic-and-logit-models&#34;&gt;The Logistic and Logit Models&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-discriminant-function-method&#34;&gt;The Discriminant Function Method&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;

&lt;h2 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;Van Buuren demonstrates various techniques using data set 88 from Hand et al (1994). This data set is availabe from R&amp;rsquo;s MASS library as &lt;code&gt;data(&amp;quot;whiteside&amp;quot;)&lt;/code&gt;. The original data set can be downloaded from the 
&lt;a href=&#34;https://www.routledge.com/A-Handbook-of-Small-Data-Sets/Hand-Daly-McConway-Lunn-Ostrowski/p/book/9780367449667&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;publisher&amp;rsquo;s website&lt;/a&gt;. The name of the relevant data file is INSULATE.DAT. If you want to follow along using SAS, you can use 
&lt;a href=&#34;./code/whiteside.sas&#34;&gt;this data step&lt;/a&gt;. It matches the way the data appears in R except that I have added a variable &lt;code&gt;R&lt;/code&gt; indicating the observation that van Buuren deletes for demonstration purposes.&lt;/p&gt;
&lt;p&gt;For purposes of this post, we assume one or more predictors $x$ are completely observed, while some variable of interest $y$ is only partially observed. Methods for dealing with this type of problem are available using the 
&lt;a href=&#34;http://documentation.sas.com/doc/en/statug/15.2/statug_mi_syntax09.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;monotone&lt;/a&gt; keyword in PROC MI. A data set has a monotone missing pattern if it consists of variables $Y_1$, $Y_2$, $\ldots$, $Y_p$ such that if $Y_j$ is missing for one individual, all subsequent variables $Y_k$ for $j &amp;lt; k \leq p$ are also missing. Schematically, the data set will look like this:&lt;/p&gt;
&lt;p&gt;$$R = \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 1 \\ 1 &amp;amp; 1 &amp;amp; 0 \\ 1 &amp;amp; 0 &amp;amp; 0 \end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;where 1 indicates an observed value and 0 a missing value. The monotone statement in SAS can impute missing values by completing the columns in turn using univariate methods. See the 
&lt;a href=&#34;http://documentation.sas.com/doc/en/statug/15.2/statug_mi_details06.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt; for specifics.&lt;/p&gt;
&lt;h2 id=&#34;imputing-under-a-normal-linear-model&#34;&gt;Imputing under a Normal Linear Model&lt;/h2&gt;
&lt;p&gt;For completion, I will mention all of the main linear model methods van Buuren mentions in his text, even though the first two are not implemented in PROC MI.&lt;/p&gt;
&lt;h3 id=&#34;regression-imputation&#34;&gt;Regression Imputation&lt;/h3&gt;
&lt;p&gt;Van Buuren also refers to this as the &amp;ldquo;prediction&amp;rdquo; method. In essence, the complete cases are used to create a linear model. This linear model is then used to fill in the missing values:&lt;/p&gt;
&lt;p&gt;$$ \dot{y} = \hat\beta_0 + X_\text{mis}\,\hat\beta_1$$&lt;/p&gt;
&lt;p&gt;where $\hat\beta_i$ are least squares estimates.&lt;/p&gt;
&lt;p&gt;This method has a variety of drawbacks. For one, it artificially strengthens the relationships between variables as they appear in the linear model by increasing correlations. Variability in the data is reduced. See section 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-simplesolutions.html#sec:regimp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1.3.4&lt;/a&gt; in van Buuren for details.&lt;/p&gt;
&lt;p&gt;The mice package implements this method as &lt;code&gt;norm.predict&lt;/code&gt;. PROC MI does not implement this method; to use this technique in SAS, you could use the regression PROCs or IML.&lt;/p&gt;
&lt;h3 id=&#34;stochastic-regression-imputation&#34;&gt;Stochastic Regression Imputation&lt;/h3&gt;
&lt;p&gt;This method proceeds as above, except that Gaussian noise is added to the imputed value:
$$ \dot{y} = \hat\beta_0 + X_\text{mis}\,\hat\beta_1 + \dot\epsilon$$
where $\dot\epsilon \sim N(0, \hat\sigma^2)$. An advantage of this method over plain regression is that it can preserve correlation between variables.&lt;/p&gt;
&lt;p&gt;The mice package implements this method as &lt;code&gt;norm.nob&lt;/code&gt;. It is not available in PROC MI but can be implemented with IML.&lt;/p&gt;
&lt;h3 id=&#34;bayesianbootstrap-multiple-imputation&#34;&gt;Bayesian/Bootstrap Multiple Imputation&lt;/h3&gt;
&lt;p&gt;Van Buuren also refrers to this as &amp;ldquo;predict + noise + parameters uncertainty.&amp;rdquo; This technique is based on a Bayesian linear regression using draws from the posterior as parameters:
$$\dot y = \dot\beta_0 + X_\text{mis}\, \dot\beta_1 + \dot\epsilon$$
where $\dot\epsilon\sim N(0,\dot\sigma^2)$ and $\dot\beta_i$, $\dot\sigma$ are random draws from the posterior distribution.&lt;/p&gt;
&lt;p&gt;This the default method in PROC MI for continuous data. Both SAS and mice use an algorithm based on Rubin (1987, pp. 166-167). See the 
&lt;a href=&#34;http://documentation.sas.com/doc/en/statug/15.2/statug_mi_details07.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt; and 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-linearnormal.html#def:norm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Algorithm 3.1&lt;/a&gt; in van Buuren for details. The mice package implements this method as &lt;code&gt;norm&lt;/code&gt;. Here is an example of how the Bayesian regression can be used in PROC MI:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mi data=whiteside_miss out=regimp nimpute=5;
	var temp gas;
	monotone regression(gas);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The regression keyword may be abbreviated as &lt;code&gt;reg&lt;/code&gt;. A fully worked example is available in the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_examples03.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt;, with the associated code available on 
&lt;a href=&#34;https://github.com/sassoftware/doc-supplement-statug/blob/main/Examples/m-n/miex3.sas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The mice package also implements a variant of this method using bootstrapping instead of a Bayesian model. This method is available as &lt;code&gt;norm.boot&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;what-if-my-data-are-non-normal&#34;&gt;What if my data are non-normal?&lt;/h3&gt;
&lt;p&gt;In case the data are non-normal, one could proceed to a non-regression technique like 
&lt;a href=&#34;#predictive-mean-matching&#34;&gt;predictive mean matching&lt;/a&gt;. Alternatively, one could adjust the regression methods to utizilise a generalized linear model instead. That technique is implemented in the 
&lt;a href=&#34;https://github.com/dsalfran/ImputeRobust&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ImputeRobust&lt;/a&gt; package for R. See section 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-nonnormal.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3.3&lt;/a&gt; in van Buuren for details.&lt;/p&gt;
&lt;h2 id=&#34;predictive-mean-matching&#34;&gt;Predictive Mean Matching&lt;/h2&gt;
&lt;p&gt;Similar to 
&lt;a href=&#34;#bayesianbootstrap-multiple-imputation&#34;&gt;Bayesian regression&lt;/a&gt; above, a predicted value is calculated for each missing observation. Instead of adding noise to this prediction, however, a set of $k$ observations whose predicted values are close to the predicted missing value are sought. The missing value is then replaced by a random draw from this set of candidate donors. In mice, this method is available as &lt;code&gt;pmm&lt;/code&gt;. In PROC MI, you can use the &lt;code&gt;regpredmeanmatch&lt;/code&gt; keyword:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mi data=whiteside_miss out=regimp nimpute=5;
	var temp gas;
	monotone regpredmeanmatch(gas);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The keyword &lt;code&gt;regpredmeanmatch&lt;/code&gt; may be abbreviated as &lt;code&gt;regpmm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The predictive mean matching method is robust to transformations of the target variable. It may be used with both continuous and discrete data and will always generate realistic data in the sense that all generated data has been observed. Since this does not require an explicit model to describe the distribution of missing values, it is more resilient to model misspecification.&lt;/p&gt;
&lt;p&gt;See the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_details08.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt; and 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-pmm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;section 3.4&lt;/a&gt; in van Buuren for details.&lt;/p&gt;
&lt;h2 id=&#34;classification-and-regression-trees&#34;&gt;Classification and Regression Trees&lt;/h2&gt;
&lt;p&gt;An idea borrowed from the machine learning community and implemented in some R packages. In essence, the idea is similar to utiziling linear regression models except that a regression tree is utilized instead. See 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-cart.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;section 3.5&lt;/a&gt; in van Buuren.&lt;/p&gt;
&lt;h2 id=&#34;the-propensity-score-method&#34;&gt;The Propensity Score Method&lt;/h2&gt;
&lt;p&gt;With this method, propensity scores are generated for each observation estimating the probability of it being missing. The observations are then grouped by their propensity scores and an approximate Bayesian bootstrap imputation is carried out for each group. See 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_details18.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;A fully worked example is available in the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_examples02.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt;, with the associated code available on 
&lt;a href=&#34;https://github.com/sassoftware/doc-supplement-statug/blob/main/Examples/m-n/miex2.sas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mi data=Fish1 out=outex2;
   monotone propensity;
   var Length1 Length2 Length3;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This method is not implemented in the mice package.&lt;/p&gt;
&lt;h2 id=&#34;categorical-and-count-data&#34;&gt;Categorical and Count Data&lt;/h2&gt;
&lt;h3 id=&#34;the-logistic-and-logit-models&#34;&gt;The Logistic and Logit Models&lt;/h3&gt;
&lt;p&gt;Logit based regression models can be used both for nominal and ordinal data. The imputed value is generated from a Bayesian logistic regression model. The mice package implements this method as &lt;code&gt;logreg&lt;/code&gt;. PROC MI uses the &lt;code&gt;logistic&lt;/code&gt; keyword. An example of its usage is given in the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_examples04.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt;, with the associated code available on 
&lt;a href=&#34;https://github.com/sassoftware/doc-supplement-statug/blob/main/Examples/m-n/miex4.sas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;. Here&amp;rsquo;s the example code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mi data=Fish2 out=outex4;
   class Species;
   monotone reg(Width/ details)
            logistic(Species = Length Width Length*Width/ details);
   var Length Width Species;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This imputes the width variable using the 
&lt;a href=&#34;#bayesianbootstrap-multiple-imputation&#34;&gt;Bayesian linear regression&lt;/a&gt; while imputing the categorical species variable using the logistig regression method.&lt;/p&gt;
&lt;p&gt;See the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_details13.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt; for details.&lt;/p&gt;
&lt;h3 id=&#34;the-discriminant-function-method&#34;&gt;The Discriminant Function Method&lt;/h3&gt;
&lt;p&gt;This method is the default for categorical data in PROC MI. See the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_details09.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;A fully worked example is available in the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/statug/15.2/statug_mi_examples05.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS documentation&lt;/a&gt;, with the associated code available on 
&lt;a href=&#34;https://github.com/sassoftware/doc-supplement-statug/blob/main/Examples/m-n/miex5.sas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;. Here is the MI call:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mi data=Fish2 out=outex5;
   class Species;
   monotone discrim(Species = Length Width/ details);
   var Length Width Species;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mice package does not implement this method.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Hand, D. J., F. Daly, A. D. Lunn, K. J. McConway, and Ostrowski,  E. (1994), &lt;em&gt;A Handbook of Small Data Sets&lt;/em&gt;, London: Chapman &amp;amp; Hall.&lt;/p&gt;
&lt;p&gt;Rubin, D. B. (1987), &lt;em&gt;Multiple Imputation for Nonresponse in Surveys&lt;/em&gt;, New York: John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;p&gt;van Buuren, S. (2018), &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt;, Chapman and Hall/CRC interdisciplinary statistics series, Boca Raton: CRC Press, Taylor and Francis Group. Available at &lt;a href=&#34;https://stefvanbuuren.name/fimd/&#34;&gt;https://stefvanbuuren.name/fimd/&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SUC - a Slack Clone for Modern Unix</title>
      <link>https://dmsenter89.github.io/post/23-07-suc-a-unix-slack-clone/</link>
      <pubDate>Wed, 26 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-07-suc-a-unix-slack-clone/</guid>
      <description>&lt;p&gt;I love simple CLI tools and am a big fan of the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Unix_philosophy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unix philosophy&lt;/a&gt;.
Recently I came across 
&lt;a href=&#34;https://the-dam.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Dam&lt;/a&gt;, a public Unix server that implements a clever tool
they termed suc - the Simple Unix Chat. Essentially, it applies the Unix philophy to create a simple chat tool
that can be used on any modern Unix server. The key code consists of just a few lines of Bash code.
Check out the documentation for it 
&lt;a href=&#34;https://the-dam.org/docs/explanations/suc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Explore C Code With GNU Tools</title>
      <link>https://dmsenter89.github.io/post/23-07-explore-c-code-with-gnu-tools/</link>
      <pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-07-explore-c-code-with-gnu-tools/</guid>
      <description>&lt;p&gt;This post will introduce three GNU tools to help you explore your C code: ctags, cscope, and cflow.
The first two can help you navigate your code as you work on it and can be used directly within Vim.
Cflow on the other hand produces control charts that help you get to know the control flow in a project,
which is particularly helpful if you are new to the codebase.&lt;/p&gt;
&lt;h2 id=&#34;ctags&#34;&gt;ctags&lt;/h2&gt;
&lt;p&gt;In short, ctags is a program that can generate a file listing C symbols in a way that can be used by
Vim (ctags) or by Emacs (etags). Various versions exist. See 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Ctags&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this wiki page&lt;/a&gt;
for some links. A current maintained version of universal ctags can be found on 
&lt;a href=&#34;https://github.com/universal-ctags/ctags&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.
Universal ctags expands on the original ctags by including support for additional languages.&lt;/p&gt;
&lt;p&gt;The first step to using a ctags file is to generate one for your source code. Just run &lt;code&gt;ctags&lt;/code&gt; followed by the
location of your source files. If you have multipled directories, you can list them sequentially like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ ctags h/* src/*
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will generate a &amp;ldquo;tags&amp;rdquo; file in the current folder. If you open Vim from the same folder, the tags file
is automatically loaded. What is particularly useful about the tag file is that saved keywords are addressed by
patterns, not line numbers. This way, minor edits don&amp;rsquo;t require ctags to be re-run.&lt;/p&gt;
&lt;h3 id=&#34;basic-usage&#34;&gt;Basic Usage&lt;/h3&gt;
&lt;p&gt;To find the definition of a C symbol in your source code, put your cursors on the symbol and press &lt;code&gt;&amp;lt;Ctrl-]&amp;gt;&lt;/code&gt; to
jump to that symbol&amp;rsquo;s definition. To get back to where you were, press &lt;code&gt;&amp;lt;Ctrl-t&amp;gt;&lt;/code&gt;. If the symbol has multiple
definitions and you jumped to the wrong one, try using the &lt;code&gt;:tselect&lt;/code&gt; command to bring up a list of all matches.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Command&lt;/th&gt;
&lt;th&gt;Effect&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;&amp;lt;Ctrl-]&amp;gt;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Jump to definition of the keyword under the cursor.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;:ta[g] {ident}&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Jump to the definition of &lt;code&gt;{ident}&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;&amp;lt;Ctrl-t&amp;gt;&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Jump back up the tag stack.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;:tags&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Show content of tag stack.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;:po[p]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Jump to older entry in tag stack.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;:ta[g]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Jump to newer entry in tag stack.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;:ts[elect] [ident]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;List tags that match &lt;code&gt;[ident]&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;:sts[elect]&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Same as above, but splits window for tag.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For details, see &lt;code&gt;:help tags&lt;/code&gt; in Vim.&lt;/p&gt;
&lt;h2 id=&#34;cscope&#34;&gt;cscope&lt;/h2&gt;
&lt;p&gt;The cscope program has more advanced features compared to ctags. In addition to finding symbol definitions,
it can gather more advanced information than ctags. Specifically, it can tell you&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where a symbol is used in the code,&lt;/li&gt;
&lt;li&gt;where the symbol was defined,&lt;/li&gt;
&lt;li&gt;where a variable got its value from,&lt;/li&gt;
&lt;li&gt;what other functions call this function,&lt;/li&gt;
&lt;li&gt;what functions are called by a specific function,&lt;/li&gt;
&lt;li&gt;and more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similar to ctags, a database file is created by the csope program. You can run it like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cscope -b h/* src/*
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will generate a &lt;code&gt;cscope.out&lt;/code&gt; file that can be used with Vim. To make the cscope database
available, you need to add it during your Vim session by using &lt;code&gt;:cs[cope] add {file|dir}&lt;/code&gt;.
By adding the following to your &lt;code&gt;.vimrc&lt;/code&gt; you can automate this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-vim&#34;&gt;if has(&amp;quot;cscope&amp;quot;)
    &amp;quot; add any database in current directory
    if filereadable(&amp;quot;cscope.out&amp;quot;)
        cs add cscope.out
        &amp;quot; else add database pointed to by environment
    elseif $CSCOPE_DB != &amp;quot;&amp;quot;
        cs add $CSCOPE_DB
    endif
endif
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;basic-usage-1&#34;&gt;Basic Usage&lt;/h3&gt;
&lt;p&gt;The basic command used is &lt;code&gt;:cs find {querynum|querytype} {name}&lt;/code&gt;, with the following main query types:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;querynum&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;querytype&lt;/th&gt;
&lt;th&gt;Effect&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;s&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Find this C symbol&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;g&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Find this definition&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;d&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Find functions called by this function&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;c&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Find functions calling this function&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;t&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Find this text string&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;6&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;e&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Find this egrep pattern&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;7&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;f&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Find this file&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;8&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;i&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Find files #including this file&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;9&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;code&gt;a&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Find places where this symbol is assigned a value&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For details, see &lt;code&gt;:help cscope&lt;/code&gt; in Vim. For some suggested options and keymappings that make
using cscope more convenient, see &lt;code&gt;:help cscope-suggestions&lt;/code&gt;. You can also use the &lt;code&gt;querynum&lt;/code&gt;
to perform a single search using the cscope cli interface, e.g.: &lt;code&gt;cscope -L{querynum} {name} [-d]&lt;/code&gt;
where &lt;code&gt;[-d]&lt;/code&gt; suppresses updating the cscope database.&lt;/p&gt;
&lt;h2 id=&#34;cflow&#34;&gt;cflow&lt;/h2&gt;
&lt;p&gt;GNU 
&lt;a href=&#34;https://www.gnu.org/software/cflow/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cflow&lt;/a&gt; is a tool that creates charts showing control
flow within your program. It has a &lt;em&gt;lot&lt;/em&gt; of options and settings, so you&amp;rsquo;ll definitely want to
check out its 
&lt;a href=&#34;https://www.gnu.org/software/cflow/manual/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The most basic usage is &lt;code&gt;cflow {file[s]}&lt;/code&gt; which creates an indented listing of function calls
starting from &lt;code&gt;main()&lt;/code&gt;. Two important command line options are &lt;code&gt;--main&lt;/code&gt; which allows you to
set a different starting function, and &lt;code&gt;--target&lt;/code&gt; which allows you to set a target function
below which you don&amp;rsquo;t want to investigate. If you want to include functions that aren&amp;rsquo;t directly
reachable from &lt;code&gt;main()&lt;/code&gt; or &lt;code&gt;--main&lt;/code&gt; in your chart, use the &lt;code&gt;--all&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;A particularly nifty feature is that cflow can generate valid dot files using
&lt;code&gt;cflow -f dot {file[s]}&lt;/code&gt;. These can be piped to graphviz to produce visual charts of your
function calls, e.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ cflow -f dot example.c | dot -Tpng -o flow-example.png
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Sampling Regression Lines</title>
      <link>https://dmsenter89.github.io/post/23-05-sample-regression-lines/</link>
      <pubDate>Mon, 08 May 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-05-sample-regression-lines/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://dmsenter89.github.io/post/23-05-simple-regression-with-proc-mcmc/&#34;&gt;Last week&lt;/a&gt; we saw how to generate posterior samples using PROC MCMC for simple linear and logistic regression models. This week, I want to show how to sample regression lines from the data set returned by MCMC by plotting several sample regression linse on top of a scatter plot of the source data.&lt;/p&gt;
&lt;h2 id=&#34;writing-the-macro&#34;&gt;Writing the Macro&lt;/h2&gt;
&lt;p&gt;Since the majority of the steps are identical irrespective of what data set we use, and because we might want to use this iteretively during model building, I decided to write this up as a macro. To some degree this is required since I will be using a macro do-loop, which is only valid when embedded inside of a macro.&lt;/p&gt;
&lt;p&gt;This macro will assume we have fitted a simple linear model of the form&lt;/p&gt;
&lt;p&gt;\begin{aligned}
y_i &amp;amp;\sim \mathrm{Normal}(\mu_i, \sigma) \\&lt;br&gt;
\mu_i &amp;amp;= \beta_0 + \beta_1 x_i
\end{aligned}&lt;/p&gt;
&lt;h3 id=&#34;step-1-get-an-srs-sample&#34;&gt;Step 1: Get an SRS Sample&lt;/h3&gt;
&lt;p&gt;The first step is the simplest - selecting a subset of the posterior samples. This is easily achieved by calling 
&lt;a href=&#34;https://support.sas.com/rnd/app/stat/procedures/surveyselect.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PROC SURVEYSELECT&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc surveyselect data=&amp;amp;posterior. method=srs n=&amp;amp;n.
                  out=SRS;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-2-make-a-macro-list-of-parameters&#34;&gt;Step 2: Make a Macro List of Parameters&lt;/h3&gt;
&lt;p&gt;Next we need to generate a list of intercepts and slopes. I find it easiest to read those in PROC SQL using the &lt;code&gt;into&lt;/code&gt; operation. Additionally, we&amp;rsquo;ll also collect the $x$- and $y$-ranges of our data. This will be used to make sure our plot is centered on the scatter-plot values of our source data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc sql noprint;
  select beta0, beta1 
  into :intercepts separated by &#39; &#39;,
       :slopes  separated by &#39; &#39;
  from SRS;

  select min(x), max(x),
         min(y), max(y)
  into :minx, :maxx,
       :miny, :maxy
  from &amp;amp;ds.;
quit;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-3-macro-loop-to-add-lines-to-a-scatter-plot&#34;&gt;Step 3: Macro-Loop to Add Lines to a Scatter Plot&lt;/h3&gt;
&lt;p&gt;Now all the parts have been assembled and you can call PROC SGPLOT. We use the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/grstatproc/p1lcbd3lhs3t3bn1jk6d8sjt2yqx.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scatter&lt;/a&gt; statement to create a scatter plot of the source data. Then we use a do-loop to iteratively paste different 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/grstatproc/n1h6n82pw2uqo6n10avwmph63r7o.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lineparm&lt;/a&gt; statements corresponding to our different samples into the SGPLOT statement. Lastly, use the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/grstatproc/n0n6uml63c6h8dn16phbd1arm9g9.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;xaxis&lt;/a&gt; and 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/grstatproc/n0n6uml63c6h8dn16phbd1arm9g9.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;yaxis&lt;/a&gt; statements to focus the graph on the scatter plot data, and not forcing the x-intercepts of the different fitted lines into&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc sgplot data=&amp;amp;ds. noautolegend;
  scatter x=x y=y;
  %do i = 1 %to &amp;amp;n.;
    lineparm x=0 y=%scan(&amp;amp;intercepts, &amp;amp;i, &#39; &#39;) 
         slope=%scan(&amp;amp;slopes, &amp;amp;i, &#39; &#39;) / transparency=0.7;
  %end;

  xaxis min=&amp;amp;minx. max=&amp;amp;maxx.;
  yaxis min=&amp;amp;miny. max=&amp;amp;maxy.;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;calling-the-macro&#34;&gt;Calling the Macro&lt;/h2&gt;
&lt;p&gt;And that&amp;rsquo;s it. Assuming we declared the macro as follows&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;%macro sample_regression(ds=, posterior=, n=);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we can now call it.&lt;/p&gt;
&lt;p&gt;As a particular example, let&amp;rsquo;s run PROC MCMC&amp;rsquo;s getting started example 1 straight from 
&lt;a href=&#34;https://github.com/sassoftware/doc-supplement-statug/blob/main/Examples/m-n/mcmcgs1.sas&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;filename url gs1 &#39;https://raw.githubusercontent.com/sassoftware/doc-supplement-statug/main/Examples/m-n/mcmcgs1.sas&#39;;
%include gs1;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, we predict weight based on height in the &lt;code&gt;sashelp.class&lt;/code&gt; data set. The posterior samples are available as &lt;code&gt;work.classout&lt;/code&gt;. We&amp;rsquo;ll want to rename the height and weight variables to $x$ and $y$ in order to work with the chosen macro names. This is easily accomplished by using the &lt;code&gt;rename&lt;/code&gt; statement in the macro call itself. We&amp;rsquo;ll call it with $n=15$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;%sample_regression(ds=sashelp.class(rename=(Height=x Weight=y)),
                  posterior=work.classout,
                  n=15);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will produce the following output:&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-05-sample-regression-lines/class-regression-sample_hu8718c9b6d2106aec942f1d2ab5f5552b_53162_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-05-sample-regression-lines/class-regression-sample_hu8718c9b6d2106aec942f1d2ab5f5552b_53162_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;With slight modifications you can also use this macro to help you refine your priors. By using the&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;model general(0);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;statement in PROC MCMC in lieu of a regular model you will get estimates of the prior parameters. See the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/statug/statug_mcmc_examples01.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; for examples.&lt;/p&gt;
&lt;!-- 
```sas sample-regression.sas
/*
 * This code was generated for the blog post &#34;Sampling Regression Lines&#34;
 * available at dmsenter89.github.io/post/23-05-sample-regression-lines/
 *
 * Author: Michael Senter, PhD
 */

&lt;&lt;&lt;macro-first-line&gt;&gt;&gt;
&lt;&lt;&lt;srs&gt;&gt;&gt;

&lt;&lt;&lt;parameter-list&gt;&gt;&gt;

&lt;&lt;&lt;lineparm&gt;&gt;&gt;
%mend sample_regression;

&lt;&lt;&lt;mcmc-gs1&gt;&gt;&gt;

&lt;&lt;&lt;macro-call&gt;&gt;&gt;
```
--&gt;
</description>
    </item>
    
    <item>
      <title>Simple Regression With PROC MCMC</title>
      <link>https://dmsenter89.github.io/post/23-05-simple-regression-with-proc-mcmc/</link>
      <pubDate>Tue, 02 May 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-05-simple-regression-with-proc-mcmc/</guid>
      <description>&lt;p&gt;In this post I&amp;rsquo;ll show how to fit simple linear and logistic regression models using the 
&lt;a href=&#34;https://support.sas.com/rnd/app/stat/procedures/mcmc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MCMC&lt;/a&gt; procedure in SAS. Note that the point of this post is to show how the mathematical model is translated into PROC MCMC syntax and not to discuss the method itself. I will include links to relevant sections in Johnson, Ott, and Dogucu (2022) if you&amp;rsquo;d like to read more about Bayesian modeling.&lt;/p&gt;
&lt;h2 id=&#34;the-mcmc-statement&#34;&gt;The MCMC Statement&lt;/h2&gt;
&lt;p&gt;The basic 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_mcmc_syntax.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;syntax&lt;/a&gt; for MCMC is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-nohighlight&#34;&gt;proc mcmc &amp;lt;options&amp;gt;;
    parms parameter &amp;lt;=&amp;gt; number &amp;lt;/options&amp;gt;;
    prior parameter ~ distribution;
    programming statements;
    model varaiable ~ distribution &amp;lt;/options&amp;gt;; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This covers the basic components of most Bayesian models - the model itself (&lt;code&gt;model&lt;/code&gt;), the parameters that need to be fit (&lt;code&gt;parms&lt;/code&gt;), and their prior distribution (&lt;code&gt;prior&lt;/code&gt;). Note that PROC MCMC requires you to always specify your priors, unlike some Bayesian modeling software that will default to some diffuse priors when they are omitted from the problem statement.&lt;/p&gt;
&lt;p&gt;The most common options you&amp;rsquo;ll use in the MCMC statement will be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data=&lt;/code&gt; the name of the input data set.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;outpost=&lt;/code&gt; the name of the output data set for posterior samples of parameters.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nbi=&lt;/code&gt; the number of burn-in iterations.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nmc=&lt;/code&gt; the number of MCMC iterations, excluding the burn-in iterations.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;seed=&lt;/code&gt; specify a random seed for the the simulation.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;thin=&lt;/code&gt; specify the thinning rate; see 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_introbayes_sect016.htm#statug.introbayes.bayesburnin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The names and function calls for the included distributions are described in the documentation on the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_mcmc_syntax09.htm#statug_mcmc002612&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MODEL statement&lt;/a&gt;. Their density definitions are documented 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_mcmc_details17.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;a-simple-linear-model&#34;&gt;A Simple Linear Model&lt;/h2&gt;
&lt;p&gt;A basic linear model looks something like this:&lt;/p&gt;
&lt;p&gt;\begin{aligned}
y_i &amp;amp;\sim \mathrm{Normal}(\mu_i, \sigma) \\&lt;br&gt;
\mu_i &amp;amp;= \beta_0 + \beta_1 x_i
\end{aligned}&lt;/p&gt;
&lt;p&gt;which will need to be combined with priors for $\beta_0$, $\beta_1$, and $\sigma$. Assume we have a data set &lt;code&gt;work.mydata&lt;/code&gt; that contains two variables: our predictor &lt;code&gt;x&lt;/code&gt; and our measured variable &lt;code&gt;y&lt;/code&gt;. Assume we use the above model together with the following priors:&lt;/p&gt;
&lt;p&gt;\begin{aligned}
\beta_0 &amp;amp;\sim \mathrm{Normal}(0, 10) \\&lt;br&gt;
\beta_1 &amp;amp;\sim \mathrm{Normal}(0, 10) \\&lt;br&gt;
\sigma &amp;amp;\sim \mathrm{Uniform}(0,50).
\end{aligned}&lt;/p&gt;
&lt;p&gt;Translating this into PROC MCMC is straightforward. Even though we can specify the statements in any order, it is common to define the model &amp;ldquo;upside down&amp;rdquo; so that each line contains only variables that have already been defined. This is for convenience, so you don&amp;rsquo;t forget to specify something before hitting &amp;ldquo;run.&amp;rdquo;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mcmc data=mydata
      outpost=posterior /* posterior sim */
      nmc=2000; /* # of data points in posterior */
    /* define the parameters. Optionally give an initial value */
    parms beta0 0 beta1 1;
    parms sigma; /* no initial value - mcmc finds its own */
    
    * define your priors;
    prior beta: ~ normal(mean=0, sd=10);
    prior sigma ~ uniform(0, 50);
    
    * define the mean and the model;
    mu = beta0 + beta1*x;
    model y ~ normal(mu, sd=sigma);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more info on simple regression, check out 
&lt;a href=&#34;https://www.bayesrulesbook.com/chapter-9.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chapters 9-11&lt;/a&gt; in Bayes Rules!&lt;/p&gt;
&lt;h2 id=&#34;a-simple-logistic-model&#34;&gt;A Simple Logistic Model&lt;/h2&gt;
&lt;p&gt;A basic logistic model will look as follows:&lt;/p&gt;
&lt;p&gt;\begin{aligned}
y_i &amp;amp;\sim \mathrm{Binomial}(n_i, p_i) \\&lt;br&gt;
\mathrm{logit}(p_i) &amp;amp;= \beta_0 + \beta_1 x_i
\end{aligned}&lt;/p&gt;
&lt;p&gt;combined with appropriate priors for $\beta_0$, $\beta_1$. Here $y_i$ is a positive integer response, $n_i$ is a count, and $x_i$ is still our predictor. In many medical studies we are interested in the special case where $y_i \in \{0,1\}$ so that the model becomes&lt;/p&gt;
&lt;p&gt;\begin{aligned}
y_i &amp;amp;\sim \mathrm{Bern}(p_i) \\&lt;br&gt;
\mathrm{logit}(p_i) &amp;amp;= \beta_0 + \beta_1 x_i.
\end{aligned}&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s assume a diffuse prior like this:&lt;/p&gt;
&lt;p&gt;\begin{aligned}
\beta_0 &amp;amp;\sim \mathrm{Normal}(0, 100) \\&lt;br&gt;
\beta_1 &amp;amp;\sim \mathrm{Normal}(0, 100).
\end{aligned}&lt;/p&gt;
&lt;p&gt;Then we can translate to PROC MCMC as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mcmc data=mydata
      outpost=posterior /* posterior sim */
      nmc=2000; /* # of data points in posterior */
    parms (beta0 beta1) 0;
    prior beta: ~ normal(0, sd=100);

    /* now define the logistic part: */
    p = logistic(beta0 + beta1*x);
    model y ~ bern(p);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Often we are not so much directly interested in the $\beta$ coefficients, but in the odds $e^{\beta_0}$ and the multiplicative change in odds $e^{\beta_1}$. While these values can be calculated and analyzed from the &lt;code&gt;outpost&lt;/code&gt; data set, we can use the &lt;code&gt;nodata&lt;/code&gt; block (delimited by &lt;code&gt;beginnodata&lt;/code&gt; and &lt;code&gt;endnodata&lt;/code&gt; statements) to directly calculate these values in our simulation. The amended procedure reads like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mcmc data=mydata
      outpost=posterior
      nmc=2000
      monitor=(odds modds);
    parms (beta0 beta1) 0;
    prior beta: ~ normal(0, sd=100);

    beginnodata;
      odds = exp(beta0);
      modds = exp(beta1);
    endnodata;

    p = logistic(beta0 + beta1*x);
    model y ~ bern(p);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See 
&lt;a href=&#34;https://www.bayesrulesbook.com/chapter-13.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chapter 13&lt;/a&gt; in Bayes Rules! for more info on logistic regression models.&lt;/p&gt;
&lt;h2 id=&#34;adding-a-random-effect&#34;&gt;Adding a Random-Effect&lt;/h2&gt;
&lt;p&gt;Random-effects, also known as hierarchical modeling, looks at group structures in the data seta nd models group-specific effects. In a clinical setting, this might be the study site.&lt;/p&gt;
&lt;p&gt;As a simple example, assume we have a data set &lt;code&gt;ht&lt;/code&gt; containing the height (&lt;code&gt;h&lt;/code&gt;) and sex (&lt;code&gt;s&lt;/code&gt;) of a population sample. Assume we are interested in modeling the distribution of height in our data set. We know that on average males are taller than females (mean 167 cm vs 156 cm based on NHANES 2006). We could build a model similar to this:&lt;/p&gt;
&lt;p&gt;\begin{aligned}
h_i &amp;amp;\sim \mathrm{Normal}(\mu_i, \sigma) \\&lt;br&gt;
\mu_i &amp;amp;= \alpha_{\mathrm{sex[i]}} \\&lt;br&gt;
\alpha_j &amp;amp;\sim \mathrm{Normal}(160, 20) \quad \text{for } j=1,2 \\&lt;br&gt;
\sigma &amp;amp;\sim \mathrm{Uniform}(0,50).
\end{aligned}&lt;/p&gt;
&lt;p&gt;In SAS, the random effect is specified with the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_mcmc_gettingstarted03.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;random&lt;/a&gt; statement. We specify the categories with the &lt;code&gt;subject&lt;/code&gt; keyword in the random statement. SAS will then automatically create the necessary number of parameters for the random effect. Our model translates to the following MCMC call:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mcmc data=ht
      outpost=posterior
      nmc=2000;
    parms sigma 5 sigmaA 6;
    prior sigma: ~ uniform(0,50);

    random alpha ~ normal(160, sd=sigmaA) subject=s monitor=(alpha);
    mu = alpha;
    model h ~ normal(mu, sd=sigma);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Assuming $s\in\{1,2\}$ this will cause SAS to create two alpha variables for the two levels of &lt;code&gt;s&lt;/code&gt;: &lt;code&gt;alpha_1&lt;/code&gt; and &lt;code&gt;alpha_2&lt;/code&gt;. Had &lt;code&gt;s&lt;/code&gt; been a character variable, say with values m and f, then SAS would have created &lt;code&gt;alpha_m&lt;/code&gt; and &lt;code&gt;alpha_f&lt;/code&gt; instead.&lt;/p&gt;
&lt;p&gt;For more information on this, check out 
&lt;a href=&#34;https://www.bayesrulesbook.com/chapter-15.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;unit IV&lt;/a&gt; of Bayes Rules!&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;Johnson, A. A., Ott, M. Q., and Dogucu, M. (2022), &lt;em&gt;Bayes Rules!: An Introduction to Applied Bayesian Modeling&lt;/em&gt;, Boca Raton, FL: CRC Press, DOI: 
&lt;a href=&#34;https://doi.org/10.1201/9780429288340&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.1201/9780429288340&lt;/a&gt;. Available online at 
&lt;a href=&#34;https://www.bayesrulesbook.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.bayesrulesbook.com&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Loading Several XPT Files From a URL</title>
      <link>https://dmsenter89.github.io/post/23-04-loading-several-xpt-files-from-a-url/</link>
      <pubDate>Mon, 24 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-04-loading-several-xpt-files-from-a-url/</guid>
      <description>&lt;p&gt;The SAS Transport File Format (XPORT) is an open file format maintained by SAS for exchanging datasets. Its use is mandated by the FDA for data set submission for new drug or device applications and the CDC uses this format to distribute public data. For details regrading this format, see 
&lt;a href=&#34;https://www.loc.gov/preservation/digital/formats/fdd/fdd000464.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this Library of Congress page&lt;/a&gt;. This post will explore how to read several of these files into a SAS session with the URL filename statement using the National Health and Nutrition Examination Survey, or 
&lt;a href=&#34;https://www.cdc.gov/nchs/nhanes/index.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NHANES&lt;/a&gt;, as an example.&lt;/p&gt;
&lt;h2 id=&#34;loading-a-single-xpt-file&#34;&gt;Loading a Single XPT File&lt;/h2&gt;
&lt;p&gt;By far the easiest way to read an XPT file is to use the &lt;code&gt;XPT2LOC&lt;/code&gt; autocall macro if it is available on your SAS installation. As an example, this snippet would load the demographics table from the 2017-2018 NHANES data set into the work library:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;filename demo &amp;quot;/data/Nhanes/2017-2018/DEMO_J.XPT&amp;quot;;
%XPT2LOC(filespec=demo, libref=work);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This macro correctly resolves the name of the data set, and it would be available as &lt;code&gt;work.demo_j&lt;/code&gt; now.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; See the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/movefile/p1tp8gighlgeifn173i6kzw2w3bu.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; for more details on this macro.&lt;/p&gt;
&lt;p&gt;If we cannot or do not want to use this macro, we&amp;rsquo;ll have to assign a LIBREF to the XPT file. This might seem weird at first, because you typically will only find a single data set in an XPT file. But if you consider that the file standard allows for multiple data sets to reside in the same XPT file, it makes sense. Using the LIBREF, we can achieve the same result as above using this snippet:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;filename xpt url &amp;quot;https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT&amp;quot;;
libname xpt xport;

proc copy in=xpt out=work; run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;loading-multiple-xpt-files-with-a-macro&#34;&gt;Loading Multiple XPT Files with a Macro&lt;/h2&gt;
&lt;p&gt;This is all fine if you only need to load one or two files that way, but becomes tedious (and repetitive) if you have to load many data sets this way. Ignoring the restricted data sets for a minute, NHANES contains many data sets spread across five domains:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Domain&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;# of Data Sets&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Demographics Data&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dietary Data&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Examination Data&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Laboratory Data&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;51&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Questionnaire Data&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;TOTAL&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;124&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Even if you only need a subset of this, you&amp;rsquo;ll find yourself wanting to shortcut having to type out all the repetitive information. This is where a macro call comes in handy.&lt;/p&gt;
&lt;p&gt;A great trick for this is to use a codebook like data set that you can iterate over. Here is a minimal example using four data sets from NHANES:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;/* create a location to hold saved data */
libname nhanes &#39;~/data/NHANES&#39;;

data nhanes.datasets;
    length df $10. dfname $100.;
    input df $ dfname $;
    infile datalines dsd;
datalines;
DEMO_J,Demographic Variables and Sample Weights
BPX_J,Blood Pressure
BMX_J,Body Measures
OHXDEN_J,Oral Health - Dentition
;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can either create this data set yourself or use a webscraping tool to make it for you. Wrapping the autocall macro or the PROC COPY into a macro is straightforward:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;%macro load_data(name=);
  /* allow bad SSL; this is due to an issue with cdc.gov */
  options set=SSLREQCERT=&amp;quot;allow&amp;quot;;

  /* set up for import */
  filename xpt url &amp;quot;https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/&amp;amp;name..XPT&amp;quot;;
  libname xpt xport;

  proc copy in=xpt out=nhanes; run;

  /* make sure to clear libname &amp;amp; filename for next macro call */
  filename xpt; libname xpt;
%mend;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only question now is how to trigger this macro for each data set listed in &lt;code&gt;nhanes.datasets&lt;/code&gt;. That&amp;rsquo;s where the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/v_031/lefunctionsref/p1blnvlvciwgs9n0zcilud6d6ei9.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CALL EXECUTE&lt;/a&gt; routine comes in. It allows us to invoke a macro for each line in the source data set while giving us access to the variables in the source data. Since this is executed as part of a data step, you can use more fine grained control by having if/else conditions, where clauses, etc. In our example, we&amp;rsquo;d use this data step:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data _NULL_;
    set nhanes.datasets;
    call execute(&#39;%load_data(name=&#39;||df||&#39;);&#39;);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After running our script, the folder specified by &lt;code&gt;libname nhanes&lt;/code&gt; will contain both our &amp;ldquo;codebook&amp;rdquo; of data sets, as well as all of the data sets listed in the file.&lt;/p&gt;
&lt;!-- 
```sas nhanes_load.sas
/*
 * This code was generated for the blog post &#34;Loading Several XPT Files From a URL&#34;
 * available at dmsenter89.github.io/post/23-04-loading-several-xpt-files-from-a-url/
 *
 * Author: Michael Senter, PhD
 */

options dlcreatedir;

&lt;&lt;&lt;codebook&gt;&gt;&gt;

&lt;&lt;&lt;macro&gt;&gt;&gt;

&lt;&lt;&lt;iteration&gt;&gt;&gt;
```

--&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Note that using this macro requires you to first download the file for processing. You can do this easily with a 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/v_037/lestmtsglobal/p05r9vhhqbhfzun1qo9mw64s4700.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TEMP filename&lt;/a&gt; statement. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Are the Rich Paying Their Fair Share?</title>
      <link>https://dmsenter89.github.io/post/23-04-are-the-rich-paying-their-fair-share/</link>
      <pubDate>Tue, 18 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-04-are-the-rich-paying-their-fair-share/</guid>
      <description>&lt;!-- the code in this file can be extracted with lmt
    https://github.com/driusan/lmt

```sas rich-paying-their-fair-share.sas
/*
 * This code was generated for the blog post &#34;Are the Rich Paying Their Fair Share?&#34;
 * available at dmsenter89.github.io/post/23-04-are-the-rich-paying-their-fair-share/.
 * 
 * Graphs are output to path &amp;TAXDIR. if defined, otherwise to `~/data`.
 *
 * Author: Michael Senter, PhD
 */

/* Convenience snippet. */
%if %symexist(TAXDIR) ne %then %do; 
	%put Workdir not set. Setting to Home;
	%let TAXDIR=~;
%end;
%put Current working directory is &amp;TAXDIR.;

/* temp file so the import needs only one download */
&lt;&lt;&lt;save temp file&gt;&gt;&gt;

/* load the data from excel sheet */
&lt;&lt;&lt;macro-function&gt;&gt;&gt;

&lt;&lt;&lt;macro-call&gt;&gt;&gt;

/* Join and Reformat the Data */
&lt;&lt;&lt;proc format&gt;&gt;&gt;

proc sql;
    create table IRS as 
    select T.A as Year label=&#34;Year&#34;
        , input(T.ExcelCol, coln.) as Percentile 
               format=PERCENT10.3 label=&#34;Top Percentile&#34;
        , TAXSH/100 as CumTaxShare 
                format=PERCENT10.3 label=&#34;Cumulative Tax Share&#34;
        , AGISH/100 as CumAGIShare 
                format=PERCENT10.3 label=&#34;Cumulative AGI Share&#34;
        , calculated CumTaxShare/calculated CumAGIShare as Ratio
                format=8.3
        from TAXSH as T left join AGISH as G
        on T.A = G.A and T.ExcelCol=G.ExcelCol
        order by Year, calculated Percentile;
quit;

/* Visualization */
ods listing gpath=&#34;&amp;TAXDIR.&#34;;

ods graphics / imagename=&#34;PVCTSH&#34; imagefmt=png;
proc sgplot data=IRS;
    where year=2020;
    loess x=Percentile y=CumTaxShare / legendlabel=&#34;Actual&#34;;
    lineparm x=0 y=0 slope=1 / CLIP 
        legendlabel=&#34;Reference&#34;
        lineattrs=(color=gray pattern=dash);
     xaxis ranges=(0.0-0.5) 
     	values=(0.01 0.05 0.1 0.2 0.3 0.4 0.5)
     	tickvalueformat=PERCENT10.;
run;

/* Calculating the TINI Coefficient */
data tini;
	set irs;
	by Year;

    /* setup lag values */
	LagPercentile = lag(Percentile);
	LagTax = lag(CumTaxShare);
	If first.Year then do;
		LagPercentile = 0; LagTax = 0;
		Sum = 0;
	end;

    /* trapezoid rule for area under curve */
	TaxCurve = (Percentile-LagPercentile)*(CumTaxShare + LagTax)/2;
	EqualityCurve = (Percentile**2 - LagPercentile**2)/2;
	
    /* Output TINI once year is processed */
    Diff = (TaxCurve-EqualityCurve);
	Sum + Diff;
	if last.Year then do;
		TINI = Sum / (3/8);
		output;
	end;
	
	Drop Lag: TaxCurve EqualityCurve Diff Sum;
run;

ods graphics / imagename=&#34;TINI&#34; imagefmt=png;
proc sgplot data=tini noautolegend;
	loess x=Year y=Tini;
	xaxis values=(2000 2005 2010 2015 2020);
	yaxis ranges=(0.65-.8) values=(0.65 0.7 0.75 0.8);
run;

/* Dot Plot of Ratio */
ods graphics / imagename=&#34;RATIO_DOT&#34; imagefmt=png;
proc sgplot data=irs(where=(Percentile&lt;0.1)) noautolegend;
	dot percentile / response=Ratio stat=mean limitstat=stddev numstd=1;
	xaxis label=&#39;Ratio of Cumulative Tax Share / Cumulative AGI Share&#39;; 
run;
```
--&gt;
&lt;p&gt;Today is tax day in the US. In celebration we&amp;rsquo;re going to take a look at some of the data available on the 
&lt;a href=&#34;https://www.irs.gov/statistics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IRS Statistics&lt;/a&gt; page. Since the current administration focuses a lot of rhetoric on making the &amp;ldquo;rich pay their fair share,&amp;rdquo; I thought it might be interesting to see if any of the data made available by the IRS could be used to look at whether or not this is true at the moment. Luckily for us, the IRS makes an 
&lt;a href=&#34;https://www.irs.gov/pub/irs-soi/20in41ts.xls&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Excel sheet&lt;/a&gt; available that includes what income percentiles earn what share of taxable income and what portion of the tax burden is born by these same percentiles.&lt;/p&gt;
&lt;p&gt;It is important to note that this Excel sheet only reports on what the IRS refers to as &amp;ldquo;
&lt;a href=&#34;https://www.irs.gov/e-file-providers/definition-of-adjusted-gross-income&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adjusted Gross Income&lt;/a&gt;,&amp;rdquo; which is - as the name suggests - gross income after adjustment by tax breaks and the like. While this accounts for capital gains, it does not account for all taxes paid by the population. Consumption taxes, for example sales tax revenue, is not included. State taxes are also excluded from this data set. Nevertheless I think this data set is interesting to look at, and we will note a research paper below that shows this trend continues even after adding in consumption taxes.&lt;/p&gt;
&lt;h2 id=&#34;defining-a-notion-of-fairness&#34;&gt;Defining a Notion of &amp;lsquo;Fairness&amp;rsquo;&lt;/h2&gt;
&lt;p&gt;The question of &amp;ldquo;fairness&amp;rdquo; can be viewed as a question of proportion. There are two obvious ways to do this: tax share being proportional to one&amp;rsquo;s share in the population, or being proportional to one&amp;rsquo;s share of the income. Let&amp;rsquo;s use an example to illustrate these two different options. You and two of your friends go out and have dinner together. The total tab amounts to 90 USD. What would be a fair way of splitting that tab?&lt;/p&gt;
&lt;p&gt;The simplest option would be to divide the entire bill by the number of people in the group. That way, everybody pays an equal amount regardless of their respective incomes. In this case, everybody would pay 30 USD towards the bill.&lt;/p&gt;
&lt;p&gt;A second option would be to make each person&amp;rsquo;s contribution equal to their ability to provide as measured by their income. Let&amp;rsquo;s say you have one friend who is currently between jobs, i.e. with no income, and your other friend makes twice what you make. If we wanted to scale by income, your friend between jobs would pay nothing towards the bill, you would pay 30 USD, and your friend with greater income would cover the remaining 60 USD of the bill.&lt;/p&gt;
&lt;p&gt;We can expect neither of these two options to be the case with the US tax system, which is decidedly progressive - the more money you earn, the higher a percentage of your income goes towards the federal goverment.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; In my experience, many people actually underestimate how robust the American welfare system is. Blanchet, Chancel and Gethin (2020) for example have shown that the US redistributes a greater share of national income to its low-income population than any European country.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; This paper also shows that the trend in the taxes we find in the IRS data holds after considering additional taxes such as consumption taxes.&lt;/p&gt;
&lt;h2 id=&#34;loading-the-data&#34;&gt;Loading the Data&lt;/h2&gt;
&lt;p&gt;The data we are interested in come in two separate ranges of the Excel sheet. First, we&amp;rsquo;ll load the XLS file using a temporary object:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;filename _httpin temp;

proc http method=&amp;quot;get&amp;quot; url=&amp;quot;https://www.irs.gov/pub/irs-soi/20in41ts.xls&amp;quot; 
		out=_httpin;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then use PROC IMPORT with the &lt;code&gt;range&lt;/code&gt; option to read the relevant ranges of the Excel sheet that we are interested in. For the AGI share information that is the range given by &lt;code&gt;TAB1$A134:P153&lt;/code&gt;. For the tax share, the relevant range is &lt;code&gt;TAB1$A155:P174&lt;/code&gt;.  Both tables are in wide format and will need transposing. Since the code is nearly identical for both ranges, I wrapped it in a macro function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;%macro get_table(name=, range=);
proc import datafile=_httpin out=&amp;amp;name. dbms=XLS replace;
    range=&amp;quot;&amp;amp;range.&amp;quot;;
run;

proc transpose data=&amp;amp;name.(drop=B)
    out=&amp;amp;name.(drop=_LABEL_ rename=(Col1=&amp;amp;name.))
    name=ExcelCol;
    by A;
run;
%mend;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then easily call this for both the tax share and AGI ranges:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;%get_table(name=AGISH, range=TAB1$A134:P153);
%get_table(name=TAXSH, range=TAB1$A155:P174);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One tricky part of this is having to deal with transorming the Excel Column headers into the requisite percentiles. An easy way of doing this is to specify a SAS format that can be applied to the data. Here we go:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc format;
	invalue coln &#39;C&#39;=0.00001 &#39;D&#39;=0.0001 &#39;E&#39;=0.001 &#39;F&#39;=0.01 &#39;G&#39;=0.02 
                 &#39;H&#39;=0.03 &#39;I&#39;=0.04 &#39;J&#39;=0.05 &#39;K&#39;=0.10 &#39;L&#39;=0.20 
                 &#39;M&#39;=0.25 &#39;N&#39;=0.30 &#39;O&#39;=0.40 &#39;P&#39;=0.50;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can now be used with the &lt;code&gt;input&lt;/code&gt; function to relabel our columns as percentiles. At this point we&amp;rsquo;re ready to merge. Here&amp;rsquo;s what the 2020 data looks like:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Top Percentile&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Cumulative Tax Share&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Cumulative AGI Share&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.001%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.143%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.379%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.010%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10.214%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5.530%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.100%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;22.056%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;11.322%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.000%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;42.313%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;22.187%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.000%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;50.375%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;27.723%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.000%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;55.508%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;31.801%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.000%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;59.495%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;35.171%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;5.000%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;62.742%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;38.107%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;10.000%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;73.670%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;49.453%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20.000%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;84.929%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;64.869%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;25.000%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;88.508%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;70.713%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;30.000%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;91.357%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;75.709%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;40.000%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;95.334%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;83.734%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;50.000%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;97.678%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;89.819%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see from this table that US taxes are quite progressive. The Top 2% of tax payers pay about half of all the income taxes paid to the federal government, while earning only a quarter of all the taxable income generated. The bottom 50% of tax payers contribute nearly nothing to these federal taxes (less than 3%). We can also see a glimpse of income inequality - nearly half of all income earned is earned by the top 10% of income earners.&lt;/p&gt;
&lt;h1 id=&#34;some-visualizations&#34;&gt;Some Visualizations&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s start by looking at a few simple visualizations of the data. In the following image, the dashed grey line represents equality between the top income percentiles (x-axis) and the cumulative tax share (y-axis). Anything above the line indicates larger-than-proportional contributions.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-top-tax-paying-percentiles-versus-cumulative-tax-share-the-reference-line-indicated-equal-shares-a-point-above-the-line-indicates-greater-contribution&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-04-are-the-rich-paying-their-fair-share/PVCTSH_hu55c1c4a0f8c1cfe9ac7457262f863057_22020_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Top tax paying percentiles versus cumulative tax share. The reference line indicated equal shares. A point above the line indicates greater contribution.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-04-are-the-rich-paying-their-fair-share/PVCTSH_hu55c1c4a0f8c1cfe9ac7457262f863057_22020_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Top tax paying percentiles versus cumulative tax share. The reference line indicated equal shares. A point above the line indicates greater contribution.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This graph basically looks as expected. We can see three distinct slopes in the graph: the steepest section is for the top 1% of tax payers. The next part is the 2-5% range, and then the 10-50% range is likewise fairly consistent.&lt;/p&gt;
&lt;p&gt;We can use this graph to calculate something akin to a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Gini_coefficient&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gini coefficient&lt;/a&gt;. The Gini coefficient represents the ratio of the area under the Lorenz curve compared to the area under the equality line, which would represent perfect equality between the cumulative population share versus cumulative share of income. A Gini coefficient of zero would imply perfect equality, while a Gini coefficient of one would imply perfect inequality.&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t have the full population size, so our coefficient won&amp;rsquo;t be completely comparable. We will define our tax GINI coefficient, let&amp;rsquo;s call it &amp;ldquo;TINI&amp;rdquo; coefficient, as the ratio of the area between the cumulative Tax Share and the equality line to the total area above the equality line on the box $(0,0.5)\times(0,1)$. The total area above the equality line is $3/8$. We can run through this with a data step using a trapezoidal approximation. See the accompanying SAS script for details.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-my-calculated-tini-coefficient-over-time&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-04-are-the-rich-paying-their-fair-share/TINI_hu18cac775c3802fc7938884e9d5eccaeb_14679_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;My calculated TINI coefficient over time.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-04-are-the-rich-paying-their-fair-share/TINI_hu18cac775c3802fc7938884e9d5eccaeb_14679_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    My calculated TINI coefficient over time.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This TINI coefficient is centered at 0.74 over the data period. While quite stable from about 2008 to 2016, the overall trend has been upwards towards greater inequality. Compare this to the US&#39; 
&lt;a href=&#34;https://fred.stlouisfed.org/graph/?g=12BAp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gini coefficient&lt;/a&gt; which has fluctuated around 0.4 during that same time period. From this, it would appear that the inequality is greater when looking at tax contributions than wealth distribution.&lt;/p&gt;
&lt;p&gt;Another interesting way to look at this data is to think about the ratio of cumulative share of taxes paid over the cumulative share of AGI earned by the different percentiles. For the top 5% of earners, this ratio tends to hover between 1.6 and 1.9 over the data period as can be seen from the plot below.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-dot-plot-of-the-tax-obligation-to-agi-share-over-the-data-set-points-represent-the-mean-with-bars-indicating-1-standard-deviation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-04-are-the-rich-paying-their-fair-share/RATIO_DOT_hu4028af64a11b15502edb51c16a9cb376_11626_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Dot plot of the tax obligation to AGI share over the data set. Points represent the mean, with bars indicating ±1 standard deviation.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-04-are-the-rich-paying-their-fair-share/RATIO_DOT_hu4028af64a11b15502edb51c16a9cb376_11626_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Dot plot of the tax obligation to AGI share over the data set. Points represent the mean, with bars indicating ±1 standard deviation.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Even though I expected to see some inequality due to the progressive nature of the tax system, I was surprised by how &amp;ldquo;top-heavy&amp;rdquo; the tax burden is given the popular rhetoric.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;See 
&lt;a href=&#34;https://www.irs.gov/newsroom/irs-provides-tax-inflation-adjustments-for-tax-year-2023&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this IRS&lt;/a&gt; for a marginal rates for the 2023 tax year. In this year, the rates range from a low of 10% to a high of 37%. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Blanchet, T., Chancel, L., and Gethin, A. (2020), &amp;ldquo;Why Is Europe More Equal than the United States?&amp;rdquo;, &lt;em&gt;American Economic Journal: Applied Economics&lt;/em&gt;, 14 (4), 450&amp;ndash;518. DOI: 
&lt;a href=&#34;https://doi.org/10.1257/app.20200703&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.1257/app.20200703&lt;/a&gt;. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Takeaways from &#39;Deep Work&#39;</title>
      <link>https://dmsenter89.github.io/post/23-04-takeaways-deep-work/</link>
      <pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-04-takeaways-deep-work/</guid>
      <description>&lt;p&gt;I have recently read Cal Newport&amp;rsquo;s book &amp;ldquo;Deep Work&amp;rdquo; (2016). Overall, it is a short but engaging read discussing his tips for how to spend more time doing intellectually focused and engaging work in a society whose attention and focus is ever more divided. Below are my takeways from the book.&lt;/p&gt;
&lt;h2 id=&#34;the-big-picture&#34;&gt;The Big Picture&lt;/h2&gt;
&lt;p&gt;The book focuses on what Newport calls the &amp;ldquo;Deep Work Hypothesis:&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ability to perform deep work is becoming increasingly rare at exactly the same time it is becoming increasingly valuable in our economy. As a consequence, the few who cultivate this skill, and then make it the core of their working life, will thrive.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;He defines &amp;ldquo;deep work&amp;rdquo; as&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Professional activities performed in a state of distraction-free concetration that push your cognitive capabilities to the limit. These efforts create new value, improve your skill, and are hard to replicate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is in contrast to &amp;ldquo;shallow work&amp;rdquo;, which he describes as&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Noncognitively demanding, logistical-style tasks, often performed while distracted. These efforts tend to not create much new value in the world and are easy to replicate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Overall, he splits the book into two parts. The first describes what he considers to be &amp;ldquo;deep work&amp;rdquo; by illustration with various examples. This is coupled with an explanation for why he thinks deep work is valuable, rare, and meaningful, with a chapter dedicated to each of these three topics. In the second part of the book he lists his four major rules for accomplishing an increasing amount of deep work.&lt;/p&gt;
&lt;p&gt;Part 1 of the book struck me as the weakest, but that may be based on my background. The first chapter prepares a good definition of what Cal means by &amp;ldquo;deep work&amp;rdquo; and why it is increasingly valuable in our modern economy. His chapter showing deep work is rare will likely not come as a surprise to most. The key insight here is that while productivity is all the rage these days, most people are actually merely &amp;ldquo;busy,&amp;rdquo; but not productive. His chapter tying deep work to the good life should be familiar to all with even rudimentary exposure to philosophy.&lt;/p&gt;
&lt;p&gt;Here is a brief summary of his four rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Rule #1: Work Deeply.&lt;/strong&gt; His first rule is a bit of a catch-all, describing several different methods of deep work at different levels, but the heart of it is the idea that deep work requires deliberate effort, planning, and ritual.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rule #2: Embrace Boredom.&lt;/strong&gt; The theme of his chapter reminds me a lot of Sir Bertrand Russell and his wonderful book 
&lt;a href=&#34;https://www.themarginalian.org/2015/01/21/bertrand-russell-boredom-conquest-of-happiness/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Conquest of Happiness&lt;/a&gt;, in which boredom plays a prominent role. In essence, we are hurting ourselves by our constant efforts to escape the feeling of boredom. By embracing the idea that boredom is not inherently bad, we can relearn proper focus which aids in deep work.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rule #3: Quit Social Media.&lt;/strong&gt; Here he focuses on one particular source of distraction in our world - social media. The essence of the chapter is that as a society, we are not sufficiently skeptical of the benefit social media consumption offers compared to its pitfalls.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rule #4: Drain the Shallows.&lt;/strong&gt; Part of what keeps us from doing deep work is the distraction of shallow work. This chapter centers around a few different tips for reducing the shallow work load, thereby making time for more deep work. Specifically, he suggests using proper time management, saying no to taking on additional but unnecessary obligations, and restructuring one&amp;rsquo;s email habits.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#the-big-picture&#34;&gt;The Big Picture&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#part-1---the-idea&#34;&gt;Part 1 - The Idea&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#deep-work-is-valuable&#34;&gt;Deep work is Valuable&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#deep-work-is-rare&#34;&gt;Deep Work is Rare&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#deep-work-is-meaningful&#34;&gt;Deep Work is Meaningful&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#part-2---the-rules&#34;&gt;Part 2 - The Rules&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#rule-1-work-deeply&#34;&gt;Rule #1: Work Deeply&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#rule-2-embrace-boredom&#34;&gt;Rule #2: Embrace Boredom&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#rule-3-quit-social-media&#34;&gt;Rule #3: Quit Social Media&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#rule-4-drain-the-shallows&#34;&gt;Rule #4: Drain the Shallows&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#reference&#34;&gt;Reference&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;

&lt;h2 id=&#34;part-1---the-idea&#34;&gt;Part 1 - The Idea&lt;/h2&gt;
&lt;h3 id=&#34;deep-work-is-valuable&#34;&gt;Deep work is Valuable&lt;/h3&gt;
&lt;p&gt;This chapter is largely a reflection of the economic import of deep work for knowledge workers. Our economy is shifting and even prior to COVID, regional limitations on finding workers have started to become less meaningful. That means that knowledge workers compete with an increasing amount of other knowledge workers. At the same time, technological advances make technological skill and adaptability ever more important.&lt;/p&gt;
&lt;p&gt;Newport identifies two core abilities for thriving in our new economy:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;The ability to quickly master hard things.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;The ability to produce at an elite level, in terms of both quality and quantity.&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Learning hard things quickly is identified with deep work.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To learn hard things quickly, you must focus intensely without distraction. To learn, in other words, is an act of deep work. If you&amp;rsquo;re comfortable going deep, you&amp;rsquo;ll be comfortable mastering the increasingly complex systems and skills needed to thrive in our economy. If you instead remain one of the many for whom depth is uncomfortable and distraction ubiquitous, you shouldn&amp;rsquo;t expect these systems and skills to come easily to you.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Improving your skill requires deliberate practice, whose two key components he lists as&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;focusing your attention tightly on a specifc skill you are attempting to improve, and&lt;/li&gt;
&lt;li&gt;receiving feedback that allows for correction without loss of attention.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;deep-work-is-rare&#34;&gt;Deep Work is Rare&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The Principle of Least Resistance: In a business setting, without clear feedback on the impact of various behaviors to the bottom line, we will tend toward behaviors that are easiest in the moment.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Many knowledge workers struggle with being productive. This is despite exhibiting external signs of &amp;ldquo;productivity:&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Knowledge workers, I&amp;rsquo;m arguing, are tending toward increasingly visible busyness because they lack a better way to demonstrate their value. Let&amp;rsquo;s give this tendency a name. Busyness as Proxy for Productivity: In the absence of clear indicators of what it means to be productive and valuable in their jobs, many knowledge workers turn back toward an industrial indicator of productivity: doing lots of stuff in a visible manner.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The problem he identifies is that it is difficult for a knowledge worker to demonstrate their value, and that modern society has developed a &amp;ldquo;productivity-fetish&amp;rdquo; for lack of  better term. Everything should be measured and quantified and be &amp;ldquo;efficient.&amp;rdquo; The difficulty with this is that knowledge-work is inherently different from the type of labor many of these productivity measures are derived from.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Knowledge work is not an assembly line, and extracting value from information is an activity that&amp;rsquo;s often at odds with busyness, not supported by it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All this amounts to actual productivity, which is ascribed to deep work, being rare.&lt;/p&gt;
&lt;h3 id=&#34;deep-work-is-meaningful&#34;&gt;Deep Work is Meaningful&lt;/h3&gt;
&lt;p&gt;The previous chapters provided external motivation for deep work. In this chapter, Cal argues that deep work is &amp;ldquo;meaningful,&amp;rdquo; in the sense of being part of living &amp;ldquo;the good life.&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The goal of this chapter is to convince you that deep work can generate as much satisfaction in an information economy as it so clearly does in a craft economy. [&amp;hellip;]  The thesis of this final chapter in Part 1, therefore, is that a deep life is not just economically lucrative, but also a life well lived.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;He points out research concerning the effects of deliberate attention in general, and how we allocate deliberate attention. &amp;ldquo;Skillful management of attention is the sine qua non of the good life and the key to improving virtually every aspect of your experience.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Deep work aids a person in achieving happiness in two ways. One, it provides distraction that keeps us from noticing the &amp;ldquo;many smaller and less pleasant things that unavoidably and persistently populate our lives:&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Our brains instead construct our worldview based on what we pay attention to. If you focus on a cancer diagnosis, you and your life become unhappy and dark, but if you focus instead on an evening martini, you and your life become more pleasant—even though the circumstances in both scenarios are the same. As Gallagher summarizes: “Who you are, what you think, feel, and do, what you love — is the sum of what you focus on.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Second, being engaged in deep work is connected to flow:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Deep work is an activity well suited to generate a flow state (the phrases used by Csikszentmihalyi to describe what generates flow include notions of stretching your mind to its limits, concentrating, and losing yourself in an activity—all of which also describe deep work).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since flow has been shown to generate happiness, deep work is argued to generate happiness as well.&lt;/p&gt;
&lt;h2 id=&#34;part-2---the-rules&#34;&gt;Part 2 - The Rules&lt;/h2&gt;
&lt;h3 id=&#34;rule-1-work-deeply&#34;&gt;Rule #1: Work Deeply&lt;/h3&gt;
&lt;p&gt;Cal lists four main methods, or &amp;ldquo;philosophies,&amp;rdquo; for practicing deep work:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The &lt;em&gt;monastic philosophy&lt;/em&gt;. What is sounds like. Isolate yourself completely from shallow work for extended periods of time. Same famous examples are cited, but Cal notes this is not realistic for most workers. You can imagine your employer not taking kindly to not being able to reach you at all for a few weeks at a time because you are secluded into a deep work state.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;em&gt;bimodal philosophy.&lt;/em&gt; Exemplified in the book by Carl Jung. Divide your time into clearly defined stretches of deep-work, at least a full day but preferably several days a week. During these stretches of time you essentially act monastically. This method is substantially more realistic for most workers, particularly for academics like Cal. As an example, you could set aside Thursdays/Fridays for deep work. During these days, you wouldn&amp;rsquo;t be checking email, be on Teams/Slack, you would have no meetings, etc. You just focus on whatever deep work project you are working on. During the rest of the week on the other hand, you&amp;rsquo;d act like a regular worker with meetings, email checking, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;em&gt;rhythmic philosophy.&lt;/em&gt; This method focuses most heavily on routines. Create a regular deep work routine during each work day. Create a consistent block of time set aside for deep work, each and every day. While you don&amp;rsquo;t have the same amount of time to dig as &amp;ldquo;deep&amp;rdquo; as in the previous methods, &amp;ldquo;by supporting deep work with rock-solid routines that make sure a little bit gets done on a regular basis, the rhythmic scheduler will often log a larger total number of deep hours per year.&amp;rdquo; This method is also being pushed into more office worker&amp;rsquo;s consciousness with Microsoft&amp;rsquo;s Viva Insights. Now integrated into Outlook, it suggests automatically scheduling &amp;ldquo;focus hours&amp;rdquo; for you, helping you get a head start with this method.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;em&gt;journalistic philophy.&lt;/em&gt; Basically switch between deep and shallow work in undefined blocks, perhaps multiple times a day. Not recommended as most people are not able to do this, and even if they can they do so only with substantial training.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Overall, the rhythmic and bimodal philosphies seem to be most achievable for most workers. He gives further support for the rhythmic philosophy by noting research that suggests a person can only engage in deep work for about one to four hours a day, depending on the level of training they have.&lt;/p&gt;
&lt;p&gt;Cal thoroughly stresses the need for routines in helping someone get into a deep work state of mind.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The key to developing a deep work habit is to move beyond good intentions and add routines and rituals to your working life designed to minimize the amount of your limited willpower necessary to transition into and maintain a state of unbroken concentration.&lt;/p&gt;
&lt;p&gt;There is a popular notion that artists work from inspiration—that there is some strike or bolt or bubbling up of creative mojo from who knows where… but I hope [my work] makes clear that waiting for inspiration to strike is a terrible, terrible plan. In fact, perhaps the single best piece of advice I can offer to anyone trying to do creative work is to ignore inspiration. In a New York Times column on the topic, David Brooks summarizes this reality more bluntly: “[Great creative minds] think like artists but work like accountants.”&lt;/p&gt;
&lt;p&gt;To make the most out of your deep work sessions, build rituals of the same level of strictness and idiosyncrasy as the important thinkers mentioned previously. There’s a good reason for this mimicry. Great minds like Caro and Darwin didn’t deploy rituals to be weird; they did so because success in their work depended on their ability to go deep, again and again—there’s no way to win a Pulitzer Prize or conceive a grand theory without pushing your brain to its limit.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This means you have to plan your deep work sessions ahead of time. Decide&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Where you’ll work and for how long.&lt;/li&gt;
&lt;li&gt;How you’ll work once you start to work.&lt;/li&gt;
&lt;li&gt;How you’ll support your work.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;He discusses the 4DX framework, originally idented for businesses, and adapts it to personal deep work. The 4DX framework is built up of four key disciplines:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Focus on the Wildly Important&lt;/li&gt;
&lt;li&gt;Act on the Lead Measures&lt;/li&gt;
&lt;li&gt;Keep a Compelling Scoreboard&lt;/li&gt;
&lt;li&gt;Create a Cadence of Accountability&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Lastly, he argues for the importrance of &amp;ldquo;injecting regular and substantial freedom from professional concerns into your day, providing you with the idleness paradoxically required to get (deep) work done.&amp;rdquo; This is discussed in the context of 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Attention_restoration_theory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;attention restoration theory&lt;/a&gt;. The basic idea is that concentration, or &amp;ldquo;directed attention,&amp;rdquo; is a finite resource that can get exhausted. For this reason, he argues to keep all work strictly confined to the work day. This way, you can spend your time outside of work hours recharging. This in turn helps you be more productive while working. To facilitate this move away from work and towards attention restoring activities, he argues for a commitment to a strict shutdown ritual. This is Cal&amp;rsquo;s description:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In more detail, this ritual should ensure that every incomplete task, goal, or project has been reviewed and that for each you have confirmed that either (1) you have a plan you trust for its completion, or (2) it’s captured in a place where it will be revisited when the time is right. The process should be an algorithm: a series of steps you always conduct, one after another. When you’re done, have a set phrase you say that indicates completion (to end my own ritual, I say, “Shutdown complete”). This final step sounds cheesy, but it provides a simple cue to your mind that it’s safe to release work-related thoughts for the rest of the day.  [&amp;hellip;]  The concept of a shutdown ritual might at first seem extreme, but there’s a good reason for it: the Zeigarnik effect.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;rule-2-embrace-boredom&#34;&gt;Rule #2: Embrace Boredom&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The ability to concentrate intensely is a skill that must be trained.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Deep work requires focus, and sometimes this focus can be difficult and uncomfortable. In our society we are accustomed to constantly available, on-demand distraction that can keep us from having to have uncomfortable moments where we are left alone with our thoughts. These distractions keep us from being bored. But this comes with a downside: the more accustomed we are to the easy availability of distraction, the harder it becomes to focus when we need to. This problem is particularly pronounced in individuals who frequently multitask:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;People who multitask all the time can’t filter out irrelevancy. They can’t manage a working memory. They’re chronically distracted. They initiate much larger parts of their brain that are irrelevant to the task at hand&amp;hellip; they’re pretty much mental wrecks. [&amp;hellip;] Once your brain has become accustomed to on-demand distraction, Nass discovered, it’s hard to shake the addiction even when you want to concentrate. To put this more concretely: If every moment of potential boredom in your life—say, having to wait five minutes in line or sit alone in a restaurant until a friend arrives—is relieved with a quick glance at your smartphone, then your brain has likely been rewired to a point where, like the “mental wrecks” in Nass’s research, it’s not ready for deep work—even if you regularly schedule time to practice this concentration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To counteract this, you need to train yourself with two goals in mind: &amp;ldquo;improving your ability to concentrate intensely and overcoming your desire for distraction.&amp;rdquo; As part of this training, Cal suggests a type of intermittent &amp;ldquo;internet Sabbath.&amp;rdquo; The intent is to schedule a break from focus where you&amp;rsquo;re allowed to give in to distraction.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;With these rough categorizations established, the strategy works as follows: Schedule in advance when you’ll use the Internet, and then avoid it altogether outside these times. I suggest that you keep a notepad near your computer at work. On this pad, record the next time you’re allowed to use the Internet. Until you arrive at that time, absolutely no network connectivity is allowed—no matter how tempting.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;He emphasizes the need to rigoursly stick to this schedule and really, completely avoid the internet during the scheduled focus time. For best results, he suggests restricting internet use at home as well as at work. This further helps train yourself to use the internet as an intentional tool, as opposed to as an escape mechanism from potential boredom.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To summarize, to succeed with deep work you must rewire your brain to be comfortable resisting distracting stimuli. This doesn’t mean that you have to eliminate distracting behaviors; it’s sufficient that you instead eliminate the ability of such behaviors to hijack your attention. The simple strategy proposed here of scheduling Internet blocks goes a long way toward helping you regain this attention autonomy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;He also suggests taking up what he calls &lt;em&gt;productive meditation.&lt;/em&gt; Essentially, taking a time where you are physically but not mentally occupied (e.g., walking, jogging, driving) and focusing on a single, well-defined problem. This activity helps practice focusing and makes productive use of these times we might otherwise wast by distracting ourselves with things like podcasts.&lt;/p&gt;
&lt;p&gt;Lastly, he suggets practicing memorization. He gives the example of learning how to memorize a deck of cards. The point here is not the party-trick of being able to quickly memorize a deck of cards, but rather to provide a workout for the mind.&lt;/p&gt;
&lt;h3 id=&#34;rule-3-quit-social-media&#34;&gt;Rule #3: Quit Social Media&lt;/h3&gt;
&lt;p&gt;This chapter focuses on one particular source of distraction - social media. He lists sites like FaceBook, Twitter and Instagram (the book was written prior to TikTok). Most people are signed up for several of these services, despite them offering limited benefits in Cal&amp;rsquo;s view. He proposes the following root-cause of this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Any-Benefit Approach to Network Tool Selection: You’re justified in using a network tool if you can identify any possible benefit to its use, or anything you might possibly miss out on if you don’t use it. The problem with this approach, of course, is that it ignores all the negatives that come along with the tools in question. These services are engineered to be addictive—robbing time and attention from activities that more directly support your professional and personal goals (such as deep work).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;He repeatedly emphasizes that there are legitimate benefits to social media; his point is not that they are morally &amp;ldquo;bad&amp;rdquo; or &amp;ldquo;useless,&amp;rdquo; but rather that they are attention robbing while providing limited benefit. In other words, we can spend our time more profitably by doing something else with it rather than scrolling through FaceBook or TikTok feeds. Our days are short and we only have limited time. Spending time on social media typically is either (1) a distraction, filling otherwise empty space (see his 
&lt;a href=&#34;#rule-2-embrace-boredom&#34;&gt;rule #2&lt;/a&gt; above), or (2) a suboptimal use of our free time.&lt;/p&gt;
&lt;p&gt;Cal proposes the following alternative:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Craftsman Approach to Tool Selection: Identify the core factors that determine success and happiness in your professional and personal life. Adopt a tool only if its positive impacts on these factors substantially outweigh its negative impacts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;He suggests writing a list of high-level gaols of what&amp;rsquo;s most important in your personal and professional life, then listing the two or three most important activities that can help you satisfy that goal. If a networking tool doesn&amp;rsquo;t fit with your goals, it might be more of a distraction than a useful tool. We might worry about missing out if we eliminate some or all of our social media, but&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Stuff accumulates in people’s lives, in part, because when faced with a specific act of elimination it’s easy to worry, “What if I need this one day?,”and then use this worry as an excuse to keep the item in question sitting around. Nicodemus’s packing party provided him with definitive evidence that most of his stuff was not something he needed, and it therefore supported his quest to simplify.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Cal suggests cold-quitting the &lt;em&gt;use&lt;/em&gt; of all social media for 30 days, without announcing it. Note that he suggests quitting the use, not shutting the services. Re-evaluate your use of the services after these 30 days. Overall, Cal argues that we deserve to put more thought into our leisure activities, and shouldn&amp;rsquo;t just default to whatever is easily available at the moment. Even if the torrent of funny, short videos is funny.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To summarize, if you want to eliminate the addictive pull of entertainment sites on your time and attention, give your brain a quality alternative. Not only will this preserve your ability to resist distraction and concentrate, but you might even fulfill Arnold Bennett’s ambitious goal of experiencing, perhaps for the first time, what it means to live, and not just exist.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;rule-4-drain-the-shallows&#34;&gt;Rule #4: Drain the Shallows&lt;/h3&gt;
&lt;p&gt;This chapter focuses on how to make more room for deep work by reducing the amount of low-value shallow work. He acknowledges that some shallow work is necessary, so he doesn&amp;rsquo;t encourage us to &amp;ldquo;quixotically pursue a schedule in which all of [our] time is invested in depth.&amp;rdquo; Nevertheless, he argues much shallow work can be eliminated without loss since it&amp;rsquo;s value is frequently overestimated.&lt;/p&gt;
&lt;p&gt;Part of his advice is to utilize &lt;strong&gt;block-scheduling&lt;/strong&gt; as a time management routine.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here&amp;rsquo;s my suggestion: At the beginning of each workday, turn to a new page of lined paper in a notebook you dedicate to this purpose. Down the left-hand side of the page, mark every other line with an hour of the day, covering the full set of hours you typically work. Now comes the important part: Divide the hours of your workday into blocks and assign activities to the blocks. For example, you might block off nine a.m. to eleven a.m. for writing a client&amp;rsquo;s press release. To do so, actually draw a box that covers the lines corresponding to these hours, then write “press release”inside the box. Not every block need be dedicated to a work task. There might be time blocks for lunch or relaxation breaks. To keep things reasonably clean, the minimum length of a block should be thirty minutes (i.e., one line on your page). This means, for example, that instead of having a unique small box for each small task on your plate for the day—respond to boss&amp;rsquo;s e-mail, submit reimbursement form, ask Carl about report—you can batch similar things into more generic task blocks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;He advises not to stick too rigidly to these blocks, but instead use them to make sure you use your time intentionally instead of haphazardly. He also advises scheduling more time than you think you need at first, until you are used to the method and have a better sense of being able to gauge how long tasks will actually take when you give them your full attention.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To summarize, the motivation for this strategy is the recognition that a deep work habit requires you to treat your time with respect. A good first step toward this respectful handling is the advice outlined here: Decide in advance what you’re going to do with every minute of your workday. It’s natural, at first, to resist this idea, as it’s undoubtedly easier to continue to allow the twin forces of internal whim and external requests to drive your schedule. But you must overcome this distrust of structure if you want to approach your true potential as someone who creates things that matter.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When deciding on a task schedule, &lt;strong&gt;quantify the depth of every activity&lt;/strong&gt;. This can help in both prioritization and in figuring out what is actually shallow work masquerading as deep work. He advocates the following heuristic in judging where an activity falls on the shallow-depth continuum:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;evaluate activities by asking a simple (but surprisingly illuminating) question: How long would it take (in months) to train a smart recent college graduate with no specialized training in my field to complete this task?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The shorter the amount of time, the more shallow the task. Conversely, the longer it would take, the more important that task is for you as an individual since it utilizes the skills you have acquired and puts you to &amp;ldquo;best use.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Expanding on a topic brought up in rule #1, he advocates for what he terms &lt;strong&gt;fixed-schedule productivity&lt;/strong&gt;. In essence: fix a time of day that ends your work day (he suggests 5:30 PM). From there, work backwards into making your day&amp;rsquo;s work fit that schedule. This way you are guaranteed to be done with work and available to engage in attention restoration and living your life outside of work, while at the same time nudging yourself to use the now more limited work time to the best of your ability.&lt;/p&gt;
&lt;p&gt;Since you will have more limited time in your workday, this motivates you to start &lt;strong&gt;saying no&lt;/strong&gt; to potential commitments that don&amp;rsquo;t actually further your professional goals significantly. He brings up his own example of earning tenure and that of a colleague to show that even though we might think we need to say &amp;ldquo;yes&amp;rdquo; to all opportunities and work all hours of the day in order to succeed, we may not need to do either if we use our time productively.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To summarize these observations, Nagpal and I can both succeed in academia without Tom-style overload due to two reasons. First, we’re asymmetric in the culling forced by our fixed-schedule commitment. By ruthlessly reducing the shallow while preserving the deep, this strategy frees up our time without diminishing the amount of new value we generate. Indeed, I would go so far as to argue that the reduction in shallow frees up more energy for the deep alternative, allowing us to produce more than if we had defaulted to a more typical crowded schedule. Second, the limits to our time necessitate more careful thinking about our organizational habits, also leading to more value produced as compared to longer but less organized schedules. The key claim of this strategy is that these same benefits hold for most knowledge work fields.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This observation definitely matches up with my own personal observations of academic life. An incredible amount of time is wasted there at various levels.&lt;/p&gt;
&lt;p&gt;His final piece of advice is mostly related to email - &amp;ldquo;&lt;strong&gt;become hard to reach&lt;/strong&gt;.&amp;rdquo; Email sucks up a large amount of time in many workers&#39; day. And if we really think about it, many emails are not effective uses of our time. He offers three tips for reducing the impact of this potential time waster:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Make People Who Send You E-mail Do More Work.&lt;/strong&gt; Set people up with the expectation that you won&amp;rsquo;t reply unless it is actually worth your time. It is common for people to expect replies to something as mundane and pointless as a forwarded email with the single line &amp;ldquo;what are your thougths?&amp;rdquo; prefixed to it. Start to make clear that you won&amp;rsquo;t respond to emails of this type. Raise the expectation that people need to communicate to you why you ought to reply to the email in the first place. This can be done by using a &lt;em&gt;sender filter.&lt;/em&gt; On your website, where you list your email, you could communicate that you won&amp;rsquo;t respond unless the message fits your schedule and interests. By communicating what you will respond to ahead of time, you can reset people&amp;rsquo;s expectation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Do More Work When You Send or Reply to E-mails.&lt;/strong&gt; This is related to the above. Just as you won&amp;rsquo;t reply to pointless emails, don&amp;rsquo;t write pointless emails. But also think ahead and make sure your emails include all of the necessary information. This avoids short, chat-style back-and-forths. As a particular example, this goes with one of my pet peeves: people sending meeting requests with absolutely no indication of what times they might be available. Don&amp;rsquo;t do that. If you want to meet with somebody, include &lt;em&gt;why&lt;/em&gt; you want to meet and give &lt;em&gt;several&lt;/em&gt;  time windows. If your organization uses Outlook and basic calendar sharing, take a look at the other parties calenders and make sure to only suggest times that aren&amp;rsquo;t already blocked off on their calendars. That way, finding a meeting time can happen in two or three emails as opposed to five. In more detail, he writes:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;pause a moment before replying [to an email] and take the time to answer the following key prompt: What is the project represented by this message, and what is the most efficient (in terms of messages generated) process for bringing this project to a successful conclusion? Once you’ve answered this question for yourself, replace a quick response with one that takes the time to describe the process you identified, points out the current step, and emphasizes the step that comes next. I call this the process-centric approach to e-mail, and it’s designed to minimize both the number of e-mails you receive and the amount of mental clutter they generate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Just &lt;strong&gt;don&amp;rsquo;t respond.&lt;/strong&gt; You can use this technique as a heuristic for gauging when to not respond to an email:&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Professorial E-mail Sorting: Do not reply to an e-mail message if any of the following applies:&lt;/p&gt;
&lt;p&gt;• It&amp;rsquo;s ambiguous or otherwise makes it hard for you to generate a reasonable response.&lt;/p&gt;
&lt;p&gt;• It&amp;rsquo;s not a question or proposal that interests you.&lt;/p&gt;
&lt;p&gt;• Nothing really good would happen if you did respond and nothing really bad would happen if you didn&amp;rsquo;t.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;Newport, C. (2016), &lt;em&gt;Deep Work: Rules for Focused Success in a Distracted World&lt;/em&gt;, New York, NY: Grand Central Publishing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Tests as Linear Models</title>
      <link>https://dmsenter89.github.io/post/23-04-statistical-tests-as-linear-models/</link>
      <pubDate>Wed, 05 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-04-statistical-tests-as-linear-models/</guid>
      <description>&lt;p&gt;Andrew Gelman&amp;rsquo;s 
&lt;a href=&#34;https://statmodeling.stat.columbia.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;statmodeling blog&lt;/a&gt; recently contained a link to an interesting document by 
&lt;a href=&#34;http://personprofil.aau.dk/117060&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonas Lindeløv&lt;/a&gt;. It tries to explain various statistical tests in terms of linear models. Here&amp;rsquo;s the chart from the post:&lt;/p&gt;















&lt;figure id=&#34;figure-the-summary-chart-from-lindeløvs-post&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.png&#34; data-caption=&#34;The summary chart from Lindeløv&amp;amp;rsquo;s post.&#34;&gt;


  &lt;img src=&#34;https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The summary chart from Lindeløv&amp;rsquo;s post.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The entire notebook can be viewed 
&lt;a href=&#34;https://lindeloev.github.io/tests-as-linear/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, and Andrew&amp;rsquo;s comments are available 
&lt;a href=&#34;https://statmodeling.stat.columbia.edu/2023/03/30/replacing-the-zoo-of-named-tests-by-linear-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sharing SSH Keys With a Devcontainer</title>
      <link>https://dmsenter89.github.io/post/23-04-sharing-ssh-keys-with-devcontainer/</link>
      <pubDate>Tue, 04 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-04-sharing-ssh-keys-with-devcontainer/</guid>
      <description>&lt;p&gt;VS Code devcontainers are a great resource for creating reusable containers to share between developers on the same project. When properly setup, it automatically passes your SSH credentials to the container. When this is not set up, the git push/pull functionality in VS Code won&amp;rsquo;t work (you will still be able to make commits in the devcontainer and then push/pull from the CLI you launched Code with).&lt;/p&gt;
&lt;p&gt;The way to do this is to use an SSH agent to forward your credentials. On most systems these aren&amp;rsquo;t started automatically, so for convenience you will probably want to add the start up to your &lt;code&gt;.bash_profile&lt;/code&gt; or &lt;code&gt;.bashrc&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I have found the following useful; it includes a short check to make sure you aren&amp;rsquo;t running multiple ssh-agents in one session:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;SSH_ENV=&amp;quot;$HOME/.ssh/agent-environment&amp;quot;

function start_agent {
    echo &amp;quot;Initialising new SSH agent...&amp;quot;
    /usr/bin/ssh-agent | sed &#39;s/^echo/#echo/&#39; &amp;gt; &amp;quot;${SSH_ENV}&amp;quot;
    echo succeeded
    chmod 600 &amp;quot;${SSH_ENV}&amp;quot;
    . &amp;quot;${SSH_ENV}&amp;quot; &amp;gt; /dev/null
    /usr/bin/ssh-add;
}

# Source SSH settings, if applicable
if [ -f &amp;quot;${SSH_ENV}&amp;quot; ]; then
    . &amp;quot;${SSH_ENV}&amp;quot; &amp;gt; /dev/null
    ps -ef | grep ${SSH_AGENT_PID} | grep ssh-agent$ &amp;gt; /dev/null || {
        start_agent;
    }
else
    start_agent;
fi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can verify that your keys are working by running &lt;code&gt;ssh-add -l&lt;/code&gt; from the VS Code terminal. This should print your host SSH key.&lt;/p&gt;
&lt;p&gt;See also the 
&lt;a href=&#34;https://code.visualstudio.com/remote/advancedcontainers/sharing-git-credentials&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Takeaways - &#39;Big Data Is Dead&#39;</title>
      <link>https://dmsenter89.github.io/post/23-03-takeaways-big-data-is-dead/</link>
      <pubDate>Wed, 22 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-03-takeaways-big-data-is-dead/</guid>
      <description>&lt;p&gt;I recently read a great 
&lt;a href=&#34;https://motherduck.com/blog/big-data-is-dead/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt; by Jordan Tigani about Big Data. While Jordan&amp;rsquo;s post focuses on enterprise needs, I believe it contains relevant insights to individual researchers as well.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;ve had any exposure to the technology space in the past decade, you will have heard of big data. Advances in storage capabilities have unleashed a massive data collection effort across the board. Everyone was excited and assumed that soon we would be completely inundated by data. New methods and technology were needed so we won&amp;rsquo;t drown in all that data and lose out on important insights along the way.&lt;/p&gt;
&lt;p&gt;While the amount of data stored has grown significantly, it has not grown as massively as many had feared. Jordan mentions that most enterprises require approximately 100 GB of storage for their data warehousing needs. More interesting to me though is the mentioned difference between compute and storage needs. In his experience, the vast majority of compute queries used 100 MB or less of data - even for companies with terrabytes of data in storage.&lt;/p&gt;
&lt;p&gt;Intuitively this should make sense. As data accumulate, a large portion of it will be historic and won&amp;rsquo;t need continuing re-analyis. There are also many techniques that have been developed to update models as data come in, so that you don&amp;rsquo;t need to re-run the entire modeling process each time you get new data, further reducing compute needs. A good example are large language models like ChatGPT. Training the model requires massive amounts of data and compute capabilities, but once the model is trained, storing and working with the computed weights is nearly trivial by comparison.&lt;/p&gt;
&lt;p&gt;So far for the blog post. Let&amp;rsquo;s turn to the academic space. The rising popularity of big data and big data concepts has started spilling over into academic research outside of cs/math/stats as well. I encounter incrising numbers of academics in other fields referring to their data as &amp;ldquo;big&amp;rdquo; or aspiring to have &amp;ldquo;big data.&amp;rdquo; The numbers in Jordan&amp;rsquo;s post are a good reminder of what we are actually talking about when discussing big data and related concepts. Jordan quotes the definition that I tend to give researchers that aren&amp;rsquo;t familiar with the big data field: big data is &amp;ldquo;whatever doesn&amp;rsquo;t fit on a single machine.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Using that definition, average researchers hoping for &amp;ldquo;big data&amp;rdquo; run into the issue of massive improvements in storage and RAM on consumer grade laptops. Hundreds of gigabytes of SSD storage are the norm and laptop reviews these days frequently complain if a laptop is equipped with less than 8 GB of RAM. Outgrowing those needs will be challening for individual researchers or small teams of researchers in most fields, considering an Excel file with 360,000 observations and 90 variables is still only about 90 MB.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; If you&amp;rsquo;re able to use Excel, you are very, very far removed from having big data - even if your data set is significantly larger than what is typical in your field.&lt;/p&gt;
&lt;p&gt;But that&amp;rsquo;s frankly not a bad thing. Most research doesn&amp;rsquo;t require big data. Acquiring &lt;em&gt;quality&lt;/em&gt; big data is also very difficult. As is dealing with big data tools and infrastructure if you&amp;rsquo;re not used to it. And that&amp;rsquo;s not even touching on the analytical methodology relevant to big data. So most researchers who aren&amp;rsquo;t already in the big data field probably shouldn&amp;rsquo;t worry about it.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;Tigani, J. (2023), &amp;ldquo;Big Data is Dead&amp;rdquo;, MotherDuck, Available at 
&lt;a href=&#34;http://web.archive.org/web/20230318062005/https://motherduck.com/blog/big-data-is-dead/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;motherduck.com/blog/big-data-is-dead&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;See 
&lt;a href=&#34;https://www.quora.com/Roughly-how-big-is-an-Excel-file-containing-1-million-rows-and-20-columns&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this quora answer&lt;/a&gt;. For illustration purposes only. Don&amp;rsquo;t use Excel for that many rows/columns. Just avoid Excel in general. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>PROC MI Added to SASPy</title>
      <link>https://dmsenter89.github.io/post/23-02-proc-mi-added-to-saspy/</link>
      <pubDate>Mon, 06 Feb 2023 14:45:00 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/23-02-proc-mi-added-to-saspy/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m excited to announce that the new 
&lt;a href=&#34;https://github.com/sassoftware/saspy/releases/tag/v4.6.0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAPy v4.6.0&lt;/a&gt; release includes a pull request of mine that adds 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/statug/15.2/statug_mi_toc.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PROC MI&lt;/a&gt; to the SAS/STAT procedures directly exposed in SASPy. This procedure allows you to analyze missing data patterns and create imputations for missing data.&lt;/p&gt;
&lt;h2 id=&#34;syntax&#34;&gt;Syntax&lt;/h2&gt;
&lt;p&gt;PROC MI is accessed via the &lt;code&gt;mi&lt;/code&gt; function that has been added to the &lt;code&gt;SASstat&lt;/code&gt; class. Like other procedures, the SAS statements in MI are called as keyword arguments to the function whose name matches the SAS 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/statug/15.2/statug_mi_syntax.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;syntax&lt;/a&gt;:&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;PROC MI options;
  BY variables;
  CLASS variables;
  EM &amp;lt;options&amp;gt;;
  FCS &amp;lt;options&amp;gt;;
  FREQ variable;
  MCMC &amp;lt;options&amp;gt;;
  MNAR options;
  MONOTONE &amp;lt;options&amp;gt;;
  TRANSFORM transform (variables&amp;lt;/ options&amp;gt;) &amp;lt;…transform (variables&amp;lt;/ options&amp;gt;)&amp;gt;;
  VAR variables;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the corresponding function signature in Python:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def mi(self, data: (&#39;SASdata&#39;, str) = None,
        by: (str, list) = None,
        cls: (str, list) = None,
        em: str = None,
        fcs: str = None,
        freq: str = None,
        mcmc: str = None,
        mnar: str = None,
        monotone: str = None,
        transform: str = None,
        var: str = None,
        procopts: str = None,
        stmtpassthrough: str = None,
        **kwargs: dict) -&amp;gt; &#39;SASresults&#39;:
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Statements  like &lt;code&gt;EM&lt;/code&gt; or &lt;code&gt;MCMC&lt;/code&gt;, which can stand alone in SAS, are called with an empty string argument in Python.&lt;/p&gt;
&lt;h2 id=&#34;basic-example&#34;&gt;Basic Example&lt;/h2&gt;
&lt;!-- 
```python saspy_mi.py
#!/usr/bin/env python3
#
# Example: how to access PROC MI with SASPy. To accompany
#   dmsenter89.github.io/post/23-02-proc-mi-added-to-saspy
# 
# Author: Michael Senter, PhD

import saspy


# starting the SAS Session
&lt;&lt;&lt;session&gt;&gt;&gt;

&lt;&lt;&lt;procmi&gt;&gt;&gt;

&lt;&lt;&lt;sasdata&gt;&gt;&gt;


# ending the SAS session 
sas.endsas()
```
--&gt;
&lt;p&gt;To use the new MI functionality, make sure you have updated to the newest SASPy release. In addition to starting  a SAS Session as per usual, you will also want to enable access to the SAS/STAT procedures:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sas = saspy.SASsession()  # loads a session using your default profile
stat = sas.sasstat()      # gives access to SAS/STAT procedures 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once these session objects are loaded, you can start using the mi function with &lt;code&gt;stat.mi&lt;/code&gt;. The simplest possible call is to invoke MI with a built-in data set and all defaults as &lt;code&gt;stat.mi(data=&#39;sashelp.heart&#39;)&lt;/code&gt;. For best results, store the output in a SASResults object. From there you can access the SAS log associated with the function call (&lt;code&gt;LOG&lt;/code&gt;) as well as all ODS Output using the ODS 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/statug/15.2/statug_mi_details82.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;table names&lt;/a&gt; in all caps. The default uses the EM method with 25 imputations.&lt;/p&gt;
&lt;p&gt;A more realistic use might look something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ods = stat.mi(data=&#39;sashelp.heart&#39;, em=&amp;quot;outem=outem&amp;quot;,
              var=&amp;quot;Cholesterol Height Smoking Weight&amp;quot;,
              procopts=&amp;quot;simple nimpute=20 out=imp&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is equivalent to running&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;proc mi data=sashelp.heart simple nimpute=20 out=imp;
    em outem=outem;
    var Cholesterol Height Smoking Weight;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in SAS. This call uses the EM procedure to impute values for the cholesterol, height, smoking, and weight variables. The &lt;code&gt;simple&lt;/code&gt; option displays univariate statistics and correlations. The &lt;code&gt;outem&lt;/code&gt; option saves a data set containing the computed MLE to &lt;code&gt;work.outem&lt;/code&gt;. The imputed data sets are saved to &lt;code&gt;work.imp&lt;/code&gt;, which contains the additional variable &lt;code&gt;_IMPUTATION_&lt;/code&gt; with the imputation number. This can be used as a &lt;code&gt;by&lt;/code&gt; variable in other procedures, and the results can later be pooled using PROC MIANALYZE.&lt;/p&gt;
&lt;p&gt;The resulting &lt;code&gt;ods&lt;/code&gt; object for our example exposes the following ODS outputs to your Python instance, in addition to the log:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;CORR&#39;, &#39;EMESTIMATES&#39;, &#39;EMINITESTIMATES&#39;, &#39;EMPOSTESTIMATES&#39;, &#39;MISSPATTERN&#39;, &#39;MODELINFO&#39;, &#39;PARAMETERESTIMATES&#39;, &#39;UNIVARIATE&#39;, &#39;VARIANCEINFO&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See the SAS documentation for details. To use the imputed data with Python tools, create a SAS data object. We&amp;rsquo;ll also print the first few entries so we can see what it looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;imputed = sas.sasdata(table=&amp;quot;imp&amp;quot;, libref=&amp;quot;work&amp;quot;)
imputed.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;One exception is the SAS &lt;code&gt;class&lt;/code&gt; statement, which is implemented as &lt;code&gt;cls&lt;/code&gt; due to &lt;code&gt;class&lt;/code&gt; being a reserved keyword in Python. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Why I&#39;m Not Worried About ChatGPT</title>
      <link>https://dmsenter89.github.io/post/23-01-why-im-not-worried-about-chatgpt/</link>
      <pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-01-why-im-not-worried-about-chatgpt/</guid>
      <description>&lt;p&gt;ChatGPT has been all over my newsfeed lately, with a considerable amount of hype. In particular, many are wondering or  even worrying whether the emergence of this technology will threaten jobs with moderate to high education requirments. See for example 
&lt;a href=&#34;https://www.theatlantic.com/ideas/archive/2023/01/chatgpt-ai-economy-automation-jobs/672767/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How ChatGPT Will Destabilize White-Collar Work&amp;rdquo; (The Atlantic)&lt;/a&gt;, where Annie Lowrey leads with &amp;ldquo;In the next five years, it is likely that AI will begin to reduce employment for college-educated workers.&amp;rdquo; I do not share these views. In fact, I am somewhat underwhelmed by the threat of ChatGPT for a number of reasons. Since this topic has come up a few times for me lately, I will write down my thoughts here so I can reference them more easily.&lt;/p&gt;
&lt;h2 id=&#34;chatgpt-cannot-think&#34;&gt;ChatGPT Cannot Think&lt;/h2&gt;
&lt;p&gt;The first issue I take with many of the AI hype articles is that despite what the news coverage may imply, ChatGPT cannot think. To be honest, when I see articles talking about ChatGPT as &amp;ldquo;intelligent&amp;rdquo; or &amp;ldquo;thinking,&amp;rdquo; the first thing that comes to mind is 
&lt;a href=&#34;http://smbc-comics.com/comic/2011-08-17&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this SMBC&lt;/a&gt; from 2011-08-17:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.smbc-comics.com/comics/20110817.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In my view, ChatGPT is a lot like this parrot - except that I do think it is fundamentally different, and ChatGPT is not &amp;ldquo;conscious&amp;rdquo; and does not &amp;ldquo;think&amp;rdquo; in a meaningful way. Despite the many advances made, artificial intelligence (AI) functions differently than a natural intelligence (NI), and in any ChatGPT is not designed to &amp;ldquo;think.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Before giving a big picture view of how a large language model like ChatGPT works, I want to illustrate the limited flexibility of AI with an example from image recognition. An NI can readily distinguish between what is in the foreground and background of an image. Think of an image like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Insect_on_blue_flower.jpg/500px-Insect_on_blue_flower.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;A human will have no problem distinguishing between the insect in the image, the flower it is on, and the foliage in the background as distinct objects in different planes. This holds true even if the individual is not familiar with the particular plant or insect in the image. If given additional images of either the insect on a different background or the same background without the insect, we would not mistake the the plant for the insect or the other way around.&lt;/p&gt;
&lt;p&gt;Now consider an AI model trained to recognize insects. The algorithm doesn&amp;rsquo;t have a concept of &amp;ldquo;insect&amp;rdquo; or &amp;ldquo;plant,&amp;rdquo; per se. Rather, it notices patterns in images that are labeled &amp;ldquo;insect&amp;rdquo; or labeled with a particular insect. The pattern it learns does not depend on it having a concept of &amp;ldquo;insect.&amp;rdquo; What that means in practice, is that our model might learn that the background is equally or even more important than the foreground. If we train our data set with bees on flowers, but not flowers without bees, we may end up with a model that declares flower photos &amp;ldquo;bees.&amp;rdquo; This phenomeon is known in image recognition, and people are actively working on methods around this problem. But it nicely illustrates how AI is not &amp;ldquo;smart,&amp;rdquo; and humans need to do a lot of heavy lifting to get the AI algorithm to perform as intended, even if the application domain is relatively limited. For more information on this application, see 
&lt;a href=&#34;https://gradientscience.org/background/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this article&lt;/a&gt; from GradientScience.&lt;/p&gt;
&lt;h2 id=&#34;how-does-it-work&#34;&gt;How does it Work?&lt;/h2&gt;
&lt;p&gt;With this background, let&amp;rsquo;s get an overview of how models like ChatGPT work. A good summary of the techniques involved is detailed in 
&lt;a href=&#34;https://www.assemblyai.com/blog/how-chatgpt-actually-works/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt; by AssemblyAI. In simple terms, a model is exposed to large amounts of data in order to learn about the structure of words and how the are aligned in sentences. In principle, this is not too different from the text prediction feature you have on your phone while texting. But this methodology only works to help produce coherent or seemingly coherent sentences by completion. Marked language modeling is a method use to help the model learn about syntax as well to improve the output.&lt;/p&gt;
&lt;p&gt;What is new with ChatGPT is that in addition labeled training material, it utilizes human feedback to improve its output. Deep down, AI models can be thought of as optimizing some (very complicated) function. This goal function need not necessarily be written down explicitly. OpenAI uses a method where a model gives two possible outputs for a prompt, and then a human judges which is &amp;ldquo;better,&amp;rdquo; somewhat similar to when an optometrist asks you if &amp;ldquo;1&amp;rdquo; or &amp;ldquo;2&amp;rdquo; is better. It then uses this feedback to improve its output iteratively. See 
&lt;a href=&#34;https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this blog post&lt;/a&gt; from OpenAI where they use this methodology to animate a backflip.&lt;/p&gt;
&lt;p&gt;ChatGPT uses 
&lt;a href=&#34;https://www.assemblyai.com/blog/how-chatgpt-actually-works/#reinforcement-learning-from-human-feedback&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;three steps&lt;/a&gt; for human feedback based reinforcement learning. You can already imagine some of the issues that can arise from using this method. For one, if human feedback is used to train the model, then we can expect the model to reflect the thoughts and opinions of the labelers to some degree. Labelers may be mistaken and might not be experts in whatever topic they are reviewing. They may be fundamentally mistaken or biased about what we would consider high school-level knowledge.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; This is on top of the issues of the large amount of source text used in the initial training phase. These source texts may vary wildly in style and accuracy. Even humans reviewing an article may not be able to distinguish facts from opinion, let alone a language model using many source texts as input. Which leads us to what I see as a main problem for ChatGPT.&lt;/p&gt;
&lt;h2 id=&#34;factual-inaccuracies&#34;&gt;Factual Inaccuracies&lt;/h2&gt;
&lt;p&gt;Despite the confidence exuded by ChatGPTs output, it will readily produce a number of factual inaccuracies or give bad advice when explaining how to do tasks. See for example Avram Piltch&amp;rsquo;s 
&lt;a href=&#34;https://www.tomshardware.com/news/chatgpt-told-me-break-my-cpu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;I Asked ChatGPT How to Build a PC. It Told Me to Break My CPU&amp;rdquo; (Tom&amp;rsquo;s Hardware)&lt;/a&gt;, where ChatGPT gives instructions for a computer assembly that is potentially damaging to the hardware.&lt;/p&gt;
&lt;p&gt;Or 
&lt;a href=&#34;https://toolguyd.com/ai-chatgpt-cordless-drill-recommendation-2023/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this article (ToolGuyd)&lt;/a&gt; where Stuart asked ChatGPT to recommend a cordless powerdrill. ChatGPT made three recommendations. In explaining its recommendations, it gave several tech specs about the recommended products. The only problem is that it got several of these items wrong. It made mistakes about what type of drill a particular model was, whether the battery is included in the particular SKU it listed or not, and how many BPM the model delivers. It also recommended a discontinued model.&lt;/p&gt;
&lt;p&gt;As a third example, consider 
&lt;a href=&#34;https://betonit.substack.com/p/chatgpt-takes-my-midterm-and-gets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt; where economics professor Bryan Caplan attempts to let ChatGPT take one his more recent midterms. It&amp;rsquo;s quite detailed and includes the questions, answers, and grading rubric Bryan used. He gave ChatGPT a D on this exam, substantially below the average grade human students in the class received.&lt;/p&gt;
&lt;p&gt;I would like to highlight that my argument isn&amp;rsquo;t that ChatGPT gets everything wrong - it doesn&amp;rsquo;t. It can even perform exceptionally well at certain tasks. See 
&lt;a href=&#34;https://mackinstitute.wharton.upenn.edu/wp-content/uploads/2023/01/Christian-Terwiesch-Chat-GTP.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this white paper&lt;/a&gt; by Christian Terwiesch grading ChatGPT&amp;rsquo;s attempt at the final exam Wharton Business School MBA core course for just one example. A little googling will quickly lead to other examples, such as it passing law school exams or giving decent answers to tech sector interview questions.&lt;/p&gt;
&lt;p&gt;My concern is that it sounds very confident in its answers, but it is not always trivial for the average person to verify whether or not ChatGPT&amp;rsquo;s output is trustworthy. As Rupert Goodwin 
&lt;a href=&#34;https://www.theregister.com/2022/12/12/chatgpt_has_mastered_the_confidence/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;put it&lt;/a&gt;, ChatGPT is &amp;ldquo;a Dunning-Kruger effect knowledge simulator par excellence.&amp;rdquo; And that&amp;rsquo;s a problem if people decide to just trust it to produce truth, when ChatGPT has no idea what &amp;ldquo;truth&amp;rdquo; is. It&amp;rsquo;s important to know that OpenAI is aware of this and it even says so on it&amp;rsquo;s 
&lt;a href=&#34;https://help.openai.com/en/articles/6783457-chatgpt-faq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FAQ page&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Can I trust that the AI is telling me the truth?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;a. ChatGPT is not connected to the internet, and it can occasionally produce incorrect answers. It has limited knowledge of world and events after 2021 and may also occasionally produce harmful instructions or biased content.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;d recommend checking whether responses from the model are accurate or not. If you find an answer is incorrect, please provide that feedback by using the &amp;ldquo;Thumbs Down&amp;rdquo; button.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In my opinion this is reasonable and to be expected. I think some people may get too excited and feel too confident in this technology when it just isn&amp;rsquo;t as reliable as many would wish at this stage. And for those reasons, I don&amp;rsquo;t think it&amp;rsquo;s coming for our jobs any time soon.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; If you use ChatGPT, be careful to not give it any sensitive information. OpenAI isn&amp;rsquo;t making this very expensive model available to you for free out of the goodness of their hearts. They&amp;rsquo;re using your interaction with it to 
&lt;a href=&#34;https://help.openai.com/en/articles/6783457-chatgpt-faq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;further train the model&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update 3/21:&lt;/em&gt; There is a 
&lt;a href=&#34;http://web.archive.org/web/20230318145629/https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;good article&lt;/a&gt; in the New Yorker regarding my point that ChatGPT doesn&amp;rsquo;t &amp;ldquo;think.&amp;rdquo; This is &lt;em&gt;contra&lt;/em&gt; Daniel Miessler&amp;rsquo;s 
&lt;a href=&#34;https://danielmiessler.com/blog/yes-gpts-llms-understand-argument/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;argument&lt;/a&gt; that ChatGPT and similar models exhibit &amp;ldquo;understanding.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update 4/4:&lt;/em&gt; And here a 
&lt;a href=&#34;https://fakenous.substack.com/p/how-much-should-you-freak-out-about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;good post&lt;/a&gt; by Michael Huemer on this issue.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;For a good review of the many ways in which typical adults are uninformed and mistaken about issues contra accepted expert opinion, see: B. Caplan, &lt;em&gt;The Myth of the Rational Voter: Why Democracies Choose Bad Policies&lt;/em&gt;, Princeton University Press, Princeton, NJ, 2007. And B. Caplan, &lt;em&gt;The Case against Education: Why the Education System Is a Waste of Time and Money&lt;/em&gt;, Princeton University Press, Princeton, NJ, 2019. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Dec22 SAS ODA Update - Impact on SASPy Users</title>
      <link>https://dmsenter89.github.io/post/23-01-sas-oda-update-saspy-impact/</link>
      <pubDate>Mon, 09 Jan 2023 15:20:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-01-sas-oda-update-saspy-impact/</guid>
      <description>&lt;p&gt;During December 2022, SAS ODA received substantial updates - see the 
&lt;a href=&#34;https://support.sas.com/ondemand/upgrade2022.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;upgrade page&lt;/a&gt; for details. It&amp;rsquo;s really nice to see that ODA is now using SAS 9.4M7. If you are a SASPy user, you may now bump into an error while logging in with your existing configuration. The specific error I encountered was &amp;ldquo;An exception was thrown during the encryption key exchange.&amp;rdquo; Nothing is wrong with your password, however. Due to changes with the AES encryption, SASPy will now need access to 3 encrpytion JARs in its classpath. See 
&lt;a href=&#34;https://sassoftware.github.io/saspy/configuration.html#attn-as-of-saspy-version-3-3-3-the-classpath-is-no-longer-required-in-your-configuration-file&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this note&lt;/a&gt; in the official SASPy docs. Download the required JAR files 
&lt;a href=&#34;https://support.sas.com/downloads/package.htm?pid=2494&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; (requires login) and add them to your SASPy package&amp;rsquo;s path here:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;path/to/python/site-packages/saspy/java/iomclient/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make sure your JAR files are set to executable and you&amp;rsquo;ll be good to go again.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Setting up a Virtual Lab Computer</title>
      <link>https://dmsenter89.github.io/post/23-01-virtual-lab/</link>
      <pubDate>Fri, 06 Jan 2023 19:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/23-01-virtual-lab/</guid>
      <description>&lt;p&gt;Dealing with computer resources in a modern lab can be tricky. Even if all participating researchers have laptops, a central location for storage or to host licensed software is desirable. While a physical computer can be setup for such a use, that is not always the most desirable solution. We want multiple people to have concurrent access to our resources while providing safe, sandboxed environments. Sometimes lab members want/need root access to learn certain tasks, but we don&amp;rsquo;t want them to accidentally take down our carefully configured systems. This leads us to the idea of containerization which can provide various failsafes. In this post, we will be setting up the 
&lt;a href=&#34;https://wiki.archlinux.org/title/LXD&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LXD (ArchWiki)&lt;/a&gt; environment as a virtual lab computer. This solution gives an entire working system, including systemd, similar to but more lightweight than VirtualBox.&lt;/p&gt;
&lt;p&gt;Setting up a successful virtual computer is very similar to setting up a regular Linux machine, with some minor LXD overhead. Note that only the individual administering the containers will need to deal with that LXD overhead. From the point of view of the end user, it&amp;rsquo;ll look the same as if they were interacting with a &amp;ldquo;regular&amp;rdquo; computer.  This post deals with the setup from the point of view of the admin. The lab members should be setup as users inside the container and can then SSH into the container or use VNC if a GUI is needed, similar to how they interact with a regular remote computer.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#install-and-setup-of-lxd&#34;&gt;Install and Setup of LXD&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#container-setup&#34;&gt;Container Setup&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#getting-the-image&#34;&gt;Getting the Image&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#basic-container-management&#34;&gt;Basic Container Management&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#setting-up-the-container&#34;&gt;Setting up the Container&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#networking&#34;&gt;Networking&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#giving-the-container-access-to-the-internet&#34;&gt;Giving the Container Access to the Internet&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#network-forwarding&#34;&gt;Network Forwarding&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#getting-a-gui-running&#34;&gt;Getting a GUI Running&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;

&lt;h2 id=&#34;install-and-setup-of-lxd&#34;&gt;Install and Setup of LXD&lt;/h2&gt;
&lt;p&gt;LXD can be installed from a snap with &lt;code&gt;sudo snap install lxd&lt;/code&gt;, but that requires you to have snap running. On Arch, LXD is available in the repos with &lt;code&gt;pacman -S lxd&lt;/code&gt;. To get a RPM install in Fedora, you&amp;rsquo;ll need to use an additional COPR repository like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;dnf copr enable ganto/lxc4
dnf install lxd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once installed, you&amp;rsquo;ll need to either enable the &lt;code&gt;lxd.socket&lt;/code&gt; or &lt;code&gt;lxd.service&lt;/code&gt; (if you want instances to be able to autostart). You&amp;rsquo;ll want to modify the subuid and subgid files so you can run unpriviliged containers (recommended), e.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# for root user and systemd:
usermod -v 1000000-1000999999 -w 1000000-1000999999 root 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this done, run &lt;code&gt;lxd init&lt;/code&gt; to go through a configuration guide for your new setup. If this is your first time using LXD, you will likely be fine just using the default settings, except maybe the size of the storage pool - but you can always attach other storage to your containers later, so if you will run tasks producing a lot of data you might want to consider just mounting a dedicated filesystem later. If you have multiple computers available in your lab, you might want to consider turning on 
&lt;a href=&#34;https://linuxcontainers.org/lxd/docs/latest/explanation/clustering/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;clustering (documentation)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For more details, see the 
&lt;a href=&#34;https://wiki.archlinux.org/title/LXD#Setup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ArchWiki&lt;/a&gt; or the 
&lt;a href=&#34;https://linuxcontainers.org/lxd/getting-started-cli/#initial-configuration&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Official Getting Started Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;container-setup&#34;&gt;Container Setup&lt;/h2&gt;
&lt;h3 id=&#34;getting-the-image&#34;&gt;Getting the Image&lt;/h3&gt;
&lt;p&gt;The first step in setting up a container is picking a suitable image to start from. Similar to Docker, many distributions are available to chose from. There are also arm and amd64 images available, so you can pick what works with your platform. To list available images on the image server, use the syntax &lt;code&gt;lxc image list images:&amp;lt;keyword&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# ArchLinux images:
lxc image list images:archlinux amd64 
# Fedora images, using key/value pairs:
lxc image list images:fedora arch=amd64 type=container
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a new image without starting it, use &lt;code&gt;lxc init &amp;lt;image&amp;gt; &amp;lt;container-name&amp;gt;&lt;/code&gt;. To both initialize and start a new container, use &lt;code&gt;lxc init &amp;lt;image&amp;gt; &amp;lt;container-name&amp;gt;&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# create a base image called myarch without starting:
lxc init images:archlinux myarch
# you can also specify version and arch, e.g. Fedora 36 / 64bit:
lxc init images:fedora/36/amd64 myfedora
# create and launch an image:
lxc init images:rockylinux/9 myrocky
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that you can have a large number of concurrent containers in use, which may but need not share the same base image. This can be useful for larger teams, where you can setup systems for particular tasks or projects. For example, you could have a main machine for your graduate students, a separate one for people moving in and out of the lab like REU students, and a third container for a class that you&amp;rsquo;re teaching to use.&lt;/p&gt;
&lt;h3 id=&#34;basic-container-management&#34;&gt;Basic Container Management&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Starting, stopping etc. is intuitive
lxc start   &amp;lt;container&amp;gt; # starts container
lxc stop    &amp;lt;container&amp;gt; [--force] # stops the container
lxc restart &amp;lt;container&amp;gt; [--force] # restart
lxc pause   &amp;lt;container&amp;gt; # send SIGSTOP to all container processes

# what containers do I have?
lxc list 
lxc info myarch # get detailed info about this container
lxc copy &amp;lt;name1&amp;gt; &amp;lt;name2&amp;gt; # make a copy of an existing container
lxc delete &amp;lt;container&amp;gt; [--force]

# edit container configuration
lxc config edit &amp;lt;container&amp;gt; # launches config in VISUAL editor
lxc config set &amp;lt;container&amp;gt; &amp;lt;key&amp;gt; &amp;lt;value&amp;gt;  # change a single config item
lxc config device add &amp;lt;container&amp;gt; &amp;lt;dev&amp;gt; &amp;lt;type&amp;gt; &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;
lxc config show [--expanded] &amp;lt;container&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to these commmands, you can also snapshot your containers. This creates a restorable copy of your container in case something bad happens - like someone typing &lt;code&gt;rm -rf *&lt;/code&gt; into the wrong root shell. By default, snapshots are named in a numbered pattern snapX where X is an integer.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;lxc snapshot &amp;lt;container&amp;gt; &amp;lt;snap&amp;gt; # create new snapshot
lxc restore  &amp;lt;container&amp;gt; &amp;lt;snap&amp;gt; # restore container to snapshot
lxc copy &amp;lt;container&amp;gt;/&amp;lt;snap&amp;gt; &amp;lt;new-container&amp;gt; # new container from snapshot
lxc delete &amp;lt;container&amp;gt;/&amp;lt;snap&amp;gt;   # delete the snapshot
lxc info &amp;lt;container&amp;gt;            # lists available snapshots, plus other info
lxc move &amp;lt;container&amp;gt;/&amp;lt;snap&amp;gt; &amp;lt;container&amp;gt;/&amp;lt;new-snap&amp;gt; # rename snapshot
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;setting-up-the-container&#34;&gt;Setting up the Container&lt;/h3&gt;
&lt;p&gt;You can enter your container immediately with a root shell with &lt;code&gt;lxc shell &amp;lt;container&amp;gt;&lt;/code&gt; and proceed with your regular setup, such as updating and installing packages, setting up new users, etc. To make this process more repeatedly, you can also just move a setup script from the host to the container first, and then execute that script inside the container. That way you can have a record of what you did when you first set up the container.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# drop into root shell
lxc shell &amp;lt;container&amp;gt;
# execute arbitrary command in container
lxc exec &amp;lt;container&amp;gt; -- &amp;lt;program&amp;gt; [&amp;lt;options&amp;gt;]

# move a file from host to container
lxc file push /host/file &amp;lt;container&amp;gt;/path/on/container 
# move a file from container to host
lxc file pull &amp;lt;container&amp;gt;/path/to/file /path/on/host
# edit a file inside container
lxc file edit &amp;lt;container&amp;gt;/etc/passwd 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See my earlier blog post for a list of 
&lt;a href=&#34;https://dmsenter89.github.io/post/22-12-some-cli-tools/&#34;&gt;some CLI tools&lt;/a&gt; I like to install on new systems.&lt;/p&gt;
&lt;h2 id=&#34;networking&#34;&gt;Networking&lt;/h2&gt;
&lt;h3 id=&#34;giving-the-container-access-to-the-internet&#34;&gt;Giving the Container Access to the Internet&lt;/h3&gt;
&lt;p&gt;The first step in container networking is to make sure your container can access the network. This may require your firewall to let traffic through on the default bridge. On an ArchLinux host, use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ufw route allow in on lxdbr0
ufw allow in on lxdbr0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;while on a Fedora host you might use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;firewall-cmd --zone=trusted --change-interface=lxdbr0 --permanent
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;network-forwarding&#34;&gt;Network Forwarding&lt;/h3&gt;
&lt;p&gt;At this point your container is available from the host on wich the LXD service is running. But the whole point of the exercise is to make the container accessible from the lab members&#39; various devices. I&amp;rsquo;ll present two options here for setting this up, depending on whether you need access from outside of your local network or not. Either way, make sure SSH is 
&lt;a href=&#34;https://wiki.archlinux.org/title/OpenSSH&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;set up&lt;/a&gt; inside your container and you can SSH into the container from the host shell. Both methods rely on using the network forward feature built into LXD. See the 
&lt;a href=&#34;https://linuxcontainers.org/lxd/docs/master/howto/network_forwards/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;For network forwarding to work we need to know two things about our container: what device our container is using to connect to the internet; on a default setup, this will be lxdbr0 but check with &lt;code&gt;lxc network list&lt;/code&gt; to be sure. The second item we need is the IP address of our container, which can be displayed with &lt;code&gt;lxc list &amp;lt;container&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The next item we need is an IP address to forward from. We can either get an IP address dedicated to the container, or hijack some ports from our host for re-routing.&lt;/p&gt;
&lt;p&gt;To add a second IP to your existing network device, use the &lt;code&gt;ip a&lt;/code&gt; command to find the device name (on your host) of the network device connected to your network. If you use wifi, this might be something like wlp4s0 or similar. Then pick an IP not otherwise assigned by the router and assign it to this device - in addition to the existing IP - using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ip -4 a add dev &amp;lt;device-name&amp;gt; &amp;lt;free-ip&amp;gt;/24
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this will only persist until the host reboots. You can then create a network forward on the container&amp;rsquo;s device (e.g., lxdbr0) with the newly assigned IP as the listening address. Using this command will let the container handle all incoming traffic to the new IP:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;lxc network forward create lxdbr0 &amp;lt;listening_address&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then edit the target address with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;lxc network forward edit lxdbr0 &amp;lt;listening_address&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and specify the container&amp;rsquo;s IP as the &lt;code&gt;target_address&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The alternative method is to use the host&amp;rsquo;s IP as the listening address and then just forward particular ports to the container, e.g. port 22 for SSH or 590x for VNC servers. This way you skip creating the second IP above, and just start by creating and editing a network forward with the the host IP as listening address. The edit can then list the ports you want forwarded. Here&amp;rsquo;s an example of a valid file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;description: Sample Forward
config: {}
ports:
- description: ssh
  protocol: tcp
  listen_port: &amp;quot;10022&amp;quot;  # any unused host port
  target_port: &amp;quot;22&amp;quot;    
  target_address: &amp;lt;container-ip&amp;gt;
- description: VNC servers
  protocol: tcp
  listen_port:  105901-105904 # any unused host port
  target_port:  5901-5904   
  target_address: &amp;lt;container-ip&amp;gt;
listen_address: &amp;lt;host-ip&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Aside from these forwards, you may consider setting up a 
&lt;a href=&#34;https://wiki.archlinux.org/title/Postfix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;postfix&lt;/a&gt; server and associated forward so you can use the mail command to programmatically send emails to users. One great use case for this is the sending of log files after completion of long running jobs. This keeps your users from needing to manually log in and check the status of their jobs. If you have used HPC services at your campus, you may have experienced the utility of this first hand.&lt;/p&gt;
&lt;p&gt;Network forwarding options are explained in more detail in the 
&lt;a href=&#34;https://linuxcontainers.org/lxd/docs/master/howto/network_forwards/#&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;, which also contains a link to a short YouTube video demonstring these commands in a shell session.&lt;/p&gt;
&lt;h2 id=&#34;getting-a-gui-running&#34;&gt;Getting a GUI Running&lt;/h2&gt;
&lt;p&gt;First, think about whether the tools you use require a GUI. A lot of research work can be done entirely within the command line or by using servers with particular software. So instead of installing a regular RStudio instance, you could install RStudio Server. Jupyter is already designed around the client/server model, as are RDBMS systems. If your team doesn&amp;rsquo;t feel comfortable with ViM and prefers VS Code, use the 
&lt;a href=&#34;https://code.visualstudio.com/docs/remote/ssh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;remote extension&lt;/a&gt; to use a VS Code server that can be opened up from your teams&#39; local computers using SSH.&lt;/p&gt;
&lt;p&gt;If you only need a GUI to use one GUI app at a time, say Mathematica/Matlab, then the simplest option will be to use X-forwarding via SSH. Make sure that &lt;code&gt;X11Forwarding yes&lt;/code&gt; is set in your sshd_config file and restart the sshd service to turn it on. You&amp;rsquo;ll also need to install &lt;code&gt;xorg-xauth&lt;/code&gt; on an ArchLinux container. From then on, connecting via  SSH with the &lt;code&gt;-X&lt;/code&gt; flag should work as desired.&lt;/p&gt;
&lt;p&gt;If you need an entire desktop environment available, you can set up 
&lt;a href=&#34;https://wiki.archlinux.org/title/TigerVNC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VNC&lt;/a&gt; or 
&lt;a href=&#34;https://wiki.archlinux.org/title/NoMachine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NoMachine&lt;/a&gt; the same way you would for a regular system. I have seen a lot of comments arguing for NoMachine being more performant, but the default TigerVNC on Arch/Fedora has worked sufficiently well for most of my needs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Missing Data Mechanisms</title>
      <link>https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/</link>
      <pubDate>Tue, 03 Jan 2023 10:00:00 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/</guid>
      <description>&lt;p&gt;Understanding whether a variable&amp;rsquo;s missingness from a dataset is related to the underlying value of the data is a key concept in the field of missing data analysis. We distinguish three broad categories: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). In his book &lt;em&gt;Statistical Rethinking&lt;/em&gt;, McElreath&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; gives an amusing example to illustrate this concept: he considers variants of a dog eating homework and how the dog chooses - if at all - to eat the homework. The examples he give show substantial shifts in observed values, which make for a good illustration of the types of problems you might encounter. A lecture corresponding to the  example from the book can be found on 
&lt;a href=&#34;https://www.youtube.com/watch?v=oMiSb8GKR0o&amp;amp;list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN&amp;amp;index=18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube&lt;/a&gt;. In this post, I will first briefly review the different missing data mechanisms before implementing McElreath&amp;rsquo;s examples in SAS.&lt;/p&gt;
&lt;h3 id=&#34;overview-of-missing-data-mechanisms&#34;&gt;Overview of Missing Data Mechanisms&lt;/h3&gt;
&lt;p&gt;My presentation here follows van Buuren&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Let $Y$ be a $n \times p$ matrix representing a sample of $p$ variables for $n$ units of the sample and $R$ be a corresponding $n \times p$ indicator matrix, so that&lt;/p&gt;
&lt;p&gt;$$r_{i,j} = \begin{cases} 1 &amp;amp; y_{i,j} \text{ is observed} \\ 0 &amp;amp; y_{i,j} \text{ not observed.}\end{cases} $$&lt;/p&gt;
&lt;p&gt;We denote the observed data by $Y_\text{obs}$ and the missing data that $Y_\text{miss}$ so that $Y=(Y_\text{obs},Y_\text{miss})$.&lt;/p&gt;
&lt;p&gt;We distinguish three main categories for how the distribution of $R$ may depend on $Y$. This relationship is described as the &lt;em&gt;missing data model&lt;/em&gt;. Let $\psi$ contain the parameters of this model. The general expression of the missing data model is $\mathrm{Pr}(R|Y_\text{obs}, Y_\text{miss}, \psi)$, where $\psi$ consists of the parameters of the missing data model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Missing Completely at Random (MCAR).&lt;/strong&gt; This implies that the cause of the missing data is unrelated to the data itself. In this case,&lt;/p&gt;
&lt;p&gt;$$ \mathrm{Pr}(R=0| Y_\text{obs}, Y_\text{miss}, \psi) = \mathrm{Pr}(R=0|\psi).$$&lt;/p&gt;
&lt;p&gt;This is the ideal case, but unfortunately rare in practice. Many researchers implicitly assume this when using methods such as list-wise deletion, otherwise known as complete case analysis, which can produce unbiased estimates of sample means if the data are MCAR, although the reported standard error will be too large.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Missing at Random (MAR).&lt;/strong&gt; Missingness is the same within groups defined by the observed data, so that&lt;/p&gt;
&lt;p&gt;$$ \mathrm{Pr}(R=0| Y_\text{obs}, Y_\text{miss}, \psi) = \mathrm{Pr}(R=0|Y_\text{obs},\psi).$$&lt;/p&gt;
&lt;p&gt;This is a often a more reasonable assumption in practice and the starting point for modern missing data methods.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Missing not at Random (MNAR).&lt;/strong&gt; If neither the MCAR or MAR assumptions hold, then we may find that missingness depends on the missing data itself, in which case there is no simplification and
$$ \mathrm{Pr}(R=0| Y_\text{obs}, Y_\text{miss}, \psi) = \mathrm{Pr}(R=0| Y_\text{obs}, Y_\text{miss}, \psi).$$&lt;/p&gt;
&lt;p&gt;As you can imagine, this is the most tricky case to deal with.&lt;/p&gt;
&lt;h3 id=&#34;dogs-eating-homework&#34;&gt;Dogs Eating Homework&lt;/h3&gt;
&lt;p&gt;Consider dogs (D) eating students&#39; homework. Each student&amp;rsquo;s homework score (H) is graded on a 10-point scale and each student&amp;rsquo;s score varies in proportion to how much they study (S). We assume the amount of time they study is normally distributed. A binomial is used to generate homework scores from the normed time spent studying. McElreath uses the following code to simulate the full data set:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;N &amp;lt;- 100
S &amp;lt;- rnorm( N )
H &amp;lt;- rbinom( N, size=10, inv_logit(S) )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;inv_logit(x) = exp(x)+(1+exp(x))&lt;/code&gt;, the definition used by the &lt;code&gt;LOGISTIC&lt;/code&gt; function in SAS. With a data step, this can be represented in SAS as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data full;
    DO i=1 to 100;
        S=RAND(&#39;NORM&#39;);
        H=RAND(&#39;BINO&#39;, LOGISTIC(S), 10);
        output;
    END;
    label S=&#39;Amount of Studying&#39; H=&#39;Homework Score&#39;;
    drop i;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can get closer in form to the R code by using PROC IML, but that&amp;rsquo;s a story for a different post.&lt;/p&gt;
&lt;p&gt;Say we are interested in estimating the relationship between $S$ and $H$. In our example, we assume that $H$ is not directly observable. Instead, $H^*$ is observed - a subset of the full data set $H$ with some homework values missing. We can now look at &lt;em&gt;why&lt;/em&gt; some of those values are missing. Specifically, in McElreath&amp;rsquo;s example each student has a dog $D$ and sometimes the dog eats the homework. But here we can again ask, why is the dog eating the homework? McElreath uses  
&lt;a href=&#34;https://en.wikipedia.org/wiki/Directed_acyclic_graph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;directed acyclic graphs&lt;/a&gt; (DAGs) to represent different missing data models, reproduced below. As we will see, these are some intuitive examples for our three missing data mechanism categories.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-directed-acyclic-graphs-corresponding-to-mcelreaths-examples-of-missing-data-models-s-represents-the-amount-of-time-spent-studying-which-in-turn-influences-the-homework-score-h-which-is-only-partially-observed-indicated-by-the-circle-alas-dogs-d-eat-some-of-the-homework-the-actually-observed-scores---those-not-eaten---are-indicated-by-h-adapted-from-figure-154-in-_statistical-rehinking_&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/dag_hu9d4aa2d4b53fa51c30ec3b81dc4e20f1_14343_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;The directed acyclic graphs corresponding to McElreath&amp;amp;rsquo;s examples of missing data models. $S$ represents the amount of time spent studying, which in turn influences the homework score $H$, which is only partially observed (indicated by the circle). Alas, dogs $D$ eat some of the homework. The actually observed scores - those not eaten - are indicated by $H^*$. Adapted from figure 15.4 in &amp;lt;em&amp;gt;Statistical Rehinking&amp;lt;/em&amp;gt;.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/dag_hu9d4aa2d4b53fa51c30ec3b81dc4e20f1_14343_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;665&#34; height=&#34;179&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    The directed acyclic graphs corresponding to McElreath&amp;rsquo;s examples of missing data models. $S$ represents the amount of time spent studying, which in turn influences the homework score $H$, which is only partially observed (indicated by the circle). Alas, dogs $D$ eat some of the homework. The actually observed scores - those not eaten - are indicated by $H^*$. Adapted from figure 15.4 in &lt;em&gt;Statistical Rehinking&lt;/em&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;missing-completely-at-random-mcar&#34;&gt;Missing Completely At Random (MCAR)&lt;/h4&gt;
&lt;p&gt;In the first example, the dogs eat homework completely at random. This is the most basic and benign case, and corresponds to DAG 1) in Figure 1. McElreath&amp;rsquo;s R code is given by&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;D &amp;lt;- rbern( N ) 
Hm &amp;lt;- H  # H*, but * is not a valid char for varnames in R
Hm[D==1] &amp;lt;- NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can implement this in SAS by using the &lt;code&gt;RAND&lt;/code&gt; function with the Bernoulli argument in an if/else clause:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;if RAND(&#39;BERN&#39;, 0.5) then Hm = .;
    else Hm = H;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This causes about half of our data to be hidden, but not in a biased way.&lt;/p&gt;
&lt;h4 id=&#34;missing-at-random-mar&#34;&gt;Missing at Random (MAR)&lt;/h4&gt;
&lt;p&gt;In the second example, we assume the amount of time a student spends studying decreases the amount of time they have to play with and exercise their dog. This, in turn, influences whether the homework gets eaten. Or, as McElreath puts it, the &amp;ldquo;dog eats conditional on the cause of homework.&amp;rdquo; In his particular example, the homework is eaten whenever a student spends more time studying than the average $S=0$. This corresponds to DAG 2) in Figure 1.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;D &amp;lt;- ifelse( S&amp;gt;0 , 1 , 0 )
Hm &amp;lt;- H
Hm[D==1] &amp;lt;-NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In SAS:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;if S&amp;gt;0 then Hm = .;
    else Hm = H;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;missing-not-at-random-mnar&#34;&gt;Missing not at Random (MNAR)&lt;/h4&gt;
&lt;p&gt;In this case, we have some correspondence between the missing variable&amp;rsquo;s value and whether or not it is missing from the data set. Here, the &amp;ldquo;dog eats conditional on the homework itself.&amp;rdquo; Suppose that dogs prefer to eat bad homework. In such a case, the value of $H$ is directly related to whether or not $H$ is observed in the particular unit or not. His example R code is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;# dogs prefer bad homework
D &amp;lt;- ifelse( H&amp;lt;5 , 1 , 0 )
Hm &amp;lt;- H
Hm[D==1] &amp;lt;- NA
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And in SAS:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;if H&amp;lt;5 then Hm = .;
    else Hm = H;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-full-sas-code&#34;&gt;The Full SAS Code&lt;/h3&gt;
&lt;p&gt;We can now build a SAS data set that contains a full copy of the original data set, together with our various examples of missing data mechanisms. I have added a seed to the data step for reproducibility.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data full;
    CALL streaminit( 451 ); 
    
    LABEL
        Type = &#39;Missing Data Mechanism&#39;
        S = &#39;Amount of Studying&#39;
        H = &#39;Homework Score&#39;
        Hm = &#39;Observed Homework Score&#39;
    ;

    DO i=1 to 100;
        TYPE = &#39;FULL&#39;;
        S = RAND(&#39;NORM&#39;);
        H = RAND(&#39;BINO&#39;, LOGISTIC(S), 10);
        Hm = H;
        output;
        
        /* Example 1) MCAR */
        TYPE = &#39;MCAR&#39;;
        if RAND(&#39;BERN&#39;, 0.5) then Hm = .;
            else Hm = H;
        output;
        
        /* Example 2) MAR */
        TYPE = &#39;MAR&#39;;
        if S&amp;gt;0 then Hm = .;
            else Hm = H;
        output;
        
        /* Example 3) MNAR */
        TYPE = &#39;MNAR&#39;;
        if H&amp;lt;5 then Hm = .;
            else Hm = H;
        output;
    
    END;
    
    drop i;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may want to run a &lt;code&gt;PROC SORT&lt;/code&gt; or &lt;code&gt;PROC SQL&lt;/code&gt; afterwards to group the different categories together, as they will be alternating in this data set.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;





  
  











&lt;figure id=&#34;figure-boxplot-of-our-example-data-note-that-the-mcar-data-looks-very-similar-to-the-original-data-set-unlike-the-mar-and-mnar-versions&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/mdm_boxplot_hu7d089ca360b9b11aaf1a482082b6d8dc_11177_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Boxplot of our example data. Note that the MCAR data looks very similar to the original data set, unlike the MAR and MNAR versions.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/mdm_boxplot_hu7d089ca360b9b11aaf1a482082b6d8dc_11177_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Boxplot of our example data. Note that the MCAR data looks very similar to the original data set, unlike the MAR and MNAR versions.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We can see that MCAR leads to minimal bias in our example data, while both the MAR and MNAR variations lead to substantial differences in observed vs actual homework scores for our synthetic population. For a more subtle example, see section 2.2.4 in van Buuren,&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; available 
&lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-idconcepts.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;online here&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;R. McElreath, &lt;em&gt;Statistical Rethinking&lt;/em&gt;, 2nd ed, Chapman and Hall/CRC, Boca Raton, FL, 2020. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;S. van Buuren, &lt;em&gt;Flexible Imputation of Missing Data&lt;/em&gt;, 2nd ed, Chapman and Hall/CRC, Boca Raton, FL, 2019. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Some CLI Tools</title>
      <link>https://dmsenter89.github.io/post/22-12-some-cli-tools/</link>
      <pubDate>Wed, 07 Dec 2022 15:38:16 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/22-12-some-cli-tools/</guid>
      <description>&lt;p&gt;There are certain CLI tools that I find myself installing whenever I set up a new system. I&amp;rsquo;m not talking about the general system setup, like installing vim or Python, but some drop-in replacements for older Linux tools and some cli solutions that I use quite regularly. I thought I would collect them here for convenience. The headers are sorted alphabetically, except that &amp;ldquo;Other&amp;rdquo; is last because that seems most sensible.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#better-drop-ins&#34;&gt;&amp;ldquo;Better&amp;rdquo; Drop-Ins&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-and-file-wrangling&#34;&gt;Data and File Wrangling&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#development&#34;&gt;Development&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#resource-management&#34;&gt;Resource Management&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#other&#34;&gt;Other&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;

&lt;h2 id=&#34;better-drop-ins&#34;&gt;&amp;ldquo;Better&amp;rdquo; Drop-Ins&lt;/h2&gt;
&lt;p&gt;The following tools I use as &amp;ldquo;better&amp;rdquo; drop-ins for other commands. Instead of &lt;code&gt;ls&lt;/code&gt;, I typically now use 
&lt;a href=&#34;https://github.com/ogham/exa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;exa&lt;/a&gt;. Instead of grep, I tend to use 
&lt;a href=&#34;https://github.com/BurntSushi/ripgrep&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ripgrep&lt;/a&gt;. Instead of &lt;code&gt;find&lt;/code&gt;, I tend to use 
&lt;a href=&#34;https://github.com/sharkdp/fd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fd&lt;/a&gt;. For quick viewing of text-files with syntax highlighting, I like 
&lt;a href=&#34;https://github.com/sharkdp/bat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bat&lt;/a&gt; over &lt;code&gt;cat&lt;/code&gt;. As a git/diff pager with syntax highlighting I use 
&lt;a href=&#34;https://github.com/dandavison/delta&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;delta&lt;/a&gt;. All of these tools are written in Rust. The old &lt;code&gt;df&lt;/code&gt; command can be improved with 
&lt;a href=&#34;https://github.com/muesli/duf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;duf&lt;/a&gt;, a Go implementation with output that&amp;rsquo;s nicer to read (and alternatively, outputs as JSON).&lt;/p&gt;
&lt;h2 id=&#34;data-and-file-wrangling&#34;&gt;Data and File Wrangling&lt;/h2&gt;
&lt;h4 id=&#34;calculator&#34;&gt;Calculator&lt;/h4&gt;
&lt;p&gt;A great CLI calculator implemented in C++ is 
&lt;a href=&#34;https://qalculate.github.io/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Qalculate!&lt;/a&gt;. It is mainly intended to be run with a Qt or GTK GUI, but does include a CLI version that can be invoked with &lt;code&gt;qalc&lt;/code&gt;. A Rusty alternative is 
&lt;a href=&#34;https://github.com/tiffany352/rink-rs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rink&lt;/a&gt;. A benefit of rink is that if you are trying to convert incompatible units, it will make a suggestion for a transformation that makes each side compatible, which can help with dimensional analysis. A good Rusty calculator app without units but wide support of operations, including functions, is 
&lt;a href=&#34;https://github.com/PaddiM8/kalker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kalker&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;csv-and-json&#34;&gt;CSV and JSON&lt;/h4&gt;
&lt;p&gt;CSV files are ubiquitous, and being able to manipulate them and get an overview of what is contained without needing to actually load them in Excel/Python/SAS/etc is very useful. I used to really like 
&lt;a href=&#34;https://csvkit.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;csvkit&lt;/a&gt; for that. The main drawback here is speed for large CSV files, due to it being implemented in Python. A must faster program written in Rust is 
&lt;a href=&#34;https://github.com/jqnatividad/qsv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;qsv&lt;/a&gt;, successor to BurntSushi&amp;rsquo;s 
&lt;a href=&#34;https://github.com/BurntSushi/xsv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;xsv&lt;/a&gt;. It has more features than csvkit, is faster, and seems more flexible.&lt;/p&gt;
&lt;p&gt;Another cool Python program for interacting with CSV files is 
&lt;a href=&#34;https://www.visidata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;visidata&lt;/a&gt;. It is a CSV viewer that doubles as a spreadsheet program, allows you to make plots and statistics and just do a ton of different things with your file in-memory.&lt;/p&gt;
&lt;p&gt;The C program 
&lt;a href=&#34;https://github.com/stedolan/jq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jq&lt;/a&gt; aims to be the &lt;code&gt;sed&lt;/code&gt; of working with JSON files.&lt;/p&gt;
&lt;h4 id=&#34;file-managers&#34;&gt;File Managers&lt;/h4&gt;
&lt;p&gt;There are &lt;em&gt;a lot&lt;/em&gt; of file managers to choose from these days. There are lots of popular options like mc, ranger, nnn, but I tend to keep falling back on 
&lt;a href=&#34;https://github.com/vifm/vifm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vifm&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;development&#34;&gt;Development&lt;/h2&gt;
&lt;p&gt;If you need to benchmark something, try Rusty old 
&lt;a href=&#34;https://github.com/sharkdp/hyperfine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hyperfine&lt;/a&gt;. There is an interesting make alternative called 
&lt;a href=&#34;https://github.com/casey/just&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;just&lt;/a&gt;, which looks promising but I haven&amp;rsquo;t played with it yet.&lt;/p&gt;
&lt;h2 id=&#34;resource-management&#34;&gt;Resource Management&lt;/h2&gt;
&lt;h4 id=&#34;disk-space&#34;&gt;Disk Space&lt;/h4&gt;
&lt;p&gt;There are several excellent tools here. One that can be found in most repos I&amp;rsquo;ve encountered is 
&lt;a href=&#34;https://dev.yorhel.nl/ncdu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ncdu&lt;/a&gt;, a disk usage analyzer with an ncurses interface written in C. It is reasonably fast and let&amp;rsquo;s you interactively delete folders while you&amp;rsquo;re at it. A parallel implementation of the same idea but written in Go can be found with 
&lt;a href=&#34;https://github.com/dundee/gdu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gdu&lt;/a&gt;. For non-interactive use, 
&lt;a href=&#34;https://github.com/bootandy/dust&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dust&lt;/a&gt; (du + rust) is available.&lt;/p&gt;
&lt;h4 id=&#34;system-status---general&#34;&gt;System Status - General&lt;/h4&gt;
&lt;p&gt;One of the first tools I usually install is 
&lt;a href=&#34;https://github.com/htop-dev/htop&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;htop&lt;/a&gt;. It is a widely available and fast process viewer written in C. You can use it to kill or renice a process interactively without needing to find its PID. An alternative to this is 
&lt;a href=&#34;https://github.com/nicolargo/glances&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;glances&lt;/a&gt;, &amp;ldquo;an eye on your system&amp;rdquo; written in Python. It has a lot more information, including disk usage, sensor temperatures, battery information (on laptops), etc. and can be extended with plugins. It can be used interactively on the CLI, but it also gives the option of running in client/server mode which is nifty.&lt;/p&gt;
&lt;p&gt;Similar to glances, 
&lt;a href=&#34;https://github.com/ClementTsang/bottom&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bottom&lt;/a&gt; is a Rust program giving general system information including plots, but it does not have quite the same range of information to it as glances does.&lt;/p&gt;
&lt;h4 id=&#34;system-status---networking&#34;&gt;System Status - Networking&lt;/h4&gt;
&lt;p&gt;To see what is clogging up your internet pipes, try 
&lt;a href=&#34;https://github.com/raboof/nethogs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nethogs&lt;/a&gt; written in C++. A nice rusty alternative is 
&lt;a href=&#34;https://github.com/imsnif/bandwhich&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bandwhich&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;other&#34;&gt;Other&lt;/h2&gt;
&lt;p&gt;Trying to figure out when it&amp;rsquo;s a good time to speak with a colleague in a different time zone? Install the Python package 
&lt;a href=&#34;https://gitlab.com/anarcat/undertime&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;undertime&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t want to remeber different package manager&amp;rsquo;s syntax? Install 
&lt;a href=&#34;https://github.com/icy/pacapt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pacapt&lt;/a&gt; and use ArchLinux&#39; pacman syntax on your system instead.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CSV2DS</title>
      <link>https://dmsenter89.github.io/post/22-11-csv2ds/</link>
      <pubDate>Wed, 23 Nov 2022 16:43:28 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/22-11-csv2ds/</guid>
      <description>&lt;p&gt;Creating a minimum working example (MWE) is a relatively frequent task. It is no problem to share an MWE for a feature in SAS because a large number of example data sets are shipped and installed by default. But sometimes you need an MWE because you are having trouble accomplishing a particular task with particular input data. At that point, you will need to share the data or a subset thereof together with your code. In SAS Forums, the preferred way to do this is with a datastep using a datalines/cards statement. Writing these by hand can be tedious since the data source is not typically a datalines statement to begin with. I have previously seen a SAS macro that can be used to generate a datalines statement from a SAS data set, but can&amp;rsquo;t seem to locate it at the moment. The data source I personally encounter the most often in my work is either in CSV or Excel formats. Since the latter can easily be exported to CSV, I decided to write a program that generates a SAS data step given a CSV file.&lt;/p&gt;
&lt;p&gt;For the implementation language I chose to use 
&lt;a href=&#34;https://go.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Go&lt;/a&gt;. I started learning about Go back in May when I implemented a 
&lt;a href=&#34;https://dmsenter89.github.io/post/22-05-go-wordle/&#34;&gt;simple CLI version of Wordle&lt;/a&gt;. Since then I have increasingly used Go to write various small tools at work. It has been a very enjoyable language to write in and distribution via GitHub is easy. If you have the Go toolchain installed, you can get the latest copy of csv2ds using&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;go install github.com/dmsenter89/csv2ds@latest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tool is very simple to use. Give it a CSV file or list of CSV files and it will generate a data step for each file using the CSV&amp;rsquo;s base name as the data set name. To ensure compatibility, variable names and the data set name are processed to be compatible with SAS&#39; naming scheme. The tool will attempt to guess if a particular column is numeric or not. If a column is determined to not be numeric, the longest cell will be used to set that variable&amp;rsquo;s length via a length statement to prevent truncation.&lt;/p&gt;
&lt;p&gt;I often work with the 
&lt;a href=&#34;https://csvkit.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;csvkit&lt;/a&gt; suite of command-line tools. It&amp;rsquo;s a wonderful collection of Python programs that can import data into CSV, generate basic column statistics, and use grep and SQL to extract data from a CSV file, amongst other things. This collection is designed to allow you to pipe the output from one as input to the next. Consider 
&lt;a href=&#34;https://csvkit.readthedocs.io/en/latest/tutorial/2_examining_the_data.html#csvsort-order-matters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this example&lt;/a&gt;. Csvcut is used to extract only certain columns from the file data.csv. Then csvgrep is used to subset to use only the data pertaining to one particular county. Then the data is sorted by the total_cost variable and displayed. I wanted my tool to be compatible with this suite, so if &lt;code&gt;-&lt;/code&gt; is passed as the filename, csv2ds will read the contents of STDIN instead. Changing the above csvkit example by replacing csvlook with my tool will generate the corresponding SAS data set:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;csvcut -c county,item_name,total_cost data.csv | csvgrep -c county -m LANCASTER | csvsort -c total_cost -r | csv2ds -
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point csv2ds is quite simple, but sufficient for my needs. Some minor intervention may be needed to make the data step template work for your data. Informats like DOLLAR are not recognized as numeric and minor edits would need to be made to the produced template.&lt;/p&gt;
&lt;p&gt;Checkout my new tool over on 
&lt;a href=&#34;https://github.com/dmsenter89/csv2ds&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Setup an Arbitrary WSL2 Distro</title>
      <link>https://dmsenter89.github.io/post/22-11-setup-an-arbitrary-wsl2-distro/</link>
      <pubDate>Mon, 14 Nov 2022 22:34:25 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/22-11-setup-an-arbitrary-wsl2-distro/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Windows Subsystem for Linux&lt;/a&gt; (WSL) is an important part of my daily work flow. Unfortunately, the main distro supplied by Windows
is Ubuntu, which - for a variety of reasons - is not exactly my favorite distro. Luckily, WSL2 allows you to import an arbitrary Linux distro
to use instead. I got the idea from 
&lt;a href=&#34;https://dev.to/bowmanjd/install-fedora-on-windows-subsystem-for-linux-wsl-4b26&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an article (Dev.to)&lt;/a&gt;
by Jonathan Bowman explaining how to get Fedora up and running in WSL2. This article summarizes the
key points of Bowman&amp;rsquo;s post and includes information for my long time daily driver, 
&lt;a href=&#34;https://archlinux.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arch Linux&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The short of it is that you can import a root filesystem tarball into WSL2 from Windows terminal
using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;wsl --import &amp;lt;distro-name&amp;gt; &amp;lt;distro-target-location&amp;gt; &amp;lt;path-to-tarball&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once imported, you can launch into your distro using &lt;code&gt;wsl -d &amp;lt;distro-name&amp;gt;&lt;/code&gt;. The only question is
how to get the root filesystem for this import step.&lt;/p&gt;
&lt;p&gt;There are two options we can go with: using a pre-fabricated root filesystem (rootfs), or creating our own
using Docker.&lt;/p&gt;
&lt;h2 id=&#34;using-an-existing-root-filesystem&#34;&gt;Using an Existing Root Filesystem&lt;/h2&gt;
&lt;p&gt;Some distros publish these. For Arch Linux, you can find them on 
&lt;a href=&#34;https://gitlab.archlinux.org/archlinux/archlinux-docker/-/releases&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitLab&lt;/a&gt;.
Two main images are available: base and base-devel. The latter has the base-devel 
&lt;a href=&#34;https://archlinux.org/groups/x86_64/base-devel/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;package group&lt;/a&gt; pre-installed.&lt;/p&gt;
&lt;p&gt;For Fedora, you can head over to 
&lt;a href=&#34;https://github.com/fedora-cloud/docker-brew-fedora/tree/37/x86_64&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;
to get a copy of the rootfs. Note that for Fedora, the rootfs is merely part of the repo and not a separate release
page. You&amp;rsquo;ll be able to pick your base version of Fedora by switching branches in the repository.&lt;/p&gt;
&lt;p&gt;These rootfs images are usually compressed. Before you can use them with WSL2, the tarball needs to be extracted.
The Arch Linux rootfs can be extracted with zstd and the Fedora rootfs can be extracted using 7z.&lt;/p&gt;
&lt;h2 id=&#34;making-your-own-root-filesystem&#34;&gt;Making your Own Root Filesystem&lt;/h2&gt;
&lt;p&gt;Docker allows you to export a container to a root filesystem tarball:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker export -o &amp;lt;rootfs-name&amp;gt;.tar &amp;lt;container-or-image-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The neat thing here is that you can use either an image or a container name.&lt;/p&gt;
&lt;p&gt;Arch Linux images are available from 
&lt;a href=&#34;https://hub.docker.com/_/archlinux&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DockerHub&lt;/a&gt;. Available
tags include the above mentioned base and base-devel. Fedora is also available on 
&lt;a href=&#34;https://hub.docker.com/_/fedora&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DockerHub&lt;/a&gt;
and its tags include version numbers (e.g., 37 or 36).&lt;/p&gt;
&lt;h2 id=&#34;additional-setup&#34;&gt;Additional Setup&lt;/h2&gt;
&lt;p&gt;Once you have imported the distro you only have a barebones system available. Likely only the root user is
available, which is not ideal. You&amp;rsquo;ll want to install the packages you want to use and set up your own
user in addition to root. If you are building your own rootfs using Docker, you can build everything
interactively in your container by running &lt;code&gt;docker run -it &amp;lt;image-name&amp;gt;:&amp;lt;tag&amp;gt;&lt;/code&gt; to drop into a
shell and do all your setup there. Alternatively, you can create a Dockerfile with the basic setup
and build an image from that.&lt;/p&gt;
&lt;h3 id=&#34;arch-linux&#34;&gt;Arch Linux&lt;/h3&gt;
&lt;p&gt;Pacman won&amp;rsquo;t work out-of-the-box because it doesn&amp;rsquo;t ship with keys. You&amp;rsquo;ll need to run &lt;code&gt;pacman-keys --init&lt;/code&gt;
first. Install your favorite software using pacman, e.g.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pacman -Syu exa htop vim
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;User management and other common setup tasks are covered in the Arch Wiki&amp;rsquo;s 
&lt;a href=&#34;https://wiki.archlinux.org/title/General_recommendations#System_administration&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;General Recommendations&lt;/a&gt;.
Key tasks include 
&lt;a href=&#34;https://wiki.archlinux.org/title/Users_and_groups#User_management&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;adding a new user&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;useradd -m -G wheel $username
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;fedora&#34;&gt;Fedora&lt;/h3&gt;
&lt;p&gt;Make sure to run &lt;code&gt;dnf upgrade&lt;/code&gt; to get the latest version of your packages.
You may need to install either the &lt;code&gt;util-linux&lt;/code&gt; or &lt;code&gt;util-linux-core&lt;/code&gt; packages in order to get
the mount command working (used by WSL to mount the Windows filesystem). To be
able to add a non-root user with a password you&amp;rsquo;ll need to make sure that &lt;code&gt;passwd&lt;/code&gt; is installed.&lt;/p&gt;
&lt;p&gt;To add a non-root user in Fedora, use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;useradd -G wheel $username 
passwd $username  # in interactive mode, you&#39;ll type in your password here
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;general-case&#34;&gt;General Case&lt;/h3&gt;
&lt;p&gt;In order to actually start the WSL instance as your non-root user, you&amp;rsquo;ll need
to edit &lt;code&gt;/etc/wsl.conf&lt;/code&gt; inside of your distro. If the user section doesn&amp;rsquo;t exist
yet, you can just run&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo -e &amp;quot;\n[user]\ndefault = ${username}\n&amp;quot; &amp;gt;&amp;gt; /etc/wsl.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those are the basics to get you up and running. Not everything will necessarily
work smoothly out-of-the box as you may be missing some packages that you&amp;rsquo;re not
aware of until you need them, but overall I&amp;rsquo;ve had a positive experience with this
setup.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SAS Markdown for Reproducibility</title>
      <link>https://dmsenter89.github.io/post/22-11-sas-markdown-for-reproducibility/</link>
      <pubDate>Fri, 11 Nov 2022 15:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/22-11-sas-markdown-for-reproducibility/</guid>
      <description>&lt;p&gt;One of the coolest packages for R is 
&lt;a href=&#34;https://yihui.org/knitr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;knitr&lt;/a&gt;. Essentially, it allows you to combine explanatory writing, such as a paper or blog post, directly with your analysis code in a Markdown document. When the target document
is compiled (&amp;lsquo;knitted&amp;rsquo;), the R code in the document is run and the results inserted into the final document. The target document could
be an HTML or a PDF file, for example. This is great for many reasons. You have a regular report you want to run, but the data updates?
Just re-knit and your entire report is updated. No more separate running of the code followed by copying the results into whatever
software you use to build the report itself. This makes it not just less cumbersome, but less error prone. It also improves reproducibility.
Somebody wants to see your work, perhaps because they are unsure of your results or they want to extend your work? You can share the
markdown file and the other party can see exactly what code was used to generate what part of your report or paper.&lt;/p&gt;
&lt;p&gt;While knitr is certainly not the first package that allows for this workflow, and also not the only one, I have found it to be the most consistent and easy to use.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Luckily, knitr supports 
&lt;a href=&#34;https://yihui.org/knitr/demo/engines/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a variety&lt;/a&gt; of 
&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/language-engines.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;languages&lt;/a&gt;, including 
&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/eng-sas.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS&lt;/a&gt;. And you can even mix and match multiple languages in 
&lt;a href=&#34;https://github.com/yihui/knitr-examples/blob/master/106-polyglot.Rmd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;one document&lt;/a&gt;.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;You might think that this sounds similar to Jupyter notebooks. While that is true, and there is a 
&lt;a href=&#34;https://github.com/sassoftware/sas_kernel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter kernel for SAS&lt;/a&gt; as well, knitr has some advantages over Jupyter for report-generation. Without additional tools, you have the option to execute but not display the code that generates your results, making a cleaner report. You can also elect to only show part of the code, with manual setup code running behind the scenes without being printed to the report itself. Additionally, the entire document is executed linearly. That means that if you update a code chunk towards the beginning of your document, it affects the code chunks following it, while in Jupyter you easily get in the habit of executing the chunks independently which can lead to inconsistencies if you don&amp;rsquo;t pay attention to the cell numbers.&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;In this post, I&amp;rsquo;ll demonstrate the basics of setting up a reproducible report using the SAS engine in knitr.&lt;/p&gt;
&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;
&lt;p&gt;Perhaps the easiest way to get started for beginners is to use RStudio and Anaconda. With that you can create a sample R Markdown document (&lt;code&gt;File -&amp;gt; New File -&amp;gt; R Markdown&lt;/code&gt;). Press the &lt;code&gt;knit&lt;/code&gt; button. If any packages required by knitr are missing, RStudio will install them for you. This way you can be sure that all the R parts are set up correctly. Additionally, I recommend installing the 
&lt;a href=&#34;https://github.com/Hemken/SASmarkdown&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SASmarkdown&lt;/a&gt; package with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# from CRAN:
install.packages(&amp;quot;SASmarkdown&amp;quot;)
# from GitHub: 
devtools::install_github(&amp;quot;Hemken/SASmarkdown&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once install is complete, load the package (&lt;code&gt;library(SASmarkdown)&lt;/code&gt;) and check the output. If you see a message that SAS was found, you are good to go. If not, you will either need to add SAS to your PATH or simply provide the path to SAS as an option in your document (see below).&lt;/p&gt;
&lt;h2 id=&#34;a-basic-markdown-file&#34;&gt;A Basic Markdown File&lt;/h2&gt;
&lt;p&gt;The important thing is to load the SASMarkdown package in your document. I recommend making a setup chunk at the very top of your document and setting include to FALSE.
That way the setup chunk is executed, but not printed to your final document.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r setup, include=FALSE}
library(SASmarkdown)
# if SAS is not in your path, define it manually:
saspath &amp;lt;- &amp;quot;C:/Program Files/SASHome/SASFoundation/9.4/sas.exe&amp;quot;
knitr::opts_chunk$set(engine=&amp;quot;sashtml&amp;quot;, engine.path=saspath)
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With that, we&amp;rsquo;re ready to run a basic SAS chunk using just the SAS option. This produces the typewriter-style output that is familiar from Enterprise Guide for example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;```{sas example1}
proc print data=sashelp.class; run;
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to take advantage of the modern HTML output that is standard in SAS Studio, we use the &lt;code&gt;sashtml&lt;/code&gt; engine instead:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;```{sashtml example2}
/* if you want, you can set an ODS style for HTML output: */
ods html style=journal;
proc print data=sashelp.class; run;
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want graphical output, for example from SGPLOT, you&amp;rsquo;ll need to use the &lt;code&gt;sashtml&lt;/code&gt; engine. To get the default blue look from SAS Studio, use the HTMLBLUE style:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;```{sashtml example3}
ods html style=HTMLBLUE;
proc sgplot data=sashelp.cars;
  scatter x=EngineSize y=MPG_CITY;
run;
```
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;some-additional-comments&#34;&gt;Some Additional Comments&lt;/h2&gt;
&lt;p&gt;The first thing that is important to note is that each chunk is processed &lt;em&gt;separately&lt;/em&gt;. That means each chunk should be written so as to be capable of being executed independent of the others. It is possible to get around this using the &lt;code&gt;collectcode=TRUE&lt;/code&gt; chunk option. This chunk will then subsequently be executed prior to the code from a following chunk. So for example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;```{sashtml save1, collectcode=TRUE}
data sample;
  set sashelp.class;
run;
```
  And now use it again:
```{sashtml save2}
proc means data=sample; run;
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is particularly useful for libnames and setting the preferred ODS style, so you don&amp;rsquo;t have to keep doing it again in each cell.&lt;/p&gt;
&lt;p&gt;The other thing to note is that knitr for SAS works best with HTML output. It can use SAS styles and produce output looking like what
you would expect running in SAS Studio. If you want PDF output, you can get nicer output using 
&lt;a href=&#34;https://support.sas.com/rnd/base/ods/odsmarkup/latex.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX Tagsets for ODS&lt;/a&gt; and the 
&lt;a href=&#34;https://support.sas.com/rnd/app/papers/statrep.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StatRep System&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;knitr itself was based on Sweave, but uses Markdown instead of LaTeX code. Other languages have similar packages, for
example 
&lt;a href=&#34;https://mpastell.com/pweave/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pweave&lt;/a&gt; for Python or 
&lt;a href=&#34;https://docs.juliahub.com/Weave/9EzOc/0.9.4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weave&lt;/a&gt; for Julia. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The chunks from different languages do not have access to each other&amp;rsquo;s data. To move data between the different engines,
more setup work is needed. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;If you code in Julia, there is an interesting new reactive notebook called 
&lt;a href=&#34;https://github.com/fonsp/Pluto.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pluto&lt;/a&gt; that
promises to always keep your cells in sync, while being geared towards a Jupyter-style workflow. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Does it ever make sense to play the Lottery?</title>
      <link>https://dmsenter89.github.io/post/22-09-lottery/</link>
      <pubDate>Fri, 30 Sep 2022 15:40:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-09-lottery/</guid>
      <description>&lt;p&gt;In a first semester probability course, students encounter combinatorics and point estimates such as the mean and median of a data set. A common example is the low odds of winning the lottery. When discussing the topic of point estimates, students are exposed to the idea of a &amp;ldquo;fair bet&amp;rdquo; or &amp;ldquo;fair game&amp;rdquo; - one in which the expected value of the random variable associated with the game is equal to the cost of participation or zero, depending on if a fixed cost is included in the game or tracked separately. This year, the Mega Millions had a jackpot in excess of one billion dollars. This had me thinking - mathematically, this is likely a fair game. But I still would expect to loose out playing it. In this article, I want to explore this idea further using the Mega Millions lottery as a particular example.&lt;/p&gt;
&lt;p&gt;Mega Millions is played by a choosing five numbers from 1 to 70 (the white balls) and one number from 1 to 25 (the golden &amp;ldquo;Mega Ball&amp;rdquo;). Five white balls (W) and one golden ball (G) are drawn without replacement twice per week. Prizes are earned by matching the drawn numbers. Payouts generally follow a fixed schedule for everything but the jackpot, at least outside of California where the payouts for all prizes are pari-mutual instead. Below is a table of all possible events as given on the Mega Millions 
&lt;a href=&#34;https://www.megamillions.com/How-to-Play.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;, sorted by increasing odds.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Event&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Variable&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Value&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Odds&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;5 W + G&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_1$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Jackpot&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/302,575,350&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5 W&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_2$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$1,000,000&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/12,607,306&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4 W + G&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_3$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$10,000&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/931,001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4 W&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_4$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$500&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/38,792&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3 W + G&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_5$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$200&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/14,547&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2 W + G&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_6$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$10&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/693&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3 W&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_7$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$10&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/606&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1 W + G&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_8$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/89&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_9$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No Match&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_{10}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;24/1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;the-fair-bet-analysis&#34;&gt;The Fair Bet Analysis&lt;/h2&gt;
&lt;p&gt;A Fair Bet or Fair game is one in which the expected value of the random variable doesn&amp;rsquo;t favor either the player or the house. Given a cost of 2 USD per game,
we can say that Mega Millions is fair when $E[X]=2$, or more specifically when&lt;/p&gt;
&lt;p&gt;$$E[X] = \sum_{i=1}^{10} x_i P(X=x_i) = \frac{n}{302,575,351}  + \sum_{i=2}^{10} x_i P(X=x_i) = 2$$&lt;/p&gt;
&lt;p&gt;I use Maxima to solve for the jackpot representing a fair game and to print a few representative values of the expected value for some jackpot options.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-maxima&#34;&gt;/* Define Expectation as dependent on n */
E(n) := n/302575351 + 1000000/12607307 + 10000/931002 +
		500/38793 + 200/14548 +  10/694 + 10/607 + 4/90 + 2/38;
/* solve for fair game */
float(solve(E(n)=2,n));

/* give expected return for different jackpot values */
jackpots : [5e7, 1e8, 2.5e8, 5e8, 7.5e8, 1e9, 2e9];
for i in jackpots do printf(true, &amp;quot;E(~:D) = $~$ ~%&amp;quot;, i, E(i))$
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this we learn that to have a fair jackpot, we require $n = 531,123,698.80$. Even with a fair pet, the expected value is very modest. For example, a 2 billion USD jackpot has $E[X]~=6.85$ - less than 5 USD above the ticket price.&lt;/p&gt;
&lt;h2 id=&#34;how-long-until-we-profit&#34;&gt;How long until we Profit?&lt;/h2&gt;
&lt;p&gt;Most people don&amp;rsquo;t play the lottery to win small amounts like 5 USD. They want to become millionaires. Given that our expected values are so low, let&amp;rsquo;s take a look at how long it will take us to become rich if we take the lottery game route.&lt;/p&gt;
&lt;h3 id=&#34;the-geometric-distribution-and-our-lottery&#34;&gt;The Geometric Distribution and Our Lottery&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start by considering only the events that would result in a win of a million dollars or more. In other words, events $x_1$ and $x_2$. We have&lt;/p&gt;
&lt;p&gt;$$P(x_1 \vee x_2) = \frac{315,182,658}{3,814,660,340,689,757} \approx 8.262\times 10^{-8}. $$&lt;/p&gt;
&lt;p&gt;If we are only interested in this outcome, we can treat our outcome as a Bernoulli variable with $p=P(x_1 \vee x_2)$. Then the expected number of games we need to play to win a million dollars or more is distributed like a geometric with $E[G] = 1/p$. For our specific case:&lt;/p&gt;
&lt;p&gt;$$ E[G] = \frac 1 p = \frac{3,814,660,340,689,757}{315,182,658} \approx 12,103,014.$$&lt;/p&gt;
&lt;p&gt;Recall that two games are played per week. Converting this expected number of games to years, it would take approximately $115,977$ years for us to win. Even if one drawing were held each day, we would expect to take more than $33,000$ years to win.&lt;/p&gt;
&lt;p&gt;Since the CDF of the geometric distribution is well defined, we can use it to estimate the number of games required for a certain likelihood of having a win of at least a million dollars. To have roughly 50% odds of winning, we need to play about $8,400,000$ games of Mega Millions. Note that in this case you would still likely be in the hole since the $1,000,000$ USD jackpot is nearly 24 times more likely than the main jackpot and each game costs 2 USD to play.&lt;/p&gt;
&lt;h3 id=&#34;simulating-a-lifetime-of-playing&#34;&gt;Simulating a Lifetime of Playing&lt;/h3&gt;
&lt;p&gt;At this point you might agree that the lottery is not a good get-rich-quick scheme. That alone doesn&amp;rsquo;t mean that you are all but guaranteed to loose money over a lifetime of playing. So let&amp;rsquo;s run some simulations and see what the distribution of our net worth is after taking everything into account. To make things as fair as possible, we will assume a constant jackpot of 750 million USD.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say you spend 50 years playing the Mega Millions at 2 USD for one ticket at each of the two weekly drawings. That comes out to just about $2,609$ weeks or $5,218$ games for a total price of $10,436$ USD. I simulated $50,000$ individuals each playing $5,218$ games for a constant jackpot of $750,000,000$ USD - much higher than the 
&lt;a href=&#34;https://www.megamillions.com/jackpot-history&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;typical jackpot&lt;/a&gt; and advantageous to the players. This will cost them each $10,436$ USD in ticket costs over the 50 years they play. Yet, despite the simulated lottery being rigged in the players&#39; favor, 99% of my players win less than 600 USD total over this 50 year time period.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Statistic&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Value ($)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9,169.43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SD&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20,985.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Min&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9,974.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9,912.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9,898.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;75%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9,890.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;99%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9,884.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Max&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1,990,204.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;From these results it is clear that for all but the luckiest few, even just saving the money under a mattress outperforms playing the lottery. You can explore a distribution plot of my simulation with Plotly 
&lt;a href=&#34;results.html&#34;&gt;here&lt;/a&gt;. Note that this page may take a moment to load due to the many data points. You will need to zoom in on the left-hand side to be able really make anything out.&lt;/p&gt;
&lt;h4 id=&#34;implementation-note&#34;&gt;Implementation Note&lt;/h4&gt;
&lt;p&gt;The number of simulations grows quickly given the $5,218$ games we are using. Doing $50,000$ simulations of that many games requires over 260 million random draws. Prototyping in Python often makes sense because of the many features available for analysis and plotting, but this seems like an example where a compiled language might outperform by a considerable amount. I decided to 
&lt;a href=&#34;https://github.com/dmsenter89/lottery-sims&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;try this out (GitHub)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All of the programs were written with an emphasis on simplicity over performance so as to avoid biasing the results. Since the different individuals play their games independently, I wrote both a single-threaded C++ version as well as one utilizing OpenMP&amp;rsquo;s parallel for loop. As alternative compiled languages I added implementations in Go and Rust.&lt;/p&gt;
&lt;p&gt;For scripting languages I included Python and Julia. In Julia the main loop can trivially be set to run concurrently by prepending &lt;code&gt;Threads.@threads&lt;/code&gt; to the for loop, so inlcuded that as an option as well. This instructs the Julia interpreter to run this loop with the available threads. By default this is one, but can be set higher using an environment variable or by starting Julia with the &lt;code&gt;-t&lt;/code&gt; flag and specifying the desired number of threads.&lt;/p&gt;
&lt;p&gt;I used 
&lt;a href=&#34;https://github.com/sharkdp/hyperfine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hyperfine (GitHub)&lt;/a&gt; to benchmark the performance of my programs in WSL; see output below for details.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Command&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Mean [s]&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Min [s]&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Max [s]&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Relative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;C++ (Single Thread)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.602 ± 0.087&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.514&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.828&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.88 ± 0.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;C++ (OpenMP)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.653 ± 0.170&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.506&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.092&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.08 ± 0.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Go&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9.362 ± 0.058&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9.258&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9.417&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.83 ± 0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Julia&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;28.606 ± 0.372&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;28.198&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;29.520&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;11.69 ± 0.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Julia (4 Threads)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;19.016 ± 0.274&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;18.673&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;19.511&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7.77 ± 0.23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Python&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;57.727 ± 0.530&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;59.783 ± 3.811&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;57.833&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;70.242&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rust&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.447 ± 0.062&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.391&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.579&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I was surprised by Rust&amp;rsquo;s performance. I only looked up enough Rust to be able to implement this simple example, so I find it surprising that it can keep up with a multi-threaded C++ implementation.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Updates: This blogpost has been updated with new benchmark values. The original post did not include results in Rust.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Life Expectancy Data</title>
      <link>https://dmsenter89.github.io/post/22-09-life-expectancy/</link>
      <pubDate>Fri, 02 Sep 2022 13:11:30 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/22-09-life-expectancy/</guid>
      <description>&lt;p&gt;Most people can guess the current life expectancy for Americans at birth as being in the high 70s or around 80. In fact, given the 
&lt;a href=&#34;https://www.ssa.gov/oact/STATS/table4c6.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;current mortality table&lt;/a&gt; published by the Social Security Administration (SSA), males have a life expectancy of about 76 compared to a female life expectancy of about 81. Of course that is only an expected value. Guessing the &lt;em&gt;distribution&lt;/em&gt; of a person&amp;rsquo;s life expectancy is somewhat more difficult. In this post, we&amp;rsquo;ll take a look at some simulated lives to get a feel for the distribution of life expectancy and its implications for retirement planning.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s begin by looking at our mortality table. The rows indicate an individual&amp;rsquo;s current age. For both a male and a female, three values are then given: the probability of death in a given year, the &amp;ldquo;Number of Lives&amp;rdquo;, and the life expectancy for this individual. The probability of death in a given year is somewhat self-explanatory. The &amp;ldquo;Number of Lives&amp;rdquo; variable starts with 100,000 individuals and gives the number of survivors at a given age. So for example, of the 100,000 males &amp;ldquo;born&amp;rdquo; at age 0, we expect 99,392 to be alive at age 1. The life expectancy is the expected number of years of life remaining for an individual. We can start by plotting this to get a feel for the data.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-survival-curve-for-100000-males-and-females-given-the-2019-ssa-mortality-tables-the-dashed-line-indicates-the-typical-retirement-age-of-67&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-09-life-expectancy/survivalPlot_hu8976ee6293875d841810953bda6bf5f5_123683_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Survival curve for 100,000 males and females given the 2019 SSA mortality tables. The dashed line indicates the typical retirement age of 67.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-09-life-expectancy/survivalPlot_hu8976ee6293875d841810953bda6bf5f5_123683_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2606&#34; height=&#34;1684&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Survival curve for 100,000 males and females given the 2019 SSA mortality tables. The dashed line indicates the typical retirement age of 67.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;To get a feeling for the distribution of age at death, I ran 10,000 simulations each for males and females starting at ages 0, 25, 40, 60, and 80. These ages were chosen to represent the full range of possibilities at birth, followed by early, mid- and late career individuals. Age 80 was included for comparison as an older retiree value. Since the probability of death by age 50 is so low, we expect very little difference for the first three ages, with differences becoming more pronounced as age progresses, but it is still useful to visualize.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-overview-of-the-distribution-of-age-at-death-by-sex-for-different-ages-at-the-beginning-of-the-simulation-outliers-are-are-not-represented-note-how-the-results-for-ages-0-to-40-are-nearly-indistinguishable&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-09-life-expectancy/Sample_Overview_hub6662c611b080d5ff9aca3d611ac8c57_52510_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Overview of the distribution of age at death by sex for different ages at the beginning of the simulation. Outliers are are not represented. Note how the results for ages 0 to 40 are nearly indistinguishable.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-09-life-expectancy/Sample_Overview_hub6662c611b080d5ff9aca3d611ac8c57_52510_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2606&#34; height=&#34;1684&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Overview of the distribution of age at death by sex for different ages at the beginning of the simulation. Outliers are are not represented. Note how the results for ages 0 to 40 are nearly indistinguishable.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;As expected, we see relatively little variation between birth and age 40, with some recognizable changes beginning at age 60. Given that, I will visualize the distribution for an individual starting at age 40. A 40 year old is about 25-30 years away from retirement and has probably at least started thinking about saving and how much they&amp;rsquo;ll need to put away to last through retirement.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-distribution-of-age-at-death-for-males-and-females-given-a-starting-age-of-40-half-of-the-starting-population-is-expected-to-make-it-to-at-least-8185-malefemale-and-a-quarter-will-make-it-at-least-to-8891&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-09-life-expectancy/distributionPlot_hue89b8b06e05cd8b82745d76fbdceaf60_146013_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Distribution of age at death for males and females given a starting age of 40. Half of the starting population is expected to make it to at least 81/85 (Male/Female), and a quarter will make it at least to 88/91.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-09-life-expectancy/distributionPlot_hue89b8b06e05cd8b82745d76fbdceaf60_146013_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2606&#34; height=&#34;1684&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Distribution of age at death for males and females given a starting age of 40. Half of the starting population is expected to make it to at least 81/85 (Male/Female), and a quarter will make it at least to 88/91.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;So now that have seen the distribution, let&amp;rsquo;s consider how long we&amp;rsquo;ll live past the typical retirement age of 67. The table below lists the ages by sex for the top percentiles given a starting age of 40.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Top Percentiles - Age at Death&lt;/th&gt;
&lt;th&gt;Males&lt;/th&gt;
&lt;th&gt;Females&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5%&lt;/td&gt;
&lt;td&gt;95&lt;/td&gt;
&lt;td&gt;98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10%&lt;/td&gt;
&lt;td&gt;93&lt;/td&gt;
&lt;td&gt;95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20%&lt;/td&gt;
&lt;td&gt;89&lt;/td&gt;
&lt;td&gt;92&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;30%&lt;/td&gt;
&lt;td&gt;86&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;40%&lt;/td&gt;
&lt;td&gt;84&lt;/td&gt;
&lt;td&gt;87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;50%&lt;/td&gt;
&lt;td&gt;81&lt;/td&gt;
&lt;td&gt;85&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that 40% of females and 30% of males are expected to live at least 20 years past retirement age. A little more than 5% of females will make it thirty years past retirement, but only 2.5% of males will. While only a small minority of retirees will need to fund their retirement for thirty or more years, it is not unreasonable to target retirement funds to last until we reach age 90.&lt;/p&gt;
&lt;p&gt;Unfortunately, a large share of Americans have insufficient 401k balances to cover their expected longevity (see 
&lt;a href=&#34;https://www.forbes.com/advisor/retirement/average-401k-balance-by-age/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, 
&lt;a href=&#34;https://www.investopedia.com/articles/personal-finance/010616/whats-average-401k-balance-age.asp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, and 
&lt;a href=&#34;https://mint.intuit.com/blog/retirement/average-401k-balance-by-age/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for some estimates of savings by age group). Many are likely relying on social security benefits to cover some of the difference. This system may not last that long, or at least not with current benefit levels. Social Security outlays have exceeded allocated revenues since 2010 and are currently expected to continue to do so well into the 2090s (see 
&lt;a href=&#34;https://www.cbo.gov/publication/57342&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Table A-1&lt;/a&gt;). Social security trust fund balances for old-age and survivor benefits are rapidly declining. Between 2020 and 2030, the CBO expects a drop of 80% in this fund. Curiously, over the same time period the trust fund for military personnel is expected to grow by more than 70%, while the fund for civilian government employees is expected to grow by more than 20% (see 
&lt;a href=&#34;https://www.cbo.gov/publication/56541&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CBO report&lt;/a&gt;). As such, younger Americans not working for the government will need to consider how to fund a multi-decade retirement in the face of potentially large reductions in social security benefits.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is it better to buy or rent housing?</title>
      <link>https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/</link>
      <pubDate>Sat, 20 Aug 2022 16:21:38 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/</guid>
      <description>&lt;p&gt;This post is a follow-up to my post on how to load 
&lt;a href=&#34;https://dmsenter89.github.io/post/22-08-zillow-data/&#34;&gt;data from Zillow&lt;/a&gt;. Housing prices have soared through the COVID-19 pandemic, leading to a lot of discussion about housing affordability. The quickly growing home values coupled with the subsequent raising of interest rates on mortgages are seeing more and more people priced out of  the ability to purchase a home. While rent prices have increased as well, they haven&amp;rsquo;t increased as sharply as home prices.&lt;/p&gt;
&lt;p&gt;In this post, I will look at the Zillow data set and consider a popular question for millennials - is it better to buy or rent in the current market?
For this analysis, I will use the zip code level data of the Zillow Home Value Index (ZHVI) and the Zillow Observed Rent Index (ZORI) for North Carolina metropolitan areas in the years 2019 through June 2022. To evaluate the monthly costs of owning a home, I will utilize the 30 year fixed rate mortgage average from the 
&lt;a href=&#34;https://fred.stlouisfed.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FRED database&lt;/a&gt;, a comprehensive database aggregating various economic time series maintained by the St. Louis Federal Reserve.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: This post is not financial advice. We are only exploring some aggregate data sets. Past performance is not indicative of future performance.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#initial-thoughts&#34;&gt;Initial Thoughts&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#crafting-a-mortgage-to-rent-index&#34;&gt;Crafting a Mortgage-to-Rent Index&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#a-naive-implementation&#34;&gt;A Naive Implementation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#a-slightly-better-implementation&#34;&gt;A Slightly Better Implementation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#further-thoughts&#34;&gt;Further Thoughts&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#risk-and-other-costs&#34;&gt;Risk and Other Costs&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#housing-as-an-investment&#34;&gt;Housing as an Investment&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#last-thoughts&#34;&gt;Last Thoughts&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;

&lt;h2 id=&#34;initial-thoughts&#34;&gt;Initial Thoughts&lt;/h2&gt;
&lt;p&gt;Both Zillow data sets are available with monthly data published at the
zip code level. As such, they are equally well-spaced in time. One issue
we notice at initial inspection is that the ZHVI observations are dated to
the last day of every month, while the ZORI observations are dated to the first of the month. For merging and comparison, we will set the ZHVI to the first of the month.&lt;/p&gt;
&lt;p&gt;Both data sets are available for download at the zip code level. The ZORI data set is much smaller, however. It covers 2,453 distinct zip codes compared to the ZHVI&amp;rsquo;s 27,366 distinct zip codes.&lt;/p&gt;
&lt;p&gt;An initial graph of average ZHVI and ZORI for North Carolina shows the dramatic growth in home values and the degree to which rent is lagging behind. Both y-axes have been scaled proportionally.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-average-zillow-home-value-index-zhvi-and-zillow-observed-rent-index-zori-values-for-north-carolina-the-left-and-right-axes-have-been-scaled-proportionally&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/average_ZHVI_ZORI_huc9fc43bcc57fade6a9155dc83f0fbcac_25161_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Average Zillow Home Value Index (ZHVI) and Zillow Observed Rent Index (ZORI) values for North Carolina. The left and right axes have been scaled proportionally.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/average_ZHVI_ZORI_huc9fc43bcc57fade6a9155dc83f0fbcac_25161_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Average Zillow Home Value Index (ZHVI) and Zillow Observed Rent Index (ZORI) values for North Carolina. The left and right axes have been scaled proportionally.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The FRED data set is published weekly on Thursdays
and provides a seasonally unadjusted look at the average, actual mortgage rates that
homebuyers have received throughout the US. So while this data set has the largest number of data points in time, it is the least granular on a geographic level. I don&amp;rsquo;t have detailed information about the geographic variability in mortgage rates and will ignore this for now. To allow merging with the Zillow data, the FRED data set will need to be averaged in some form. For this post, I will use simple averages by month. It is important to note that the FRED data is not in decimal notation. That means that 3.5% is written as 3.5 as opposed to 0.035.&lt;/p&gt;
&lt;p&gt;One thing that is interesting about the FRED data is that it goes back as far as 1971. Looking at the overall historic values, we see that the past decade&amp;rsquo;s very low interest rates (less than 5%) are an anomaly.&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/fredgraph_hucd93d0035b9764d5c892e209e7d9935a_74003_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/fredgraph_hucd93d0035b9764d5c892e209e7d9935a_74003_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1168&#34; height=&#34;450&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;crafting-a-mortgage-to-rent-index&#34;&gt;Crafting a Mortgage-to-Rent Index&lt;/h2&gt;
&lt;p&gt;I will create two indices, one being a very naive implementation focusing just on the actual monthly payment on the mortgage loan and the monthly rent payments. This is followed by an improved index that takes into account additional monthly expenses. After both indices have been created, I will discuss some of their shortcomings.&lt;/p&gt;
&lt;h3 id=&#34;a-naive-implementation&#34;&gt;A Naive Implementation&lt;/h3&gt;
&lt;p&gt;For monthly cost comparisons, we don&amp;rsquo;t actually care about the value of the home directly. What matters is the monthly mortgage cost. There is a lot of variability here and I will make the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mortgage originates in the month given by &lt;code&gt;Date&lt;/code&gt; at the rate given by the monthly average of the Mortgage30US time series.&lt;/li&gt;
&lt;li&gt;A 20% down payment was made, so that the mortgage amount is for 80% of the ZHVI. A 20% down payment is often recommended because it avoids the need for mortgage insurance, which would create an additional monthly expense.&lt;/li&gt;
&lt;li&gt;Closing costs are handled separately and not rolled into the mortgage.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Monthly mortgage payments can be calculated with the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/vdmmlcdc/8.1/ds2ref/n0a5e0wwqhslcvn1y173c5c4nbg2.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PMT function&lt;/a&gt; in SAS. The general syntax is &lt;code&gt;PMT( rate, number-of-periods, principal-amount [, future-amount] [, type])&lt;/code&gt;. The &lt;code&gt;rate&lt;/code&gt; is the APR divided by the number of periods in a year, in our case 12. The number of periods is 12 payments over 30 years, i.e. 360. The principal amount is 80% of ZHVI, while the future amount is zero (default value). SAS allows us to pick of we want to use end of period (&lt;code&gt;type=0&lt;/code&gt;) or beginning of period (&lt;code&gt;type=1&lt;/code&gt;) payments. For the monthly amount the difference is small, so I will use the default end of period scheme. With that, our formula for the monthly payment is &lt;code&gt;PMT(M30Rate/12, 360, 0.8*ZHVI)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now construct a simple unit-less ratio of monthly mortgage costs over monthly rent. If this index is greater than 1, it is cheaper to rent while if it is less than 1 it is cheaper to purchase a home. Below is a figure demonstrating the distribution of this index in North Carolina for 2019 through June 2022.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-mean-index-value-for-north-carolina-the-shaded-region-represents-the-25th-through-75th-percentile-of-index-values-note-that-the-index-is-1-until-2022-despite-the-sharp-increase-in-zhvi-across-the-state-seen-above&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/index_mean_q1_q3_huadcd954adcdbd90973c926bbc7d2269a_19435_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Mean index value for North Carolina. The shaded region represents the 25th through 75th percentile of index values. Note that the index is &amp;amp;lt;1 until 2022, despite the sharp increase in ZHVI across the state seen above.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/index_mean_q1_q3_huadcd954adcdbd90973c926bbc7d2269a_19435_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Mean index value for North Carolina. The shaded region represents the 25th through 75th percentile of index values. Note that the index is &amp;lt;1 until 2022, despite the sharp increase in ZHVI across the state seen above.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The index indicates that the situation was favorable to home buyers until 2022, when it flips to being generally favorable to renters. FRED data indicate that the average mortgage rate didn&amp;rsquo;t substantially begin growing until winter 2021. This lends credence to the view that historically low interest rates have helped buffer would-be homeowners from increases in home value.&lt;/p&gt;
&lt;p&gt;Individual zip codes&#39; index values behave similar in pattern to the the figure above, making it a good approximation to observed behavior.&lt;/p&gt;
&lt;h3 id=&#34;a-slightly-better-implementation&#34;&gt;A Slightly Better Implementation&lt;/h3&gt;
&lt;p&gt;The previous implementation underestimates the true monthly cost of home ownership. For one, homeowners are required to pay property taxes each year. While an individual doesn&amp;rsquo;t directly pay the local government on a monthly basis, funds for this purpose are typically collected in an escrow account so it is an ongoing cost. Property tax rates are set at the county and municipal levels and mapping them directly to the ZIP codes requires some work. The rates themselves are fixed fees for every $100 of assessed home value. For the purpose of taxation, the home value used is &lt;em&gt;not&lt;/em&gt; the ZHVI, but the value assigned to the home by the county. In North Carolina these assessments must take place at least once every 8 years, but individual counties may choose to do more frequent assessments. As such, there can be a large discrepancy between the ZHVI and the assessed value. I have seen cases where the ZHVI is roughly double that of the county&amp;rsquo;s assessed value. Finding the average assessed value is easy for individual homes (Zillow displays it on its website), but finding an aggregate value that can be used for analysis is tricky. For ease of use I&amp;rsquo;ll assume that assessed value is 70% of ZHVI. I expect this measure to underestimate the assessed values of homes in 2019 but to be close to target in 2022.&lt;/p&gt;
&lt;p&gt;Based on the 
&lt;a href=&#34;https://www.ncdor.gov/taxes-forms/property-tax/property-tax-rates&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;most recent&lt;/a&gt; effective tax rates published by the North Carolina Department of Revenue, the average tax rate is approximately 1.02%, with a minimum of 0.33% and a maximum of 1.7%.&lt;/p&gt;
&lt;p&gt;In addition to taxes, escrow accounts will typically collect monthly insurance fees as well. The two main types of insurance included are homeowners insurance and mortgage insurance. Mortgage insurance is mandatory when purchasing a home with less than a 20% down payment and protects the lender from the borrower defaulting on the loan repayment. In the above example we assumed a 20% down payment, so mortgage insurance would be optional.&lt;/p&gt;
&lt;p&gt;The cost of homeowners insurance varies wildly as can be seen in 
&lt;a href=&#34;https://www.nerdwallet.com/article/insurance/average-homeowners-insurance-cost&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this article&lt;/a&gt; from NerdWallet. Their calculated average monthly cost for North Carolina is $142. Renters may be required to purchase renters insurance, but this is not a requirement for all properties and the cost is substantially lower than homeowners insurance. NerdWallet&amp;rsquo;s 
&lt;a href=&#34;https://www.nerdwallet.com/article/insurance/how-much-is-renters-insurance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;analysis&lt;/a&gt; gives a monthly average cost of $12 for North Carolina.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-mean-adjusted-index-value-for-north-carolina-the-shaded-region-represents-the-25th-through-75th-percentile-of-index-values&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/index_mean_q1_q3_adj_huce0c902e7276254854e7cbbd20be94dc_20731_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Mean adjusted index value for North Carolina. The shaded region represents the 25th through 75th percentile of index values.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/index_mean_q1_q3_adj_huce0c902e7276254854e7cbbd20be94dc_20731_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Mean adjusted index value for North Carolina. The shaded region represents the 25th through 75th percentile of index values.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;As expected, the adjusted index shows the same overall behavior but is shifted up. This index gives a more complicated picture, in which the zip code starts mattering somewhat more in whether renting or buying is cheaper on a monthly basis.&lt;/p&gt;
&lt;h2 id=&#34;further-thoughts&#34;&gt;Further Thoughts&lt;/h2&gt;
&lt;p&gt;The above analysis was relatively simple, but a good excuse to take a look at the data made available by Zillow. This is not the end of the story, however. I have made several simplifications here, although on average I would argue they were in favor of home purchases. There additional matters to consider,  which I will take up below. We also only discussed aggregate data. Particular housing may offer other benefits, especially if you have knowledge of some likely future events. For example, a large tech company planning to move to an area tends to increase both rent and housing prices, so purchasing now may be beneficial by locking in today&amp;rsquo;s prices. We also haven&amp;rsquo;t considered whether there are differences in housing stock and location that may outweigh purely financial considerations.&lt;/p&gt;
&lt;h3 id=&#34;risk-and-other-costs&#34;&gt;Risk and Other Costs&lt;/h3&gt;
&lt;p&gt;Another item to consider is the question of risk in either scenario. A homeowner incurs somewhat larger risks of on-going expenses that we have conveniently ignored. Roofs, HVAC systems, etc. need maintenance and eventually will need to be repaired. Neither last forever either, so they will each need to be replaced at least once, if not more often, during the 30 years the house is being paid down. A 
&lt;a href=&#34;https://ipropertymanagement.com/research/hoa-statistics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;majority&lt;/a&gt; of new construction and a large number of existing homes are part of a Home Owners Association (HOA). Being part of an HOA not only increases monthly cost due to HOA fees, but also exposes a homeowner to the risk of special assessments. These are one-time financial obligations on homeowners to cover some expense for the HOA.&lt;/p&gt;
&lt;p&gt;While a renter may be immune to costs such as these, they are not immune to changing rent prices. By having a fixed-rate mortgage on the other hand, monthly housing expenses stay locked at current rates. Furthermore, while rent is not tax deductible, mortgage interest is. Under current guidelines, up to 750,000 USD in mortgage interest on your primary home can be deducted from your income tax. This is especially valuable during the first 15 years of the loan, when interest accounts for the majority of the mortgage payment.&lt;/p&gt;
&lt;h3 id=&#34;housing-as-an-investment&#34;&gt;Housing as an Investment&lt;/h3&gt;
&lt;p&gt;One issue that we have ignored so far is the idea of home purchases as investments. Home prices have historically increased. Each year lived and mortgage paid on a property increases your share in the home&amp;rsquo;s value, otherwise known as equity. Some would argue that even if the monthly cost of homeownership is in excess of the cost of renting an equivalent home, it&amp;rsquo;s worth it in the long run as an investment in your financial future. Proponents may point to homeownership being a key component in long term wealth accrual.&lt;/p&gt;
&lt;p&gt;Assessing the value of equity growth in wealth building needs to be viewed in opposition to investing the potential price differential. We are only talking about the difference in housing cost being utilized here, not in using additional funds to pay down a mortgage early. This has not been an effective strategy for nearly 30 years.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; The S&amp;amp;P 500&amp;rsquo;s average annual return during the period of 1975-2021 has been 10.2%. Home values on the other hand increased annually by an average of only 4.5% between 1975 and today.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; This would indicate investment in the S&amp;amp;P 500 would be expected to outperform real estate investment for the average person over the long run.&lt;/p&gt;
&lt;p&gt;The variability of the two is rather different, however. For comparison, we can show a distribution of annual returns from the S&amp;amp;P 500 compared to FRED&amp;rsquo;s All-Transactions House Price Index.&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-histograms-showing-the-distribution-of-annual-returns-of-the-all-transactions-house-price-index-for-north-carolina-and-the-sp-500-for-1976-2021&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/investment_housing_sp_comp_hu4e2264ca40b96fcef6fef1eaaeebef26_27041_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Histograms showing the distribution of annual returns of the All-Transactions House Price Index for North Carolina and the S&amp;amp;amp;P 500 for 1976-2021.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/investment_housing_sp_comp_hu4e2264ca40b96fcef6fef1eaaeebef26_27041_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;960&#34; height=&#34;720&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Histograms showing the distribution of annual returns of the All-Transactions House Price Index for North Carolina and the S&amp;amp;P 500 for 1976-2021.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;While this shows that we can expect greater payoff from stock market investments compared to real estate, this is not helpful if we have to pay for housing and don&amp;rsquo;t have excess funds to throw at the stock market. If the actual monthly cost of homeownership and rent are about equal and max out our available funds, then purchasing a home builds at least &lt;em&gt;some&lt;/em&gt; equity, no matter how small, compared to no investment being made. Only when rent is lower than mortgage costs and fees does it make sense to start comparing rates of return.&lt;/p&gt;
&lt;p&gt;We should never underestimate the power of compounding, however. Consider the following example: you have the option of purchasing a 400,000 USD home with a 80,000 USD (20%) down payment at 5% interest as a 30 year fixed rate mortgage. Assume no other costs. In total, this housing purchase will cost you approximately 698,000 USD over the course of 30 years.  Assuming 4.53% annual increase in housing value, this leaves you with about 1,553,000 USD in equity at the end. Given your total cost you have made a profit of approximately 855,000 USD. Now compare this to stock market returns, assuming the 10.22% annual growth we found above. Investing the down payment only, with no further payments made, would leave you with approximately 1,695,000 USD after 30 years. To match the amount of equity in our example home after 30 years would require only a 73,500 USD in initial investment. If we only wanted to match the profit of about 855,000 USD with a one-time investment, we would require an initial investment of approximately 42,500 USD.&lt;/p&gt;
&lt;p&gt;Despite these superior returns from investment, we still need to live somewhere. I have already mentioned that unlike a mortgage payment, rent is not fixed over the 30 year timespan. How much should I expect renting to cost me? Let&amp;rsquo;s assume we invest the entire down payment and rent a home that costs exactly the same as the mortgage would have cost. Assuming once annual 
&lt;a href=&#34;https://ipropertymanagement.com/research/average-rent-by-year&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;increases in rent of 4.17%&lt;/a&gt;, we spend a total of nearly 1,190,000 USD on rent over the course of 30 years. Given these expenses, the profit from investing the 80,000 USD shrinks to a mere 500,000 USD.&lt;/p&gt;
&lt;p&gt;This suggests a strategy of minimizing the down payment and investing the difference, if possible. One downside of this is that having a smaller down payment may lead to less favorable mortgage terms. In this second scenario, let&amp;rsquo;s assume that we make a 40,000 USD down payment on a 400,000 USD home with an increased, but still fixed, mortgage rate of 5.25% over 30 years coupled with a 40,000 USD initial investment in the S&amp;amp;P 500 at 10.22% increase annually with no additional investments made. Here we invest a total of approximately 766,000 USD into our home, reducing our profit on the home purchase to approximately 797,000 USD. But our initial 40,000 USD investment has accumulated to about 847,000 USD. This brings our total profit to approximately 1,644,000 USD in this scenario - nearly double the 855,000 USD profit in the 80,000 USD down payment scenario.&lt;/p&gt;
&lt;h3 id=&#34;last-thoughts&#34;&gt;Last Thoughts&lt;/h3&gt;
&lt;p&gt;Overall, we see that the more precise we want to be, the more difficult it becomes to estimate an ideal strategy as there are a lot of moving pieces, not all of which can best be approximated by a geographic average value. Given the very different variability in housing value increase compared to stock market returns, it may make sense to generate a large number of random walks to get a better feel for the distribution of outcomes after 30 years. Finally, this entire post has assumed that we remain living in the same home for the entire 30 year life of the mortgage. Many will want to move at some point during their next thirty years, perhaps to make room for a larger  family, to down-size, or to follow a job opportunity. This creates additional costs and considerations for deciding between renting and owning a home.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Average mortgage rates have been below average annual return of the S&amp;amp;P 500 since the 1990s. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Annual returns of S&amp;amp;P 500 were calculated from data provided by 
&lt;a href=&#34;https://www.investopedia.com/ask/answers/042415/what-average-annual-return-sp-500.asp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Investopedia&lt;/a&gt;. Annual returns of housing prices were calculated from the All-Transactions House Price Index for North Carolina (FRED time series NCSTHPI). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;I have chosen this measure for historic house price increases as this index is available on a quarterly basis starting in 1975, unlike Zillow data which only goes back a little more than 20 years. Both US wide data (USSTHPI) and state specific data (e.g., for North Carolina: NCSTHPI) are available from FRED. I will use the North Carolina data set for my comparison. An alternative measure to consider is the median sales price of houses sold in the South census region, available on a quarterly basis starting in 1965 from FRED (MSPS). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Cross Compiling With Go</title>
      <link>https://dmsenter89.github.io/post/22-08-cross-compiling-with-go/</link>
      <pubDate>Thu, 11 Aug 2022 09:15:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-08-cross-compiling-with-go/</guid>
      <description>&lt;p&gt;A big difference between using compiled languages like C/C++ compared to scripting languages
like Javascript or Python is that prior to execution, compiled languages require an explicit
compilation step where the human readable code is translated to machine code for execution,
the so-called &amp;ldquo;binary&amp;rdquo; of the code. Typically, a separate binary needs to be compiled for
each target operating system and architecture. Compiling for your own machine is not a problem.
The difficulty lies in creating binaries for machines that you don&amp;rsquo;t normally use, so you might
not have an extra Mac lying around just to compile your program on for other Mac users.
Compiling programs for an operating system or architecture other than the one you are working
with is called cross-compiling. This would allow a Linux developer to create binaries for
Windows and Mac computers, for example.&lt;/p&gt;
&lt;p&gt;For most languages, this requires installing additional development tools and increases the
complexity of the compilation workflow. I have found Go to be a pleasant exception to this,
because cross-compilation is built into the standard Go tools. There is no need to learn
any additional build-tools. All you need to learn about are some system variables that you
need to set when compiling.&lt;/p&gt;
&lt;p&gt;Go build tools know which system you are building for by checking the GOOS and GOARCH environment
variables. If they are unset, the tools fall back to GOHOSTOS and GOHOSTARCH. In other words,
to change the target OS/architecture for your build, all you have to do is set the GOOS and GOARCH
variables during the build. So say you want to build a simple program hello.go for a Windows
computer with the same architecture as your development machine. All you have to do is write&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;GOOS=windows go build hello.go
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;instead of just &lt;code&gt;go build hello.go&lt;/code&gt; and you&amp;rsquo;re good to go. This would produce a &lt;code&gt;hello.exe&lt;/code&gt;
binary you could copy to a Windows machine to run.&lt;/p&gt;
&lt;p&gt;To check what combinations of GOOS and GOARCH are valid, run &lt;code&gt;go tool dist list&lt;/code&gt;. To see
which environment variables Go is currently seeing, run &lt;code&gt;go env&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Loading Zillow Housing Data in SAS</title>
      <link>https://dmsenter89.github.io/post/22-08-zillow-data/</link>
      <pubDate>Mon, 01 Aug 2022 17:21:38 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-08-zillow-data/</guid>
      <description>&lt;p&gt;Zillow is a well-known website widely used by those searching for a home or curious to find out
the value of their current home. What you may not know is that Zillow has a dedicated research page.
To make their website work optimally, they churn through tons of data on the American housing market.
They share insights they gleaned via 
&lt;a href=&#34;https://www.zillow.com/research/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;zillow.com/research&lt;/a&gt;. If you
visit their research  website you&amp;rsquo;ll notice they have a data page where you can download some really
cool data sets for your own research. They even have an API with which you can load data directly, but
you&amp;rsquo;ll have to register for access. In this post, we&amp;rsquo;ll look at how to load the CSV files that are
available for direct download into SAS for analysis.&lt;/p&gt;
&lt;p&gt;The CSV files can be downloaded 
&lt;a href=&#34;https://www.zillow.com/research/data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. In the example below,
I&amp;rsquo;m working with the Zillow Home Value Index file for all homes, seasonally adjusted at the ZIP code level.
Tha file is fairly large. It has data going from January 2000 through June 2022 in more than 27,000 rows of data
and about 280 columns. Below is an image of the beginning of this file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;zhvi_data_preview.png&#34; alt=&#34;&#34; title=&#34;The beginning of the of the ZHVI &#39;flagship&#39; data file.&#34;&gt;&lt;/p&gt;
&lt;p&gt;When working with large CSV files, I find it useful to get a feel for it in the CLI with

&lt;a href=&#34;https://csvkit.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;csvkit&lt;/a&gt;. This is especially important when importing
with a SAS data step, because we need to know the number of columns and their order, amongst other things,
for our code. To get an overview of the total number of columns and their contents, run&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;csvcut -n Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is fairly long, so you may prefer piping to a pager. I don&amp;rsquo;t need all the different identifiers
in the file, so I&amp;rsquo;m going to exclude those I won&amp;rsquo;t need and put them into a separate, smaller CSV.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ignore these four columns which I won&#39;t need
csvcut -C RegionID,SizeRank,RegionType,StateName Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv &amp;gt; Zip_zhvi_small.csv
# alternatively, also cut down on date columns to only 2022 for debugging 
csvcut -C RegionID,SizeRank,RegionType,StateName,10-273 Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv &amp;gt; Zip_zhvi_small.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also reduce the file size by using &lt;code&gt;csvgrep&lt;/code&gt; to filter any of the columns. For example, if we only wanted
the data for North Carolina we could run &lt;code&gt;csvgrep -c State -m NC&lt;/code&gt; in the pipe.&lt;/p&gt;
&lt;p&gt;For SAS, we need to know the maximum length of string columns so we can allocate the appropriate length to the
corresponding SAS variables. This is easily done with the csvstat tool:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;csvcut -c Metro,City,CountyName Zip_zhvi_small.csv | csvstat --len
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also specify the list of columns in csvstat directly, but in my experience that tends to be slower.&lt;/p&gt;
&lt;p&gt;Alright, now we have everything we need to start on our DATA step! We start with the attribute statement.
One problem with importing this file is that everyhing is in wide format, with the dates used as headers.
We will get around this shortly. I have seen people use transpose etc for similar problems online, but this
is unnecessary if we feel comfortable with the DATA step. We&amp;rsquo;ll start by naming the identifying columns
just as in the CSV file. For the date columns, we will use a numeric range prefixed by date (&lt;code&gt;date1-date270&lt;/code&gt;).
You can use csvcut to find the exact number of date columns you have. We will also allocate the same number of
columns for the ZHVI values, so we&amp;rsquo;ll need to add a &lt;code&gt;val1-val270&lt;/code&gt;. This and the date variable are temporary
and will be dropped later, in favor of the &lt;code&gt;Date&lt;/code&gt; and &lt;code&gt;ZHVI&lt;/code&gt; variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;attrib 
    ZIP           informat=best12.    format=z5.
    State         informat=$2.
    City          informat=$30.
    Metro         informat=$42.
    CountyName    informat=$29.
    date1-date270 informat=YYMMDD10.  format=DATE9.
    val1-val270   informat=best16.
    Date                              format=Date9.
    ZHVI                              format=Dollar16.
  ;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we will allocate an array to hold &lt;em&gt;all&lt;/em&gt; of the date and ZHVI values during the processing of each row.
Since the date column won&amp;rsquo;t change, we&amp;rsquo;ll tell SAS to retain its values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;   retain date1-date270;
   array d(270) date1-date270;
   array v(270) val1-val270;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is where the magic happens now. You may not know it, but you are not limited to a single INPUT statement
in a DATA step. We use this and start by reading in only the first row. Because we use an OUTPUT
statement later, this reading of row 1 will be processed, but not saved into the output data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;if _n_ = 1 then do;
  input ZIP $ State $ City $ Metro $ CountyName $ date1-date270;
  PUT _ALL_; /* if you want to see what that looks like */
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this if clause, the date1 through date270 variables will be populated, and because we used a retain
statement earlier, these values remain available to us during the processing of every other row. You can
probably guess where this is going now: we will process each row, and then OUTPUT one line per date which
we have access to now thanks to our array and the retain statement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;input ZIP $ State $ City $ Metro $ CountyName $ val1-val270;
do i=1 to 270;
  Date  = d(i); /* look up date for column i */
  ZHVI =  v(i); /* use the corresponding i-th value for ZHVI */
  OUTPUT;       /* This output creates one line per date column */
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the end of your data step, don&amp;rsquo;t forget to&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;drop i date1-date270 val1-val270;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;so those variables don&amp;rsquo;t clutter your data set. And that&amp;rsquo;s it! You now
have the data set loaded and available in SAS.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;SAS_data_set.png&#34; alt=&#34;&#34; title=&#34;The beginning of the resulting SAS data set.&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conditional RegEx Matching with Python</title>
      <link>https://dmsenter89.github.io/post/22-05-conditional-regex-python/</link>
      <pubDate>Thu, 19 May 2022 21:45:46 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-05-conditional-regex-python/</guid>
      <description>&lt;p&gt;Recently I&amp;rsquo;ve needed to capture the entries of a datalines statement in SAS for editing. Generally,
this is a straight forward problem if I only need to do it with one file or all of the files that
I am using are formatted identically. But then I started thinking about the more general case. SAS
doesn&amp;rsquo;t care about the case of my keywords, so I need a case insensitive match. I need to account for
possible extra whitespace. So far so good. But what if I have two different keywords that can start
my data section, and the end of the data section is indicated with different characters depending on
the chosen keyword? Could I still use a single regular expression?&lt;/p&gt;
&lt;p&gt;SAS does in fact allow a number of different keywords to enter data in a data step.
In my experience, the most common are the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/v_027/lestmtsref/p0114gachtut3nn1and4ap8ke9nf.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;datalines&lt;/a&gt;
and 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/v_027/lestmtsref/p1mm9b070wj962n16q0v1d9uku5q.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;datalines4&lt;/a&gt;
statements. The main difference between them is how the end of the data is indicated. For datalines,
a single semicolon is used, while datalines4 uses a sequence of four semicolons, thereby allowing
the use of semicolons in the data itself. There are some aliases for these commands that can be used:
cards/lines and cards4/lines4 with matching behavior. A simple data step with these statements
could look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data person;
  input name $ sex $ age;
  datalines; /* or `cards` or `lines` */
Alfred M 14
Alice F 13
;

data person4;
  input name $ sex $ age;
  datalines4; /* or `cards4` or `lines4` */
Alfred M 14
Alice F 13
;;;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could write two separate RegEx expressions, one for the datalines/cards/lines statement and
a second one for the datalines4/cards4/lines4 statement. But, if the RegEx engine we are using allows
conditionals, e.g. the Python RegEx engine, then we can write a single statement that can capture
both types of statements. The basic format of the conditonal capture is &lt;code&gt;(?(D)A|B)&lt;/code&gt;, which can be read as &amp;ldquo;if capture
group D is set, then match A, otherwise match B.&amp;rdquo; For more details, see 
&lt;a href=&#34;https://www.regular-expressions.info/conditional.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Using this technique, we can capture both types of statements in one go.
The short form of the solution I found is this
regular expression: &lt;code&gt;r&amp;quot;(?:(?:(?:data)?lines)|cards)(4)?\s*;(.*?)(?(1);{4}|;)&amp;quot;&lt;/code&gt;
with two flags set: case insensitive and dot-all. If we utilize Python&amp;rsquo;s verbose flag,
we can format this a bit nicer as well:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;re.compile(
  r&amp;quot;&amp;quot;&amp;quot;(?:(?:    # mark groups as non-capture groups
      (?:data)? # maybe match `data`, but don&#39;t capture
       lines)   # matches `lines`
      |cards)   # alternatively, matches `cards`
      (4)?      # a `4` may be present
      \s*;      # there might be whitespace before the ;
      (.*?)     # lazy-match data content
      (?(1)     # check if capture group 1 is set, if so
      ;{4}      # match `;;;;`
      |;)       # otherwise, match a single ;
  &amp;quot;&amp;quot;&amp;quot;, flags=re.DOTALL | re.X | re.I)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A great website to help you build up a regular expression is 
&lt;a href=&#34;https://regex101.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;regex101.com&lt;/a&gt;.
It allows you to copy a sample text and regular expression. It then explains your expression and lists
the capture groups by number, which can be convenient. It also allows you to try out different RegEx engines.
Try setting it to Python with the flags we mentioned, and see how it works!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wordle in Golang</title>
      <link>https://dmsenter89.github.io/post/22-05-go-wordle/</link>
      <pubDate>Thu, 05 May 2022 16:30:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-05-go-wordle/</guid>
      <description>&lt;p&gt;Lately I&amp;rsquo;ve been playing around with 
&lt;a href=&#34;https://go.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Go&lt;/a&gt;. I&amp;rsquo;ve read about Go for a few years
and have been using some software written in Go (this website is built with 
&lt;a href=&#34;https://gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo&lt;/a&gt;),
but never tried it before. So what better way to give Go a shake
than to write some code. Since Wordle has been popular, I thought I&amp;rsquo;d write a very simple
Wordle implementation in Go; you can check it out on 
&lt;a href=&#34;https://github.com/dmsenter89/go-wordle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.
It&amp;rsquo;s been a good way for me to get familiar with some of the
basisc of Go, such as variables and their types, functions, etc. So far I&amp;rsquo;ve been enjoying it.&lt;/p&gt;
&lt;p&gt;The Go website has a very nicely written 
&lt;a href=&#34;https://go.dev/doc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; and

&lt;a href=&#34;https://pkg.go.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;package page&lt;/a&gt;. The 
&lt;a href=&#34;https://go.dev/play/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Go Playground&lt;/a&gt;
let&amp;rsquo;s you test out Go in your browser without needing to install anything. I&amp;rsquo;ve also found
Bodner&amp;rsquo;s &amp;ldquo;
&lt;a href=&#34;https://www.oreilly.com/library/view/learning-go/9781492077206/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning Go&lt;/a&gt;&amp;rdquo; to be helpful.&lt;/p&gt;
&lt;p&gt;Go is a compiled language with a pretty picky compiler. It won&amp;rsquo;t let you compile code
with unnecessary imports and variable declarations, which help keeps your code clean.
Cross-compilation is 
&lt;a href=&#34;https://freshman.tech/snippets/go/cross-compile-go-programs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;built-in&lt;/a&gt;.
While Go is not a common language in scientific computing, the 
&lt;a href=&#34;https://www.gonum.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gonum&lt;/a&gt; package
has implemented a number of important functions and seems to be well developed. I look forward
to learning more about Go in the future.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The INDSNAME Option in SAS</title>
      <link>https://dmsenter89.github.io/post/22-04-sas-indsname-option/</link>
      <pubDate>Wed, 20 Apr 2022 11:42:02 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-04-sas-indsname-option/</guid>
      <description>&lt;p&gt;I frequently find myself needing to concatenate data sets but also wanting to be able to distinguish
which row came from which data set originally. Introductory SAS courses tend to teach the &lt;code&gt;in&lt;/code&gt; keyword,
for a workflow similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data Concat1;
set data1(in = ds0)  
    data2(in = ds1);
if ds0 then source = &amp;quot;data1&amp;quot;;
else if ds1 then source = &amp;quot;data2&amp;quot;;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With more than two input data sets, this can get unwieldy and repetitive. In an old 
&lt;a href=&#34;https://blogs.sas.com/content/iml/2015/08/03/indsname-option.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;
on Rick Wicklin&amp;rsquo;s DO LOOP, a better method is introduced - the &lt;code&gt;indsname&lt;/code&gt; option. Using this method, the above code looks much nicer:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data Concat2;
set data1-data2 indsname = source;  /* the INDSNAME= option is on the SET statement */
libref = scan(source,1,&#39;.&#39;);        /* extract the libref */
dsname = scan(source,2,&#39;.&#39;);        /* extract the data set name */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As long as your input data sets are reasonably named, you&amp;rsquo;ll now have access to all the information needed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Working with the Census API Directly from SAS</title>
      <link>https://dmsenter89.github.io/post/22-04-census-api-with-sas/</link>
      <pubDate>Wed, 13 Apr 2022 08:27:35 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-04-census-api-with-sas/</guid>
      <description>&lt;p&gt;In a previous 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;post&lt;/a&gt;, I have shown how to connect to the Census API and load data
with Python. In this post, I will do the same using SAS instead. Before we get started, two important links
from last time: a guide to the API can be found 
&lt;a href=&#34;https://www.census.gov/data/developers/guidance/api-user-guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and a list of the
available data sets can be accessed 
&lt;a href=&#34;https://api.census.gov/data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;picking-the-data&#34;&gt;Picking the Data&lt;/h2&gt;
&lt;p&gt;For this post, I&amp;rsquo;ll use the same data as last time. There we used the 2018 American Community Survey 1-Year Detailed Table
and asked for three variables - total population, household income, and median monthly cost for Alamance and Orange
counties in North Carolina (FIPS codes 37001 and 37135). The variable names are not very intuitive, so I highly recommend starting
your code with a comment section that includes a markdown-style table of the variables that you want to use. Here is
an example table for our data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;B01003_001E&lt;/td&gt;
&lt;td&gt;Total Population&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B19001_001E&lt;/td&gt;
&lt;td&gt;Household Income (12 Month)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B25105_001E&lt;/td&gt;
&lt;td&gt;Median Monthly Housing Cost&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;building-the-query&#34;&gt;Building the Query&lt;/h2&gt;
&lt;p&gt;The next step is to build the query. Like last time, the API consists of a base
URL that points us to the data set we are looking for, a list of the variables
we want to request, and a description of the geography for which we want to
request those variables. Just like last time, I&amp;rsquo;ll build the query using several
macros for flexibility purposes. Note that since &lt;code&gt;&amp;amp;&lt;/code&gt; has a special meaning in SAS,
we need to use &lt;code&gt;%str(&amp;amp;)&lt;/code&gt; when referring to it to avoid having the log clobbered with
warnings about unresolved macros.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;%let baseurl=https://api.census.gov/data/2018/acs/acs1;
%let varlist=NAME,B01003_001E,B19001_001E,B25105_001E;
%let geolist=for=county:001,135%str(&amp;amp;)in=state:37;
%let fullurl=&amp;amp;baseurl.?get=&amp;amp;varlist.%str(&amp;amp;)&amp;amp;geolist.;
%put &amp;amp;=fullurl;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Your log should now show the full query URL:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FULLURL=https://api.census.gov/data/2018/acs/acs1?get=NAME,B01003_001E,B19001_001E,B25105_001E&amp;amp;for=county:001,135&amp;amp;in=state:37
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;making-the-api-request&#34;&gt;Making the API Request&lt;/h2&gt;
&lt;p&gt;The API call is achieved with a simple PROC HTTP call using a temporary file to hold the response from the server.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;filename response temp;

proc http url=&amp;quot;&amp;amp;fullurl.&amp;quot; method=&amp;quot;GET&amp;quot; out=response;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;handling-the-json-response&#34;&gt;Handling the JSON Response&lt;/h2&gt;
&lt;p&gt;We read the JSON response by utilizing the

&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lestmtsglobal/n1jfdetszx99ban1rl4zll6tej7j.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LIBNAME JSON Engine&lt;/a&gt;
in SAS.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;libname manual JSON fileref=response;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now run &lt;code&gt;proc datasets lib=manual; quit;&lt;/code&gt;. You&amp;rsquo;ll see two data sets that were created: ALLDATA which contains the whole JSON file&amp;rsquo;s contents
in a single data set, and ROOT which is a data set of all the root-level data. The latter one is the one we want. Here&amp;rsquo;s what the
first few observations in each look like:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-first-few-observations-in-the-automatically-created-data-sets&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/auto_datasets_hu58f00ccbf67d7d2f36e2c3ae0591a33b_44045_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;First few observations in the automatically created data sets.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/auto_datasets_hu58f00ccbf67d7d2f36e2c3ae0591a33b_44045_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1073&#34; height=&#34;490&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    First few observations in the automatically created data sets.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Just like with Python, all columns are treated as character variables at first. Because of the way the Census API is structured,
the first row consists of headers, which SAS didn&amp;rsquo;t use. This is something we&amp;rsquo;ll need to fix. At this point we have two main routes we can use to fix
these issues - we can manually create a new data set from ROOT with PROC SQL and address the issues in that way, or we can take
advantage of SAS&#39; JSON map feature to define how we want to load the JSON when the LIBNAME statement is executed. There are good use cases for each,
so I will show both methods.&lt;/p&gt;
&lt;h3 id=&#34;cleaning-up-via-proc-sql&#34;&gt;Cleaning up via PROC SQL&lt;/h3&gt;
&lt;p&gt;Using PROC SQL, you can rename all the character variables you want to keep. To change from character to numeric,
you&amp;rsquo;ll use the &lt;code&gt;input&lt;/code&gt; function. You can then assign formats and labels as desired. To get rid of the first row,
you can just add a conditional &lt;code&gt;having ordinal_root ne 1&lt;/code&gt; to avoid loading that line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;proc sql;
	create table census as
	select
		element1 as Name,
		input(element2, best12.) as B01003_001E format=COMMA12.  label=&#39;Total Population&#39;,
		input(element3, best12.) as B19001_001E format=DOLLAR12. label=&#39;Household Income (12 Month)&#39;,
		input(element4, best12.) as B25105_001E format=DOLLAR12. label=&#39;Median Monthly Housing Cost&#39;,
		element5 as state,
		element6 as county
	from manual.root
	having ordinal_root ne 1;
quit;
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-result-from-the-proc-sql-method&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/CensusData_SQL_hu13384c4c5502c575dbc7b9e51c49ebcb_20401_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Result from the PROC SQL method.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/CensusData_SQL_hu13384c4c5502c575dbc7b9e51c49ebcb_20401_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1022&#34; height=&#34;124&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Result from the PROC SQL method.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;A benefit of this method is that as you fix the input table, you can already begin to work with
it thanks to the &lt;code&gt;calculated&lt;/code&gt; keyword in PROC SQL. Say we weren&amp;rsquo;t actually interested in housing cost and
household income, but instead would like to know what percent of their annual income a household spends on
housing in a given county. We could just add a new variable to our PROC SQL call and build our table like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;proc sql;
	create table census as
	select
		element1 as Name,
		input(element2, best12.) as B01003_001E format=COMMA12. label=&#39;Total Population&#39;,
		input(element3, best12.) as B19001_001E format=DOLLAR12. label=&#39;Household Income (12 Month)&#39;,
		input(element4, best12.) as B25105_001E format=DOLLAR12. label=&#39;Median Monthly Housing Cost&#39;,
		/* Now calculate what we want from the new columns: */
		12*(calculated B25105_001E)/calculated B19001_001E as HousingCostPCT format=PERCENT10.2,
		element5 as state,
		element6 as county
	from manual.root
	having ordinal_root ne 1;
quit;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;using-a-json-map&#34;&gt;Using a JSON MAP&lt;/h3&gt;
&lt;p&gt;Alternatively, we could change the way SAS reads the JSON data by editing the JSON map it uses to decode
the JSON file. The first step is to ask SAS to create a map for us to edit:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;filename automap &amp;quot;sas.map&amp;quot;;
libname autodata JSON fileref=response map=automap automap=create;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The map will look something like this:





  
  











&lt;figure id=&#34;figure-beginning-of-the-automatically-created-json-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_hu793d98e625aa154a8d9815a25890d65f_22860_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Beginning of the automatically created JSON map.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_hu793d98e625aa154a8d9815a25890d65f_22860_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;460&#34; height=&#34;426&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Beginning of the automatically created JSON map.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Note that this is also a JSON file which you can edit in a text editor. With this map, you can change the names
of the data sets and variables, assign labels and formats, and also re-format incoming data. Variables and data sets
you don&amp;rsquo;t want to read can simply be deleted from the map. Here&amp;rsquo;s the beginning of my edited file:





  
  











&lt;figure id=&#34;figure-beginning-of-my-edited-json-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_EDITED_hua840053fe8872e6ad1fe911a8f4abee6_59487_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Beginning of my edited JSON map.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_EDITED_hua840053fe8872e6ad1fe911a8f4abee6_59487_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;475&#34; height=&#34;575&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Beginning of my edited JSON map.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Since the first row of observations in the JSON are actually a header and non-numeric, I add &lt;code&gt;?&lt;/code&gt; prior to the
specified informat. This prevents errors in the log and simply replaces non-matching variables with missing values.
We can now reload the JSON using our custom map by dropping the &lt;code&gt;automap=create&lt;/code&gt; option from the LIBNAME statement:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;libname autodata JSON fileref=response map=automap;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I now print the resulting data set, the header row is still there, but replaced by missing values in numeric
columns:





  
  











&lt;figure id=&#34;figure-the-data-set-as-a-result-of-the-edited-json-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/MAP_RESULT_hua2828dd6a29f70dd2548d0bd22c856b1_25610_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;The data set as a result of the edited JSON map.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/MAP_RESULT_hua2828dd6a29f70dd2548d0bd22c856b1_25610_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1026&#34; height=&#34;165&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The data set as a result of the edited JSON map.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This means we&amp;rsquo;ll need to additionally drop this row in a separate step using a delete statement either in
a PROC SQL or DATA step.&lt;/p&gt;
&lt;p&gt;Whichever method you choose, you now can access data via an API call from SAS. Happy exploring!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multline Bash Variable Replacement</title>
      <link>https://dmsenter89.github.io/post/22-03-multiline-replacement/</link>
      <pubDate>Wed, 16 Mar 2022 09:23:12 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-03-multiline-replacement/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve recently needed to append several lines of data to a SAS data step that I collected and built
via a shell script. For search-and-replace in bash I typically use sed, but this time I ran into a problem -
sed does not like multiline shell variables. Thanks to Stack, I found a way to accomplish this task using awk instead.&lt;/p&gt;
&lt;p&gt;Suppose you have a file called data.sas with the following contents:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data person;
   infile datalines delimiter=&#39;,&#39;; 
   input name :$10. dept :$30.;
   datalines4;                      
John,Sales
Mary,Accounting
Theresa,Management
Stewart,HR
;;;;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I am using a datalines4 statement so that I get an easy to identify target for the substitution.
I want to insert a multiline shell variable before the &lt;code&gt;;;;;&lt;/code&gt; to add my data to this data step. Say I have the
following variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;NEWDATA=$(cat &amp;lt;&amp;lt;-END
Will,Compliance
Sidney,Management
END
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I try to use sed (&lt;code&gt;sed &amp;quot;s/\;\{4\}/$DATA\n;;;;/&amp;quot; data.sas&lt;/code&gt;) I will get an error about an unterminated s command.
Instead of sed, I can use awk with a variable to achieve the same goal:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;awk -v r=&amp;quot;$NEWDATA\n;;;;&amp;quot; &#39;{gsub(/;{4}/, r)}1&#39; data.sas
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The one downside is that awk does not have an in-place option like sed, and if I try to redirect to the same file
I&amp;rsquo;m reading from I get an empty file out. So you&amp;rsquo;ll have to rename the original file in your processing script to
achieve a similar effect as with the inplace option in sed.&lt;/p&gt;
&lt;p&gt;For additional approaches, see this 
&lt;a href=&#34;https://stackoverflow.com/q/10107459&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StackOverflow Question&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Easy SASPy Setup from Jupyter</title>
      <link>https://dmsenter89.github.io/post/22-03-saspy-setup/</link>
      <pubDate>Fri, 11 Mar 2022 08:30:29 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-03-saspy-setup/</guid>
      <description>&lt;p&gt;I love using SASPy, but the setup can take a minute. I used to do the setup via the CLI until I
started thinking I might be able to just do it straight from a Jupyter notebook. Having just a
couple of cells in Jupyter notebook makes for easy copy-and-paste and reduces setup time. The code
below has been tested on both Windows and Linux. As a bonus,
this also works on  
&lt;a href=&#34;https://colab.research.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can easily install packages via pip from Jupyter either by using a shell cell (&lt;code&gt;!&lt;/code&gt;) or by
using the pip magic command: &lt;code&gt;%pip install saspy&lt;/code&gt;. Once done, copy and paste the following into
a code cell and run to create the sascfg_personal.py file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import saspy, platform
from pathlib import Path

# get path for configuration file
cfgpath = saspy.__file__.replace(&#39;__init__.py&#39;,&#39;sascfg_personal.py&#39;)

# To pick the path for Java, we need to know whether we&#39;re on Windows or not
if platform.system()==&#39;Windows&#39;:
    print(&amp;quot;Windows detected.&amp;quot;)
    javapath = !where java
    authfile = Path(Path.home(),&amp;quot;_authinfo&amp;quot;)
else:
    javapath = !which java
    authfile = Path(Path.home(),&amp;quot;.authinfo&amp;quot;)
    
# the `!` command returns a string list, we want only the string
javapath = javapath[0]
print(f&amp;quot;Java is present at {javapath}&amp;quot;)

# US home Region configuration string set up via string-replacement.
# For other server addresses, see https://support.sas.com/ondemand/saspy.html
cfgtext = f&amp;quot;&amp;quot;&amp;quot;SAS_config_names=[&#39;oda&#39;]
oda = {{&#39;java&#39; : &#39;{repr(javapath).strip(&amp;quot;&#39;&amp;quot;)}&#39;,
#US Home Region
&#39;iomhost&#39; : [&#39;odaws01-usw2.oda.sas.com&#39;,&#39;odaws02-usw2.oda.sas.com&#39;,&#39;odaws03-usw2.oda.sas.com&#39;,&#39;odaws04-usw2.oda.sas.com&#39;],
&#39;iomport&#39; : 8591,
&#39;authkey&#39; : &#39;oda&#39;,
&#39;encoding&#39; : &#39;utf-8&#39;
}}&amp;quot;&amp;quot;&amp;quot;

# write the configuration file
with open(cfgpath, &#39;w&#39;) as file:
    file.write(cfgtext)
    print(f&amp;quot;Wrote configuration file to {cfgpath}&amp;quot;)
    print(f&amp;quot;Content of file: \n```\n{cfgtext}\n```&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Optionally, you can set up an authentication file with your username and password. Without this file,
you&amp;rsquo;ll be prompted for your username and password each time you log in.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# change variables to match your username and password
omr_user_id = r&amp;quot;max.mustermann@sample.com&amp;quot;
omr_user_password = r&amp;quot;K5d7#QBPw&amp;quot;
with open(authfile, &amp;quot;w&amp;quot;) as file:
    file.write(f&amp;quot;oda user {omr_user_id} password {omr_user_password}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it! You&amp;rsquo;re now ready to connect to SASPy. In my experience you don&amp;rsquo;t even need to restart
the kernel to begin work with SAS on ODA. You can try the following snippet in a new cell:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# starts a new SAS session with the `oda` configuration we set up
sas_session = saspy.SASsession(cfgname=&#39;oda&#39;)

# load a SAS data set and make a scatter plot
cars = sas_session.sasdata(&#39;cars&#39;, &#39;sashelp&#39;)
cars.scatter(x=&#39;msrp&#39;, y=&#39;horsepower&#39;)

# directly run SAS code to print a table
sas_session.submitLST(&amp;quot;proc print data=sashelp.cars(obs=6); run;&amp;quot;)

# quit SAS connection
sas_session.endsas()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Cleaning up a Date String with RegEx in SAS</title>
      <link>https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/</link>
      <pubDate>Wed, 29 Sep 2021 13:41:36 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/</guid>
      <description>&lt;p&gt;Sometimes we have to deal with manually entered data, which means there is a good chance that the data needs to be cleaned for consistency due to the
inevitable errors that creep in when typing in data, not to speak of any inconsistencies between individuals entering data.&lt;/p&gt;
&lt;p&gt;In my particular case, I was recently dealing with a data set that included
manually calculated ages that had been entered as a complete string
of the number of years, months, and days of an individual. Such a string
is not particularly useful for analysis and I wanted to have the age as
a numeric variable instead. Regular expressions can help out a lot in this
type of situation. In this post, we will look at a few representative examples
of the type of entries I&amp;rsquo;ve encountered and how to read them using RegEx in SAS.&lt;/p&gt;
&lt;h2 id=&#34;lets-look-at-the-data&#34;&gt;Let&amp;rsquo;s Look at the Data&lt;/h2&gt;





  
  











&lt;figure id=&#34;figure-what-were-starting-from&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/RAW_DS_hu83735545411e40d8d73c711ad73aa038_21697_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;What we&amp;amp;rsquo;re starting from.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/RAW_DS_hu83735545411e40d8d73c711ad73aa038_21697_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;508&#34; height=&#34;250&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    What we&amp;rsquo;re starting from.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;If we look at our sample data, we notice a few things. The data is consistently
ordered from largest to smallest, in the order of year, month, and day.
For some lines, only the year variable is available. In all cases, the string
starts with two digits.&lt;/p&gt;
&lt;p&gt;Separation of the time units is inconsistent; occasionally they are separated
by commas, sometimes by hyphens, and in some cases by spaces alone. The terms
indicating the units are spelled and capitalized inconsistently as well. There
are some abbreviations and occasionally the plural &amp;rsquo;s&#39; in days is wrapped in
parentheses.&lt;/p&gt;
&lt;p&gt;If you want to follow along, you can create the sample data with the
following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data raw;
    infile datalines delimiter = &#39;,&#39; MISSOVER DSD;
    attrib
        ID     informat=best32. format=1.
        STR_AGE informat=$500.   format=$500. label=&#39;Age String&#39;
        VAR1   informat=best32. format=1.;
    input ID STR_AGE $ VAR1;

    datalines;
    1,&amp;quot;62 Years, 5 Months, 8 Days&amp;quot;,1
    2,43 Yrs. -2 Months -4 Day(s), 2
    3,33 years * months 24 days, 1
    4,58,1
    5,&amp;quot;47 Yrs. -11 Months -27 Day(s)&amp;quot;,2
    ;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-regex-patterns&#34;&gt;The RegEx Patterns&lt;/h2&gt;
&lt;p&gt;We will use a total of three regex patterns, one for each of the time units:
year, month, day.  SAS uses Pearl regex and the function &lt;code&gt;prxparse&lt;/code&gt; to define
the regex patterns that are supposed to be searched for.&lt;/p&gt;
&lt;p&gt;For the year variable, we need to match the first two digits in our string.
Therefore, the correct call is &lt;code&gt;prxparse(&#39;/^(\d{2}).*/&#39;)&lt;/code&gt;. Note that the
&lt;code&gt;(&lt;/code&gt; and &lt;code&gt;)&lt;/code&gt; delimit the capture group.&lt;/p&gt;
&lt;p&gt;The month and day regex patterns are very similar. For the months, we want to
lazy-match the until we hit between one or two digits followed by
an &amp;rsquo;m&#39; and some number of other characters. We use the &lt;code&gt;i&lt;/code&gt; flag since
we cannot guarantee capitalization: &lt;code&gt;prxparse(&#39;/.*?(\d{1,2}).M.*/i&#39;)&lt;/code&gt;.
The day pattern is nearly identical: &lt;code&gt;prxparse(&#39;/.*?(\d{1,2}).D\D*$/i&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can extract our matches using the &lt;code&gt;prxposn&lt;/code&gt; function. We use the
&lt;code&gt;prxmatch&lt;/code&gt; function to check if we actually have a match:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;/* match into strings */
if prxmatch(year_rxid, STR_AGE)  then year_dig_str = prxposn(year_rxid,1,STR_AGE);
if prxmatch(month_rxid, STR_AGE) then month_dig_str = prxposn(month_rxid,1,STR_AGE);
if prxmatch(day_rxid, STR_AGE)   then day_dig_str = prxposn(day_rxid,1, STR_AGE);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The extracted strings can then be converted to numeric variables using
the &lt;code&gt;input&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;The last step is the calculation of the age from the three components.
Since not all three time units are specified for every row, we cannot use
the standard arithmetic of &lt;code&gt;years + months + days&lt;/code&gt;, because the missing
values would propagate. We need to use the &lt;code&gt;sum&lt;/code&gt; function instead.&lt;/p&gt;
&lt;p&gt;Putting it all together, we get the correct output:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-result&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/FIXED_DS_hu37f587f0a5cd00ab598beb8689b70b5f_34165_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;The Result&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/FIXED_DS_hu37f587f0a5cd00ab598beb8689b70b5f_34165_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;867&#34; height=&#34;251&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Result
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;complete-code&#34;&gt;Complete Code&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data fixed;
    set raw;
    
   /* define the regex patterns */
   year_rxid  = prxparse(&#39;/^(\d{2}).*/&#39;);
   month_rxid = prxparse(&#39;/.*?(\d{1,2}).M.*/i&#39;);
   day_rxid   = prxparse(&#39;/.*?(\d{1,2}).D\D*$/i&#39;);   /* match 2 digits followed by D and non-digit chars  */
  
   /* make sure we have enough space to store the extraction */
   length year_dig_str month_dig_str day_dig_str $4;
   
   /* match into strings */
   /* match into strings */
   if prxmatch(year_rxid, STR_AGE)  then year_dig_str = prxposn(year_rxid,1,STR_AGE);
   if prxmatch(month_rxid, STR_AGE) then month_dig_str = prxposn(month_rxid,1,STR_AGE);
   if prxmatch(day_rxid, STR_AGE)   then day_dig_str = prxposn(day_rxid,1, STR_AGE);
   
   /* use input to convert str -&amp;gt; numeric */
   years  = input(year_dig_str, ? 12.);
   months = input(month_dig_str, ? 12.);
   days   = input(day_dig_str, ? 12.);
   
   /* Use SUM function when calculating age
    to avoid missing values propagating  */
   age = sum(years,months/12,days/365.25);
   
   /* get rid of temporary variables */ 
   drop month_rxid month_dig_str year_rxid year_dig_str day_rxid day_dig_str;
   run;
   
proc print data=fixed; run;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>From Proc Import to a Data Step with Regex</title>
      <link>https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/</link>
      <pubDate>Thu, 29 Jul 2021 08:46:10 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/</guid>
      <description>&lt;p&gt;I find myself needing to import CSV files with a relatively large number of columns. In many cases, &lt;code&gt;proc import&lt;/code&gt; works surprisingly well in giving me what I want. But sometimes, I need to do some work while reading in the file and it would be nice to just use a data step to do so, but I don&amp;rsquo;t want to type it in by hand. That&amp;rsquo;s when a combination of &lt;code&gt;proc import&lt;/code&gt; and some regex substitution can come in handy.&lt;/p&gt;
&lt;p&gt;For the first step, run a &lt;code&gt;proc import&lt;/code&gt;, like this sample code that is provided by SAS Studio when you double click on a CSV file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;FILENAME REFFILE &#39;/path/to/file/data.csv&#39;;

PROC IMPORT DATAFILE=REFFILE
    DBMS=CSV
    OUT=WORK.IMPORT;
    GETNAMES=YES;
RUN;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you run this code, you will see that SAS generates a complete data step for you. This is what the beginning of one looks like:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-sample-log-output&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/log_hu417e5750fec5319adb043ca92305efb0_26856_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Sample log output.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/log_hu417e5750fec5319adb043ca92305efb0_26856_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;797&#34; height=&#34;461&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample log output.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;There will be be two lines for each variable, one giving the &lt;code&gt;informat&lt;/code&gt; and one giving the &lt;code&gt;format&lt;/code&gt; that SAS decided on. This will be followed by an &lt;code&gt;input&lt;/code&gt; statement. You can copy that from the log into a text editor such as VSCode, but unfortunately the line numbering of the LOG will carry over. One convenient way of fixing this is to use regex search-and-replace. Each line starts with a space followed by 1-3 digits, followed by a variable number of spaces until the next word. To capture this I use &lt;code&gt;^\s\d{1,3}\s+&lt;/code&gt; as my search term and replace with nothing. This will left align the whole data step, but this can be adjusted later.&lt;/p&gt;
&lt;p&gt;At this point the data step can be saved as a SAS file or copied back over to the file you are working within SAS Studio, but I like to do one more adjustment. I really like using the &lt;code&gt;attrib&lt;/code&gt; statement, 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lestmtsref/n1wxb7p9jkxycin16lz2db7idbnt.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see documentation&lt;/a&gt;, because it allows me to see the informat, format, and label of a variable all in one place. So I use regex to re-arrange my informat statement into the beginnings of an attribute statement. Use the search term &lt;code&gt;informat\s([^\s]+)\s([^\s]+)\s+;&lt;/code&gt; to capture each informat line and create two capture groups - the variable name as group 1 and the informat as group 2. If you use the replace code &lt;code&gt;$1 informat=$2 format=$2&lt;/code&gt;, you will see the beginnings of an attribute statement. In this replacement scheme, each informat matches each format. This is fine for date and character variables, but you may want to adjust the display format for some of your numeric variables.&lt;/p&gt;
&lt;p&gt;To clean this up, get rid of the format lines (you can search for &lt;code&gt;^format.+\n&lt;/code&gt; and replace with an empty replace to delete them), add the &lt;code&gt;attrib&lt;/code&gt; statement below the &lt;code&gt;infile&lt;/code&gt; and make sure to end the block of attributes with a semicolon, and indent your code as desired.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-sample-data-step-view&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/code_snip_hub5c78044ade6674b35a08b503d783f3d_19917_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Sample data step view.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/code_snip_hub5c78044ade6674b35a08b503d783f3d_19917_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;843&#34; height=&#34;190&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample data step view.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;And there you have it! The beginning of a nicely formatted data step that you can start to work with.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making VS Code and Python Play Nice on Windows</title>
      <link>https://dmsenter89.github.io/post/21-07-vsc-python-fix/</link>
      <pubDate>Wed, 21 Jul 2021 08:49:52 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-07-vsc-python-fix/</guid>
      <description>&lt;p&gt;One of the editors I use regularly is VS Code. I work a lot with Python, but when installing Anaconda
using default settings on a Windows machine already having VSC installed there&amp;rsquo;s a good chance you&amp;rsquo;ll run into
an issue. When attempting to run Python code straight from VSC you may get an error. This should be fixed
on some newer versions of Anaconda, but I&amp;rsquo;ve needed to do something about it often enough I feel it&amp;rsquo;s
useful to save the solution 
&lt;a href=&#34;https://stackoverflow.com/users/1072989/janh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;janh&lt;/a&gt; posted on

&lt;a href=&#34;https://stackoverflow.com/questions/54828713/working-with-anaconda-in-visual-studio-code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StackExchange&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Specifically, the issue can be fixed by manually changing VSC&amp;rsquo;s default shell from PowerShell to CMD.
Just open the command palette (CTRL+SHIFT+P), search &amp;ldquo;Terminal: Select Default Profile&amp;rdquo; and switch to
&amp;ldquo;Command Prompt&amp;rdquo;. Everything should work as expected from now on!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making INPUT and LABEL Statements with AWK</title>
      <link>https://dmsenter89.github.io/post/21-07-awk-for-sas/</link>
      <pubDate>Tue, 06 Jul 2021 10:38:27 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-07-awk-for-sas/</guid>
      <description>&lt;p&gt;I am currently working with a database provided by the North Carolina Department of Public Safety
that consists of several fixed-width files. Each of these has an associated codebook that gives the
internal variable name, a label of the variable, its data type, as well as the start column and
the length of the fields for each column. To import the data sets into SAS, I could copy and paste
part of that data into my INPUT and LABEL statements, but that gets tedious pretty fast when dealing
with dozens of lines. And since I have multiple data sets like that, I didn&amp;rsquo;t really want to do it that way.
In this post I show how a simple command-line script can be written to deal with this problem.&lt;/p&gt;
&lt;h2 id=&#34;introducing-awk&#34;&gt;Introducing AWK&lt;/h2&gt;
&lt;p&gt;Here are the first few lines of one of these files:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CMDORNUM      OFFENDER NC DOC ID NUMBER          CHAR      1       7     
CMCLBRTH      OFFENDER BIRTH DATE                DATE      8       10    
CMCLSEX       OFFENDER GENDER CODE               CHAR      18      30    
CMCLRACE      OFFENDER RACE CODE                 CHAR      48      30    
CMCLHITE      OFFENDER HEIGHT (IN INCHES)        CHAR      78      2     
CMWEIGHT      OFFENDER WEIGHT (IN LBS)           CHAR      80      3     
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the data is tabular and separated by multiple spaces. Linux programs often deal
with column data and a tool is available for manipulating column-based data on the command-line:
AWK, a program that can be used for complex text manipulation from the command-line. Some useful
tutorials on AWK in general are available at 
&lt;a href=&#34;https://www.grymoire.com/Unix/Awk.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;grymoire.com&lt;/a&gt;
and at 
&lt;a href=&#34;https://www.tutorialspoint.com/awk/index.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorialspoint&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For our purposes, we want to know about the &lt;code&gt;print&lt;/code&gt; and &lt;code&gt;printf&lt;/code&gt; commands for AWK. To illustrate
how this works, make a simple list of three lines with each term separated by a space:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cat &amp;lt;&amp;lt; EOF &amp;gt; list.txt
1 one apple pie
2 two orange cake
3 three banana shake
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To print the whole file, you&amp;rsquo;d use the print statement: &lt;code&gt;awk &#39;{print}&#39; list.txt&lt;/code&gt;. But I could do that with
&lt;code&gt;cat&lt;/code&gt;, so what&amp;rsquo;s the point? Well, what if I only want &lt;em&gt;one&lt;/em&gt; of the columns? By default, &lt;code&gt;$n&lt;/code&gt; refers to the
&lt;em&gt;n&lt;/em&gt;th column in AWK. So to print only the fruits I could write &lt;code&gt;awk &#39;{print $3}&#39; list.txt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Multiple columns can be printed by listing multiple columns separated by a comma:
&lt;code&gt;awk &#39;{print $2,$3}&#39; list.txt&lt;/code&gt;. Note that if you omit the comma the two columns get concatenated into
a single column.&lt;/p&gt;
&lt;p&gt;If additional formatting is required, we can use the &lt;code&gt;printf&lt;/code&gt; command. So to create a hyphenated
fruit and food-item column, we could use &lt;code&gt;awk &#39;{printf &amp;quot;%s-%s\n&amp;quot;, $3, $4}&#39; list.txt&lt;/code&gt;. Note that we
have to indicate the end-of line or else everything will be printed into a single line of text.&lt;/p&gt;
&lt;p&gt;Now we almost have all of the skills to create the label and input statements in SAS! Let&amp;rsquo;s create
a comma-delimited list for practice:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; list.txt
1,one,apple pie
2,two,orange cake
3,three,banana shake
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;-F&lt;/code&gt; flag is used to tell AWK to use a different column separator. So to print the
third column, we&amp;rsquo;d use &lt;code&gt;awk -F &#39;,&#39; &#39;{print $3}&#39; list.txt&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;making-the-sas-statements&#34;&gt;Making the SAS statements&lt;/h2&gt;
&lt;p&gt;Now we know everything we need to know about AWK to create code we want. First we note that
our coding file uses multiple spaces as column separators as opposed to single spaces. If
each item was a single word, this wouldn&amp;rsquo;t be a problem. Unfortunately, our second column
reads &amp;ldquo;OFFENDER NC DOC ID NUMBER&amp;rdquo; which would be split into five columns by default. So we
will need to use the column separator flag as &lt;code&gt;-F &#39;[[:space:]][[:space:]]+&#39;&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-label-statement&#34;&gt;The LABEL Statement&lt;/h3&gt;
&lt;p&gt;A SAS label has the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/v_011/lestmtsref/n1r8ub0jx34xfsn1ppcjfe0u16pc.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;general form&lt;/a&gt;
&lt;code&gt;LABEL variable-1=label-1&amp;lt;...variable-n=label-n&amp;gt;;&lt;/code&gt;, so for example&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;label score1=&amp;quot;Grade on April 1 Test&amp;quot;  
      score2=&amp;quot;Grade on May 1 Test&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is a valid label statement. In our file the variable names are given in column 1
and the appropriate labels in column 2. So an AWK script to print the appropriate
labels can be written like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;awk -F &#39;[[:space:]][[:space:]]+&#39; &#39;{printf &amp;quot;\t%s=\&amp;quot;%s\&amp;quot;\n&amp;quot;, $1, $2}&#39; FILE.DAT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what everything looks like given our code:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;label.PNG&#34; alt=&#34;Sample Code returned by AWK.&#34; title=&#34;Sample Code returned by AWK.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-input-statement&#34;&gt;The INPUT STATEMENT&lt;/h3&gt;
&lt;p&gt;The INPUT statement can be made in a similar way, it just requires some minor tweaking as
INPUT can be a bit more complex to handle a variety of data, see the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lestmtsref/n0oaql83drile0n141pdacojq97s.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;.
In our case we are dealing with a fixed-width record. The fourth column gives the starting column
of the data and the fifth gives us the width of that field. The third gives us the data type.
The majority of ours are character, so it seems easiest to just have the AWK script print each
line as though it were a character together with a SAS comment giving the name and &amp;ldquo;official&amp;rdquo; data
type. Then the few lines that need adjustment can be manually adjusted. The corresponding code would
look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;awk -F &#39;[[:space:]][[:space:]]+&#39; &#39;{printf &amp;quot;\t@%s %s $%s. /*%s - %s*/\n&amp;quot;,$4, $1, $5, $3, $2}&#39; FILE.DAT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what is returned by our code (highlighted part has been manually edited):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;input.PNG&#34; alt=&#34;Sample Code returned by AWK.&#34; title=&#34;Sample Code returned by AWK.&#34;&gt;&lt;/p&gt;
&lt;p&gt;I hope you all find this useful and that it will save you some typing!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SASPy Video Tutorial</title>
      <link>https://dmsenter89.github.io/post/21-06-youtube-tutorial/</link>
      <pubDate>Tue, 29 Jun 2021 10:57:05 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-06-youtube-tutorial/</guid>
      <description>&lt;p&gt;I have been using both SAS and Python extensively for a while now. With each having great features, it was very useful to combine my
skills in both languages by seamlessly moving between SAS and Python in
a single notebook. In the video below, fellow SAS intern Ariel Chien and I show how easy it is to connect the SAS and Python kernels using the open-source SASPy package together with SAS OnDemand for Academics.
I hope you will also find that this adds to your workflow!&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/6mcsbeKwSqM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The Jupyter notebook from the video can be viewed 
&lt;a href=&#34;https://github.com/sascommunities/sas-howto-tutorials/tree/master/sastopython&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;on GitHub&lt;/a&gt;. For installation instructions, check out the 
&lt;a href=&#34;https://github.com/sassoftware/saspy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SASPy GitHub page&lt;/a&gt;. Configuration for SASPy to connect to ODA can be found 
&lt;a href=&#34;https://support.sas.com/ondemand/saspy.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;at this support page&lt;/a&gt;. For more information on SAS OnDemand for Academics, 
&lt;a href=&#34;https://www.sas.com/en_us/software/on-demand-for-academics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Census 2020 Population Estimates Updated</title>
      <link>https://dmsenter89.github.io/post/21-06-covid-county-incidence/</link>
      <pubDate>Wed, 09 Jun 2021 16:00:34 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-06-covid-county-incidence/</guid>
      <description>&lt;p&gt;The Census Bureau has updated its population estimates for 2020 with county level data. This means any
projects that have had to rely on the 2019 estimates can now switch to the 2020 estimates.&lt;/p&gt;
&lt;p&gt;This is particularly useful for those of us who have been trying to track the development of COVID-19. The
average incidence rates are typically rescaled to new cases per 100,000 people. Previous graphs and maps I
have created used the 2019 estimates. I have now updated my code for mapping North Carolina developments to
use the 2020 estimates.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-county-level-data-for-north-carolina-using-the-nyt-covid-data-set-date-set-to-june-8-2021&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-06-covid-county-incidence/nc_avg_incidence_08jun2021_hu8396d2a41a978826522d96cfe881f35d_68326_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;County level data for North Carolina using the NYT COVID data set. Date set to June 8, 2021.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-06-covid-county-incidence/nc_avg_incidence_08jun2021_hu8396d2a41a978826522d96cfe881f35d_68326_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    County level data for North Carolina using the NYT COVID data set. Date set to June 8, 2021.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Below this post is my code for loading the necessary data using SAS.
Note that I&amp;rsquo;m using a macro called &lt;code&gt;mystate&lt;/code&gt; that can be set to the statecode abbreviation of your choice.
The conditional &lt;code&gt;County ne 0&lt;/code&gt; is in the code because the county level CSV includes both the county data as
well as the totals for each state.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;
filename popdat url &#39;https://www2.census.gov/programs-surveys/popest/datasets/2010-2020/counties/totals/co-est2020-alldata.csv&#39;;

data censusdata;
	infile POPDAT delimiter=&#39;,&#39; MISSOVER DSD lrecl=32767 firstobs=2;
	informat SUMLEV REGION DIVISION State County best32.
                         STNAME $20. CTYNAME $35. 
		CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020 best32.;
	format SUMLEV REGION DIVISION STATE best32. COUNTY 5. STNAME $20. CTYNAME $35. 
		CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020 
		COMMA12. StateCode $2.;
	input SUMLEV REGION DIVISION STATE COUNTY STNAME $ CTYNAME $
                        CENSUS2010POP ESTIMATESBASE2010 
		POPESTIMATE2010-POPESTIMATE2020;

	if (State ne 0) and (State ne 72) then
		do;
			FIPS=put(State, Z2.);
			Statecode=fipstate(FIPS);

			if Statecode eq &amp;amp;mystate and County ne 0 then
				output;
		end;
	keep STNAME CTYNAME County FIPS Statecode Popestimate2020;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The media release can be 
&lt;a href=&#34;https://www.census.gov/newsroom/press-releases/2021/2020-vintage-population-estimates.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;viewed here&lt;/a&gt;. The county-level data set can be downloaded 
&lt;a href=&#34;https://www.census.gov/programs-surveys/popest/technical-documentation/research/evaluation-estimates/2020-evaluation-estimates/2010s-counties-total.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;at this page&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Git with SAS Studio</title>
      <link>https://dmsenter89.github.io/post/21-01-git-with-sas-studio/</link>
      <pubDate>Mon, 11 Jan 2021 14:42:50 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/21-01-git-with-sas-studio/</guid>
      <description>&lt;p&gt;Git is a widely used version control system that allows users to track their software
development in both public and private repositories. It is also increasingly used to store
data in text formats, see for example the 
&lt;a href=&#34;https://github.com/nytimes/covid-19-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New York Times COVID-19 data set&lt;/a&gt;.
This post will briefly demonstrate how to clone and pull updates from a GitHub repository
using the git functions that are built into SAS Studio.&lt;/p&gt;
&lt;p&gt;Git functionality has been built into SAS Studio for a little while, so there are actually
two slightly different iterations of the git functions. The examples in this post will use the versions
compatible with SAS Studio 3.8, which is the current version available at SAS OnDemand for Academics.
All git functions use the same prefix. In older versions such as SAS Studio 3.8 the prefix is &lt;code&gt;gitfn_&lt;/code&gt;,
which is followed by a git command such as &amp;ldquo;clone&amp;rdquo; or &amp;ldquo;pull&amp;rdquo;. In SAS Studio 5, the prefix has been
simplified to just &lt;code&gt;git_&lt;/code&gt;. Most git functions have the same name between the&lt;br&gt;
two versions, so that the only difference is the prefix. A complete table of the old and new
versions of the git functions is available 
&lt;a href=&#34;https://go.documentation.sas.com/?cdcId=pgmsascdc&amp;amp;cdcVersion=9.4_3.5&amp;amp;docsetId=lefunctionsref&amp;amp;docsetTarget=n1mlc3f9w9zh9fn13qswiq6hrta0.htm&amp;amp;locale=en#p0evl64wd2dljrn1l43t739qtwba&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in the documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We use the git functions by calling them in an otherwise empty DATA step. In other words, we use the
format&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
    /* use your git functions here */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;cloning-a-repo&#34;&gt;Cloning a Repo&lt;/h2&gt;
&lt;p&gt;To clone a repo from github we use &lt;code&gt;gitfn_clone&lt;/code&gt;. It takes two arguments -
the URL of the repository of interest and the path to an &lt;em&gt;empty&lt;/em&gt; folder. You can
have SAS create the folder for you by using &lt;code&gt;OPTIONS DLCREATEDIR&lt;/code&gt;. The basic
syntax for the clone is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
    rc = gitfn_clone (
     &amp;quot;&amp;amp;repoURL.&amp;quot;,    /* URL to repo */
     &amp;quot;&amp;amp;targetDIR.&amp;quot;); /* folder to put repo in */
    put rc=;         /* equals 0 if successful */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It doesn&amp;rsquo;t matter if the URL you use ends in &amp;ldquo;.git&amp;rdquo; or not. In other words, the
following two macros would both work the same:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;%LET repoURL=https://github.com/nytimes/covid-19-data;
/* works the same as */
%LET repoURL=https://github.com/nytimes/covid-19-data.git;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use password based authentication to pull in private repositories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
    rc = gitfn_clone (
     &amp;quot;&amp;amp;repoURL.&amp;quot;,   
     &amp;quot;&amp;amp;targetDIR.&amp;quot;,
     &amp;quot;&amp;amp;githubUSER.&amp;quot;,   /* your GitHub username */
     &amp;quot;&amp;amp;githubPASSW.&amp;quot;); /* your GitHub password */
    put rc=;         /* equals 0 if successful */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; GitHub is &lt;em&gt;deprecating&lt;/em&gt; 
&lt;a href=&#34;https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;password-based authentication&lt;/a&gt;; you will need to switch to OAuth authentication or SSH keys
if you are not already using them. To access a repository using an SSH key, use the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;                             
 rc = gitfn_clone(
  &amp;quot;&amp;amp;repoURL.&amp;quot;,
  &amp;quot;&amp;amp;targetDIR.&amp;quot;,
  &amp;quot;&amp;amp;sshUSER.&amp;quot;,
  &amp;quot;&amp;amp;sshPASSW.&amp;quot;,
  &amp;quot;&amp;amp;sshPUBkey.&amp;quot;,
  &amp;quot;&amp;amp;sshPRIVkey.&amp;quot;);
 put rc=;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;pull-ing-in-updates&#34;&gt;Pull-ing in Updates&lt;/h2&gt;
&lt;p&gt;It is just as easy to pull in updates to a local repository by using
&lt;code&gt;gitfn_pull(&amp;quot;&amp;amp;repoDIR.&amp;quot;)&lt;/code&gt;. This also works with SSH keys for private
repositories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
 rc = gitfn_pull(
  &amp;quot;&amp;amp;repoDIR.&amp;quot;,
  &amp;quot;&amp;amp;sshUSER.&amp;quot;,
  &amp;quot;&amp;amp;sshPASSW.&amp;quot;,
  &amp;quot;&amp;amp;sshPUBkey.&amp;quot;,
  &amp;quot;&amp;amp;sshPRIVkey.&amp;quot;);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;other-functions&#34;&gt;Other Functions&lt;/h2&gt;
&lt;p&gt;SAS also offers other built-in functions, such as &lt;code&gt;_diff&lt;/code&gt;, &lt;code&gt;_status&lt;/code&gt;, &lt;code&gt;_push&lt;/code&gt;,
&lt;code&gt;_commit&lt;/code&gt;, and others. For a complete list, see the SAS documentation 
&lt;a href=&#34;https://go.documentation.sas.com/?cdcId=pgmsascdc&amp;amp;cdcVersion=9.4_3.5&amp;amp;docsetId=lefunctionsref&amp;amp;docsetTarget=n1mlc3f9w9zh9fn13qswiq6hrta0.htm&amp;amp;locale=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>North Carolina Housing Data</title>
      <link>https://dmsenter89.github.io/post/20-11-north-carolina-housing/</link>
      <pubDate>Fri, 06 Nov 2020 10:10:01 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/20-11-north-carolina-housing/</guid>
      <description>&lt;p&gt;A popular beginners machine learning problem is the prediction of housing prices. A frequently used data set for this purpose uses housing prices in California along some additional  gathered through the 1990 Census. One such data set is available 
&lt;a href=&#34;https://www.kaggle.com/camnugent/california-housing-prices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; at Kaggle. Unfortunately, that data set is rather old. And I live in North Carolina, not California! So I figured I might as well create a new housing data set, but this time with more up-to-date information and using North Carolina as the state to be analyzed. One thing that may be interesting about North Carlina as compared to California is the position of major populations centers. In California, major population centers are near the beach, while major population centers in North Carolina are in the interior of the state. Both large citites and proximity to the beach tend to correlate with higher housing prices. In California, unlike in North Carolina, both of these go together.&lt;/p&gt;
&lt;p&gt;This post will describe the Kaggle data set with California housing prices and then walk you through how the relevant data can be acquired from the Census Bureau. I&amp;rsquo;ll also show how to clean the data. For those who just want to explore the complete data set, I have made it available for download &lt;a href=&#34;https://dmsenter89.github.io/files/NC_Housing_Prices_2018.csv&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#the-source-data-set&#34;&gt;The Source Data Set&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acquiring-the-census-data-set&#34;&gt;Acquiring the Census Data Set&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#census-variables&#34;&gt;Census Variables&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#geography-considerations&#34;&gt;Geography Considerations&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acquiring-location-data&#34;&gt;Acquiring Location Data&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-merge-with-geoid-matching&#34;&gt;Data Merge with GEOID Matching&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-cleaning&#34;&gt;Data Cleaning&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;the-source-data-set&#34;&gt;The Source Data Set&lt;/h2&gt;
&lt;p&gt;The geographic unit of the Kaggle data set is the Census block group, which means we will have several thousand data points for our analysis. For a good big-picture overview of Census geography divisions, see this 
&lt;a href=&#34;https://pitt.libguides.com/uscensus/understandinggeography&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt; from the University of Pittsburgh library. The data set&amp;rsquo;s ten columns contain geographic, housing, and Census information that can be broken down as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;geographic information
&lt;ul&gt;
&lt;li&gt;longitude&lt;/li&gt;
&lt;li&gt;latitude&lt;/li&gt;
&lt;li&gt;ocean proximity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;housing information
&lt;ul&gt;
&lt;li&gt;median age of homes&lt;/li&gt;
&lt;li&gt;median value of homes&lt;/li&gt;
&lt;li&gt;total number of rooms in area&lt;/li&gt;
&lt;li&gt;total number of bedrooms in the area&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Census information
&lt;ul&gt;
&lt;li&gt;population&lt;/li&gt;
&lt;li&gt;number of households&lt;/li&gt;
&lt;li&gt;median income&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most of these exist directly in the Census API data that we have 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;covered previously&lt;/a&gt;. The ocean proximity variable is a categorical giving approximate distance from the beach. My data set will not include this last categorical variable.&lt;/p&gt;
&lt;h2 id=&#34;acquiring-the-census-data-set&#34;&gt;Acquiring the Census Data Set&lt;/h2&gt;
&lt;h3 id=&#34;census-variables&#34;&gt;Census Variables&lt;/h3&gt;
&lt;p&gt;The first, and most time consuming aspect, is to figure out where the data we want is located. We know that the US has a decennial census, so accurate information is available every ten years at every level of geography that the Census Bureau tracks. Since it is currently a census year 2020 and the newest information hasn&amp;rsquo;t been tabulated yet, that means the last census count that is available is from 2010. While this is 20 years more current than the California set from 1990, it still seems a bit outdated. Luckily, since the introduction of the American Community Survey (ACS) we have annually updated information available - but not for every level of geography. Only the 5-year ACS average gives us census block-level information for the whole state, making it comparable to the Kaggle data set. The most recent of these is the 2018.&lt;/p&gt;
&lt;p&gt;I start by creating a data dictionary from the 
&lt;a href=&#34;https://api.census.gov/data/2018/acs/acs5/groups.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;groups&lt;/a&gt; and 
&lt;a href=&#34;https://api.census.gov/data/2018/acs/acs5/variables.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;variables&lt;/a&gt; pages of the &amp;ldquo;American Community Survey: 1-Year Estimates: Detailed Tables 5-Year&amp;rdquo; data set. Note that median home age is not directly available. Instead, we will use the median year structures were built to calculate the median home age. Our data dictionary also does not include any data for the longitude and latitude of each row. We will get that data separately.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dictionary = {
    &#39;B01001_001E&#39; : &amp;quot;population&amp;quot;,
    &#39;B11001_001E&#39; : &amp;quot;households&amp;quot;,
    &#39;B19013_001E&#39; : &amp;quot;median_income&amp;quot;, 
    &#39;B25077_001E&#39; : &amp;quot;median_house_value&amp;quot;,
    &#39;B25035_001E&#39; : &amp;quot;median_year_structure_built&amp;quot;,
    &#39;B25041_001E&#39; : &amp;quot;total_bedrooms&amp;quot;,
    &#39;B25017_001E&#39; : &amp;quot;total_rooms&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;geography-considerations&#34;&gt;Geography Considerations&lt;/h3&gt;
&lt;p&gt;The next step is figuring out exactly what level of geography we want. Our data set goes down to the Census block level at its most granular. Unfortunately, the Census API won&amp;rsquo;t let us pull the data for all the Census blocks in a state at once. Census tracts on the other hand can be acquired in one go. If we were to shortcut and use only tract data, this would be a pretty quick API call build:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;primary_geo = &amp;quot;tract:*&amp;quot;
secondary_geo = &amp;quot;state:37&amp;quot;
query = base_URL + &amp;quot;?get=&amp;quot; + &amp;quot;,&amp;quot;.join(data_dictionary.keys()) + f&amp;quot;&amp;amp;for={primary_geo}&amp;amp;in={secondary_geo}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But let&amp;rsquo;s try and do it for the Census blocks instead. This will require us to build a sequence of API calls that loops over a larger geographic area, say the different counties in the state, and pull in the respective census block data for that geographic unit. While the FIPS codes for the state counties are sorted alphabetically, they are not contiguous. A full listing of North Carolina county FIPS codes is availalbe 
&lt;a href=&#34;https://www.lib.ncsu.edu/gis/countyfips&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;from NCSU here&lt;/a&gt;. It appears to be that the county FIPS codes are three digits long, starting at &lt;code&gt;001&lt;/code&gt; and go up to &lt;code&gt;199&lt;/code&gt; in increments of 2, meaning only odd numbers are in the county set. So it looks like we will be using &lt;code&gt;range(1,200,2)&lt;/code&gt; with zero-padding to create the list of county FIPS codes. So we could use a loop similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vars_requested = &amp;quot;,&amp;quot;.join(data_dictionary.keys())

for i in range(1,200,2):
    geo_request = f&amp;quot;for=block%20group:*&amp;amp;in=state:37%20county:{i:03}&amp;quot;
    query = base_URL + f&amp;quot;?get={vars_requested}&amp;amp;{geo_request}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While practicing to write the appropriate API call, you may find it useful to give it frequent, quick tests using curl. If you are using Jupyter or IPython, you can use &lt;code&gt;!curl &amp;quot;{query}&amp;quot;&lt;/code&gt; to test your API query. Don&amp;rsquo;t forget the quotation marks, since the ampersand has special meaning in the shell. It may be helpful to test the output of your call at the county or city level with that reported on the 
&lt;a href=&#34;https://www.census.gov/quickfacts/fact/table/US/PST045219&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Census Quickfacts page&lt;/a&gt;, if your variable is listed there. This can help make sure you are pulling the data you actually want.&lt;/p&gt;
&lt;p&gt;Now that we have figured out the loop necessary for creation of the API calls, we can put everything together and create a list of Pandas DataFrames which we then concatenate to create our master list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import requests

# create the base-URL
host_name = &amp;quot;https://api.census.gov/data&amp;quot;
year = &amp;quot;2018&amp;quot;
dataset_name = &amp;quot;acs/acs5&amp;quot;
base_URL = f&amp;quot;{host_name}/{year}/{dataset_name}&amp;quot;

# build the api calls as a list
query_vars = base_URL + &amp;quot;?get=&amp;quot; + &amp;quot;,&amp;quot;.join(list(data_dictionary.keys()) + [&amp;quot;NAME&amp;quot;,&amp;quot;GEO_ID&amp;quot;])
api_calls = [query_vars + f&amp;quot;&amp;amp;for=block%20group:*&amp;amp;in=state:37%20county:{i:03}&amp;quot; for i in range(1,200,2) ]

# running the API calls will take a moment
rjson_list = [requests.get(call).json() for call in api_calls]

# create the data frame by concatenation
df_list = [pd.DataFrame(data[1:], columns=data[0]) for data in rjson_list]
df = pd.concat(df_list, ignore_index=True)

# save the raw output to disk
df.to_csv(&amp;quot;raw_census.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we have the data set! We do still have to address the issue of our values all being imported as strings as mentioned in my 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;Census API post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;acquiring-location-data&#34;&gt;Acquiring Location Data&lt;/h2&gt;
&lt;p&gt;As mentioned above, we are still missing information regarding the latitude and longitude of the different block groups. The Census Bureau makes a lot of geographically coded data available on its 
&lt;a href=&#34;https://tigerweb.geo.census.gov/tigerwebmain/TIGERweb_main.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TIGERweb&lt;/a&gt; page. You can interact with it both using a REST API and its web-interface. A page with map information exists 
&lt;a href=&#34;https://tigerweb.geo.census.gov/arcgis/rest/services/Generalized_ACS2018/Tracts_Blocks/MapServer/4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Dealing with shapefiles and the TIGERweb API can get a little complicated. Luckily, I know someone with expertise in GIS and shapefiles so we will be using a CSV file of the geographic data we need courtesy of 
&lt;a href=&#34;https://www.linkedin.com/in/summer-faircloth-652797137&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Summer Faircloth&lt;/a&gt;, a GIS intern at the North Carolina Department of Transportation. She downloaded the TIGER/Line Shapefiles for the 20189 ACS 
&lt;a href=&#34;https://catalog.data.gov/dataset/tiger-line-shapefile-2018-state-north-carolina-current-block-group-state-based&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Block Groups&lt;/a&gt; and 
&lt;a href=&#34;https://catalog.data.gov/dataset/tiger-line-shapefile-2018-state-north-carolina-current-census-tract-state-based&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Census Tracts&lt;/a&gt; and joined the data sets in ArcMap, from where she exported our CSV file, which is now &lt;a href=&#34;https://dmsenter89.github.io/files/BlockGroup_Tract2018.csv&#34; target=&#34;_blank&#34;&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t need all of the columns in the CSV file, so we will limit the import to the parts we need with the &lt;code&gt;usecols&lt;/code&gt; keyword.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&amp;quot;raw_census.csv&amp;quot;, dtype={})
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;shapedata = pd.read_csv(&amp;quot;BlockGroup_Tract2018.csv&amp;quot;, 
                        dtype={&amp;quot;GEOID&amp;quot;: str},
                        usecols=[&#39;GEOID&#39;,&#39;NAMELSAD&#39;,&#39;INTPTLAT&#39;,&#39;INTPTLON&#39;,&#39;NAMELSAD_1&#39;] )

shapedata = shapedata.rename(columns={&#39;INTPTLAT&#39; : &#39;latitude&#39;, &#39;INTPTLON&#39; : &#39;longitude&#39; })
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-merge-with-geoid-matching&#34;&gt;Data Merge with GEOID Matching&lt;/h2&gt;
&lt;p&gt;At this stage we have two data frames - the first consists of all the Census information sans the geographic coordinates of the block groups, and a second data set containing the block groups&#39; location. Both data sets contain a GEOID column that can be used for merging. The GEOID returned by the Census API includes additional information to the regular FIPS code based GEOID used in the TIGERweb system. For example, &amp;ldquo;1500000US370010204005&amp;rdquo; in the census data set is actually GEOID &amp;ldquo;370010204005&amp;rdquo; for purposes of the TIGERweb data set. We&amp;rsquo;ll use a string split to make our GEO_ID variable from the Census API compatible with the FIPS code based GEOID from the TIGERweb service.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;GEO_ID&amp;quot;] = df[&amp;quot;GEO_ID&amp;quot;].str.split(&#39;US&#39;).str[1]

df = df.merge(shapedata, left_on=&#39;GEO_ID&#39;, right_on=&amp;quot;GEOID&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-cleaning&#34;&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;Now that our data set has been assembled, we can work on cleaning up the merged data set. We have the following tasks left:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;convert column data types to numeric&lt;/li&gt;
&lt;li&gt;drop unnecessary columns&lt;/li&gt;
&lt;li&gt;rename columns&lt;/li&gt;
&lt;li&gt;handle missing values&lt;/li&gt;
&lt;li&gt;calculate median age of homes&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for col in data_dictionary.keys():
    if col not in [&amp;quot;NAME&amp;quot;, &amp;quot;GEO_ID&amp;quot;]:
        df[col] = pd.to_numeric(df[col])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To indicate missing values, the Census API returns a value of &amp;ldquo;-666666666&amp;rdquo; in numeric columns. As all of our variables - except for longitude - ought to be positive, we can use the &lt;code&gt;mask&lt;/code&gt; function to convert all negative values to missing. We&amp;rsquo;ll start by filtering out the string columns that are no longer necessary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# filter down to our numerical columns
keeps = list(data_dictionary.keys()) +[&amp;quot;latitude&amp;quot;, &amp;quot;longitude&amp;quot;]
df = df.filter(items=keeps)

# replace vals &amp;lt; 0 with missing
k = df.loc[:, df.columns != &#39;longitude&#39;]
k = k.mask(k &amp;lt; 0)
df.loc[:, df.columns != &#39;longitude&#39;] = k
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the missing values have been handled, we can go ahead and calculate our median home age.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.rename(columns=data_dictionary, inplace=True)
df[&amp;quot;housing_median_age&amp;quot;] = 2018 - df[&amp;quot;median_year_structure_built&amp;quot;]
df.drop(columns=&amp;quot;median_year_structure_built&amp;quot;, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we&amp;rsquo;re done! We will save our output data set to disk for future analysis in a different post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.to_csv(&amp;quot;NC_Housing_Prices_2018.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Teacher Salaries</title>
      <link>https://dmsenter89.github.io/post/20-10-tabula/</link>
      <pubDate>Thu, 29 Oct 2020 22:39:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/20-10-tabula/</guid>
      <description>&lt;p&gt;After reading a news article about teacher pay in the US, I was curious and wanted to look into the source data myself. Unfortunately, the source that was mentioned was a publication by the 
&lt;a href=&#34;https://www.nea.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Education Association (NEA)&lt;/a&gt; which had the data as tables embedded inside a PDF report. As those who know me can attest, I don&amp;rsquo;t like hand-copying data. It is slow and error-prone. Instead, I decided to use the tabula package to extract the information from the PDFs directly into a Pandas dataframe. In this post, I will show you how to extract the data and how to clean it up for analysis.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#the-data-source&#34;&gt;The Data Source&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#loading-the-data&#34;&gt;Loading the Data&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#cleaning-the-data&#34;&gt;Cleaning the Data&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#numeric-conversion&#34;&gt;Numeric Conversion&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#table-b-6&#34;&gt;Table B-6&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;the-data-source&#34;&gt;The Data Source&lt;/h2&gt;
&lt;p&gt;Several years worth of data are available in PDF form on the 
&lt;a href=&#34;https://www.nea.org/research-publications&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NEA website&lt;/a&gt;. Reading through the technical notes, they highlight that they did not collect all of their own salary information. Some states&#39; information is calculated from the American Community Survey (ACS) done by the Census Bureau - a great resource whose API I have covered in a 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;different post&lt;/a&gt;. Each report includes accurate data for the previous school year, as well as estimates for the current school year. As of this post, the newest report is the 2020 report which includes data for the the 2018-2019 school year, as well as estimates of the 2019-2020 school year.&lt;/p&gt;
&lt;p&gt;The 2020 report has the desired teacher salary information in two separate locations. One is in table B-6 on page 26 of the PDF, which shows a ranking of the different states&#39; average salary in addition to the average salary:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table_B-6.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;A second location is in table E-7 on page 46, which gives salary data for the completed school year as well as different states&#39; estimates for the 2019-2020 school year:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table_E-7.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Note that table E-7 lacks the star-annotation marking NEA estimated values. This, and the lack of the ranking column, makes Table E-7 easier to parse. In the main example below, this will be the source of the five years of data. I will however also show how to parse table B-6 at the end of this post for completion.&lt;/p&gt;
&lt;h2 id=&#34;loading-the-data&#34;&gt;Loading the Data&lt;/h2&gt;
&lt;p&gt;As of October 2020, the NEA site has five years worth of reports online. Unfortunately, these are not labeled consistently for all five years. Similarly the page numbers differ for each report. Prior to the 2018 report, inconsistent formats were used for the tables which require previous years to be parsed separately from the newer tables. For this reason, I&amp;rsquo;ll make a dictionary for the 2018-2020 reports only, which will simplify the example below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;report = {
    &#39;2020&#39; : {
        &#39;url&#39;  : &amp;quot;https://www.nea.org/sites/default/files/2020-10/2020%20Rankings%20and%20Estimates%20Report.pdf&amp;quot;,
        &#39;page&#39; : 46,
    },
    &#39;2019&#39; : {
        &#39;url&#39;  : &amp;quot;https://www.nea.org/sites/default/files/2020-06/2019%20Rankings%20and%20Estimates%20Report.pdf&amp;quot;,
        &#39;page&#39; : 49,
    },
    &#39;2018&#39; : {
        &#39;url&#39;  : &amp;quot;https://www.nea.org/sites/default/files/2020-07/180413-Rankings_And_Estimates_Report_2018.pdf&amp;quot;,
        &#39;page&#39; : 51,
    },
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now use dictionary comprehension to fill in a dictionary with all the source tables of interest. We will be using the tabula package to extract data from the PDFs. If you don&amp;rsquo;t have it installed, you can use &lt;code&gt;pip install tabula-py&lt;/code&gt; to get a copy. The method that reads in a PDF is aptly called &lt;code&gt;read_pdf&lt;/code&gt;. Its first argument is a file path to the PDF. Since we want to use a URL, we will use the keyword argument &lt;code&gt;stream=True&lt;/code&gt; and then name the specific page in each PDF that contains the information we are after. By default, &lt;code&gt;read_pdf&lt;/code&gt; returns a list of dataframes, so we just save the first element from the list, which is the report we are interested in.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; if you are using WSL, depending on your settings, you may get the error &lt;code&gt;Exception in thread &amp;quot;main&amp;quot; java.awt.AWTError: Can&#39;t connect to X11 window server using &#39;XXX.XXX.XXX.XXX:0&#39; as the value of the DISPLAY variable.&lt;/code&gt; error when running &lt;code&gt;read_pdf&lt;/code&gt;. This is fixed by having an X11 server running.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tabula
import pandas as pd

source_df = {year : tabula.read_pdf(report[year][&#39;url&#39;], stream=True, pages=report[year][&#39;page&#39;])[0] 
             for year in report.keys()}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it in principle. How cool is that! Of course, we still need to clean our data a little bit.&lt;/p&gt;
&lt;h3 id=&#34;cleaning-the-data&#34;&gt;Cleaning the Data&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at the first and last few entries of the 2020 report:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.concat([source_df[&#39;2020&#39;].head(), 
           source_df[&#39;2020&#39;].tail()])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;2018-19&lt;/th&gt;
      &lt;th&gt;2019-20&lt;/th&gt;
      &lt;th&gt;From 2018-19 to 2019-20&lt;/th&gt;
      &lt;th&gt;From 2010-11 to 2019-20 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;State&lt;/td&gt;
      &lt;td&gt;Salary($)&lt;/td&gt;
      &lt;td&gt;Salary($)&lt;/td&gt;
      &lt;td&gt;Change(%)&lt;/td&gt;
      &lt;td&gt;Current Dollar Constant Dollar&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
      &lt;td&gt;54,095&lt;/td&gt;
      &lt;td&gt;4.01&lt;/td&gt;
      &lt;td&gt;13.16 -2.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
      &lt;td&gt;70,877&lt;/td&gt;
      &lt;td&gt;0.85&lt;/td&gt;
      &lt;td&gt;15.36 -0.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
      &lt;td&gt;50,381&lt;/td&gt;
      &lt;td&gt;0.06&lt;/td&gt;
      &lt;td&gt;8.03 -7.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
      &lt;td&gt;49,822&lt;/td&gt;
      &lt;td&gt;0.78&lt;/td&gt;
      &lt;td&gt;8.31 -6.75&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;48&lt;/th&gt;
      &lt;td&gt;Washington&lt;/td&gt;
      &lt;td&gt;73,049&lt;/td&gt;
      &lt;td&gt;72,965&lt;/td&gt;
      &lt;td&gt;-0.11&lt;/td&gt;
      &lt;td&gt;37.86 18.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;49&lt;/th&gt;
      &lt;td&gt;West Virginia&lt;/td&gt;
      &lt;td&gt;47,681&lt;/td&gt;
      &lt;td&gt;50,238&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;13.51 -2.28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50&lt;/th&gt;
      &lt;td&gt;Wisconsin&lt;/td&gt;
      &lt;td&gt;58,277&lt;/td&gt;
      &lt;td&gt;59,176&lt;/td&gt;
      &lt;td&gt;1.54&lt;/td&gt;
      &lt;td&gt;9.17 -6.02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;51&lt;/th&gt;
      &lt;td&gt;Wyoming&lt;/td&gt;
      &lt;td&gt;58,861&lt;/td&gt;
      &lt;td&gt;59,014&lt;/td&gt;
      &lt;td&gt;0.26&lt;/td&gt;
      &lt;td&gt;5.19 -9.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;52&lt;/th&gt;
      &lt;td&gt;United States&lt;/td&gt;
      &lt;td&gt;62,304&lt;/td&gt;
      &lt;td&gt;63,645&lt;/td&gt;
      &lt;td&gt;2.15&lt;/td&gt;
      &lt;td&gt;14.14 -1.73&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We see that each column is treated as a string object (which you can confirm by running &lt;code&gt;source_df[&#39;2020&#39;].dtypes&lt;/code&gt;) and that the first row of data is actually at index 1 due to the fact that the PDF report used a two-row header. This means we can safely drop the first row of every dataframe. We can also drop the last row of every dataframe since that just contains summary data of the US as a whole, which we can easily regenerate as necessary. So row indices 0 and 52 can go for all of our data sets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for df in source_df.values():
    df.drop([0, 52], inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next up I&amp;rsquo;d like to fix the column names. The fist column is clearly the name of the state (except in the case of Washington D.C.), while the next two columns give the years for which the salary information is given. Let&amp;rsquo;s rename the second and third columns according to the pattern &lt;code&gt;Salary %YYYY-YY&lt;/code&gt; using Python&amp;rsquo;s f-string syntax.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for df in source_df.values():
    df.rename(columns={
        df.columns[0] : &amp;quot;State&amp;quot;,
        df.columns[1] : f&amp;quot;Salary {str(df.columns[1])}&amp;quot;,
        df.columns[2] : f&amp;quot;Salary {str(df.columns[2])}&amp;quot;,
    }, 
              inplace=True)
    
source_df[&amp;quot;2020&amp;quot;].head()  # show the result of our edits so far
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Salary 2018-19&lt;/th&gt;
      &lt;th&gt;Salary 2019-20&lt;/th&gt;
      &lt;th&gt;From 2018-19 to 2019-20&lt;/th&gt;
      &lt;th&gt;From 2010-11 to 2019-20 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
      &lt;td&gt;54,095&lt;/td&gt;
      &lt;td&gt;4.01&lt;/td&gt;
      &lt;td&gt;13.16 -2.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
      &lt;td&gt;70,877&lt;/td&gt;
      &lt;td&gt;0.85&lt;/td&gt;
      &lt;td&gt;15.36 -0.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
      &lt;td&gt;50,381&lt;/td&gt;
      &lt;td&gt;0.06&lt;/td&gt;
      &lt;td&gt;8.03 -7.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
      &lt;td&gt;49,822&lt;/td&gt;
      &lt;td&gt;0.78&lt;/td&gt;
      &lt;td&gt;8.31 -6.75&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;83,059&lt;/td&gt;
      &lt;td&gt;84,659&lt;/td&gt;
      &lt;td&gt;1.93&lt;/td&gt;
      &lt;td&gt;24.74 7.39&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Looks like we&amp;rsquo;re almost done! Let&amp;rsquo;s drop the unnecessary columns and check our remaining column names:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for year, df in source_df.items():
    df.drop(df.columns[3:], axis=1, inplace=True)
    print(f&amp;quot;{year}:\t{df.columns}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020:	Index([&#39;State&#39;, &#39;Salary 2018-19&#39;, &#39;Salary 2019-20&#39;], dtype=&#39;object&#39;)
2019:	Index([&#39;State&#39;, &#39;Salary 2017-18&#39;, &#39;Salary 2018-19&#39;], dtype=&#39;object&#39;)
2018:	Index([&#39;State&#39;, &#39;Salary 2017&#39;, &#39;Salary 2018&#39;], dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the column naming scheme in 2018 was different than in the previous reports. To make them all compatible for our merge, we&amp;rsquo;re going to have to do some more editing. Based on the other reports, it appears as though the 2018 report used the calendar year of the &lt;em&gt;end&lt;/em&gt; of the school year, while the others utilized a range. This can easily be solved using regex substitution. We&amp;rsquo;ll do that now.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import re

for year, df in source_df.items():
    if year != &amp;quot;2018&amp;quot;:
        df.rename(columns={
            df.columns[1] : re.sub(r&amp;quot;\d{2}-&amp;quot;, &#39;&#39;, df.columns[1]),
            df.columns[2] : re.sub(r&amp;quot;\d{2}-&amp;quot;, &#39;&#39;, df.columns[2]),
        }, 
                  inplace=True)
    # print the output for verification
    print(f&amp;quot;{year}:\t{df.columns}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020:	Index([&#39;State&#39;, &#39;Salary 2019&#39;, &#39;Salary 2020&#39;], dtype=&#39;object&#39;)
2019:	Index([&#39;State&#39;, &#39;Salary 2018&#39;, &#39;Salary 2019&#39;], dtype=&#39;object&#39;)
2018:	Index([&#39;State&#39;, &#39;Salary 2017&#39;, &#39;Salary 2018&#39;], dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that everything works, we can do our merge to create a single dataframe with the information for all of the school years we have downloaded.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge_df = source_df[&amp;quot;2018&amp;quot;].drop([&amp;quot;Salary 2018&amp;quot;], axis=1).merge(
                    source_df[&amp;quot;2019&amp;quot;].drop([&amp;quot;Salary 2019&amp;quot;], axis=1)).merge(
                    source_df[&amp;quot;2020&amp;quot;])

merge_df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Salary 2017&lt;/th&gt;
      &lt;th&gt;Salary 2018&lt;/th&gt;
      &lt;th&gt;Salary 2019&lt;/th&gt;
      &lt;th&gt;Salary 2020&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;50,391&lt;/td&gt;
      &lt;td&gt;50,568&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
      &lt;td&gt;54,095&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;6 8,138&lt;/td&gt;
      &lt;td&gt;69,682&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
      &lt;td&gt;70,877&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;4 7,403&lt;/td&gt;
      &lt;td&gt;48,723&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
      &lt;td&gt;50,381&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;4 8,304&lt;/td&gt;
      &lt;td&gt;50,544&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
      &lt;td&gt;49,822&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;7 9,128&lt;/td&gt;
      &lt;td&gt;80,680&lt;/td&gt;
      &lt;td&gt;83,059&lt;/td&gt;
      &lt;td&gt;84,659&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;numeric-conversion&#34;&gt;Numeric Conversion&lt;/h3&gt;
&lt;p&gt;We&amp;rsquo;re almost done! Notice that we still have not dealt with the fact that every column is still treated as a string. Before we can use the &lt;code&gt;to_numeric&lt;/code&gt; function, we still need to take care of two issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The commas in the numbers. While they are nice for our human eyes, Pandas doesn&amp;rsquo;t like them.&lt;/li&gt;
&lt;li&gt;In the 2017 salary column, there appears to be extraneous white space after the first digit for some entries.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Luckily, both of these problems can be remedied with a simple string replacement operation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge_df.iloc[:,1:] = merge_df.iloc[:,1:].replace(r&amp;quot;[,| ]&amp;quot;, &#39;&#39;, regex=True)

for col in merge_df.columns[1:]:
    merge_df[col] = pd.to_numeric(merge_df[col])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we&amp;rsquo;re done! We have created an overview of annual teacher salaries from the 2016-17 school year until 2019-20 extracted from a series of PDFs published by the NEA. We have cleaned up the data and converted everything to numerical values. We can now get summary statistics and do any analysis of interest with this data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge_df.describe() # summary stats of our numeric columns
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Salary 2017&lt;/th&gt;
      &lt;th&gt;Salary 2018&lt;/th&gt;
      &lt;th&gt;Salary 2019&lt;/th&gt;
      &lt;th&gt;Salary 2020&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;56536.196078&lt;/td&gt;
      &lt;td&gt;57313.039216&lt;/td&gt;
      &lt;td&gt;58983.254902&lt;/td&gt;
      &lt;td&gt;60170.647059&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;9569.444674&lt;/td&gt;
      &lt;td&gt;9795.914601&lt;/td&gt;
      &lt;td&gt;10286.843230&lt;/td&gt;
      &lt;td&gt;10410.259274&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;42925.000000&lt;/td&gt;
      &lt;td&gt;44926.000000&lt;/td&gt;
      &lt;td&gt;45105.000000&lt;/td&gt;
      &lt;td&gt;45192.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;49985.000000&lt;/td&gt;
      &lt;td&gt;50451.500000&lt;/td&gt;
      &lt;td&gt;51100.500000&lt;/td&gt;
      &lt;td&gt;52441.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;54308.000000&lt;/td&gt;
      &lt;td&gt;53815.000000&lt;/td&gt;
      &lt;td&gt;54935.000000&lt;/td&gt;
      &lt;td&gt;57091.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;61038.000000&lt;/td&gt;
      &lt;td&gt;61853.000000&lt;/td&gt;
      &lt;td&gt;64393.500000&lt;/td&gt;
      &lt;td&gt;66366.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;81902.000000&lt;/td&gt;
      &lt;td&gt;84227.000000&lt;/td&gt;
      &lt;td&gt;85889.000000&lt;/td&gt;
      &lt;td&gt;87543.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;table-b-6&#34;&gt;Table B-6&lt;/h2&gt;
&lt;p&gt;As mentioned above, table B-6 in the 2020 Report presents slightly greater challenges. A lot of the cleaning is similar or identical, so I will not reproduce it in full. Instead, I have loaded a subsetted part of table B-6 and will show how this can be cleaned up as well. But first, let&amp;rsquo;s look at the first several entries:&lt;/p&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;2017-18 (Revised)&lt;/th&gt;
      &lt;th&gt;2018-19&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;State&lt;/td&gt;
      &lt;td&gt;Salary($) Rank&lt;/td&gt;
      &lt;td&gt;Salary($)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;50,568 36&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;69,682 7&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;48,315 45&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,096 44&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;80,680 2&lt;/td&gt;
      &lt;td&gt;83,059 *&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Colorado&lt;/td&gt;
      &lt;td&gt;52,695 32&lt;/td&gt;
      &lt;td&gt;54,935&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Connecticut&lt;/td&gt;
      &lt;td&gt;74,517 * 5&lt;/td&gt;
      &lt;td&gt;76,465 *&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Delaware&lt;/td&gt;
      &lt;td&gt;62,422 13&lt;/td&gt;
      &lt;td&gt;63,662&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can see that there is an additional hurdle compared to the previous tables: the second column now contains data from two columns, both the Salary information as well as a ranking of the salary as it compares to the different states. For a few states, there is additionally a &amp;lsquo;*&amp;rsquo; to denote values that were estimated as opposed to received. We can again use a simple regex replace together with a capture group to parse out only those values that we are interested in, while dropping the extraneous information using the code below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;b6.iloc[:,1:] = b6.iloc[:,1:].replace(r&amp;quot;([\d,]+).*&amp;quot;, r&amp;quot;\1&amp;quot;, regex=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we&amp;rsquo;re back to where we were above before we did the string conversion. This is what it looks like after also dropping the first row and renaming the columns:&lt;/p&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Salary 2018&lt;/th&gt;
      &lt;th&gt;Salary 2019&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;50,568&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;69,682&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;48,315&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,096&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;80,680&lt;/td&gt;
      &lt;td&gt;83,059&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Colorado&lt;/td&gt;
      &lt;td&gt;52,695&lt;/td&gt;
      &lt;td&gt;54,935&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Connecticut&lt;/td&gt;
      &lt;td&gt;74,517&lt;/td&gt;
      &lt;td&gt;76,465&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Delaware&lt;/td&gt;
      &lt;td&gt;62,422&lt;/td&gt;
      &lt;td&gt;63,662&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From here on out, we can proceed as in the previous example.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accessing Census Data via API</title>
      <link>https://dmsenter89.github.io/post/20-08-census-api/</link>
      <pubDate>Sat, 22 Aug 2020 08:53:55 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/20-08-census-api/</guid>
      <description>&lt;p&gt;The Census Bureau makes an incredible amount of data available online. In this post, I will summarize how to get access to this data via Python by using the Census Bureau&amp;rsquo;s API. The Census Bureau makes a pretty useful guide available 
&lt;a href=&#34;https://www.census.gov/data/developers/guidance/api-user-guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; - I recommend checking it out.&lt;/p&gt;
 &lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#api-basics&#34;&gt;API Basics&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#building-an-the-base-url&#34;&gt;Building an the Base URL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#building-the-query&#34;&gt;Building the Query&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#the-get-variables&#34;&gt;The &amp;lsquo;Get&amp;rsquo; Variables&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#location-variables&#34;&gt;Location Variables&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-complete-call&#34;&gt;The Complete Call&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#making-the-api-request&#34;&gt;Making the API Request&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#reading-the-json-into-pandas&#34;&gt;Reading the JSON into Pandas&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;api-basics&#34;&gt;API Basics&lt;/h2&gt;
&lt;p&gt;We can think of an API query of consisting of two main parts: a &lt;em&gt;base URL&lt;/em&gt; (also called a root URL) and a &lt;em&gt;query&lt;/em&gt; string. These two strings are joined together with the query character &amp;ldquo;?&amp;rdquo; to create an API call. The resulting API call can in theory be copy-and-pasted into the URL bar of your browser, and I recommend this when first playing around with a new API. Seeing the raw text returned in the browser can help you understand the structure of what is being returned. In the case of the Census Bureau&amp;rsquo;s API, it returns a string that essentially looks like a list of lists from a Python perspective. This can easily be turned into a Pandas dataset. Be aware that all values are returned as strings. You&amp;rsquo;ll have to convert number columns to numeric by yourself.&lt;/p&gt;
&lt;p&gt;To get an overview of all available data sets, you can go to the 
&lt;a href=&#34;https://api.census.gov/data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;data page&lt;/a&gt; which contains a long list of data sets. This data page is incredibly useful because it gives access to all of the information needed to build a correct API call, including the base URLs of all data sets and the variables available in each.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-a-snapshot-of-two-datasets-available-as-part-of-the-2018-american-community-survey-acs&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/20-08-census-api/census_overview_hu401560307af60bd2eb326d2765e64aa0_85308_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;A snapshot of two datasets available as part of the 2018 American Community Survey (ACS).&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/20-08-census-api/census_overview_hu401560307af60bd2eb326d2765e64aa0_85308_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1805&#34; height=&#34;372&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    A snapshot of two datasets available as part of the 2018 American Community Survey (ACS).
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;building-an-the-base-url&#34;&gt;Building an the Base URL&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s build a sample API call for the 2018 American Community Survey 1-Year Detailed Table. While we could just copy the base URL from the data page, I like to assemble mine manually from its component parts. This makes it easier to write a wrapper for the API calls if you plan on scraping the same data from multiple years.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;host_name = &amp;quot;https://api.census.gov/data&amp;quot;
year = &amp;quot;2018&amp;quot;
dataset_name = &amp;quot;acs/acs1&amp;quot;
base_URL = f&amp;quot;{host_name}/{year}/{dataset_name}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;building-the-query&#34;&gt;Building the Query&lt;/h2&gt;
&lt;p&gt;Now that we have the base URL, we can work on building the query. For purposes of the Census Bureau, you will need two components: the variables of interest, which are listed after the &lt;code&gt;get=&lt;/code&gt; keyword, and the geography for which you would like the data listed after the &lt;code&gt;for=&lt;/code&gt; keyword. For certain subdivisions, like counties, you can specify two levels of geography by adding an &lt;code&gt;in=&lt;/code&gt; keyword at at the end.&lt;/p&gt;
&lt;h3 id=&#34;the-get-variables&#34;&gt;The &amp;lsquo;Get&amp;rsquo; Variables&lt;/h3&gt;
&lt;p&gt;Since many of the data sets have a large amount of variables in them, it often makes sense to take a look at the &amp;ldquo;groups&amp;rdquo; page first. This page lists variables as groups, giving you a better overview of what data is available. This page is available at &lt;code&gt;{base_URL}/groups.html&lt;/code&gt;. A complete list of all variables in the data set is available at &lt;code&gt;{base_URL}/variables.html&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s find some variables. The most basic variable we&amp;rsquo;d expect to find here is total population. We can find this variable in group &amp;ldquo;B01003&amp;rdquo;. The total estimate is in sub-variable &amp;ldquo;001E&amp;rdquo;, meaning that the variable for total population is &amp;ldquo;B01003_001E&amp;rdquo;. Let&amp;rsquo;s also get household income (group &amp;ldquo;B19001&amp;rdquo;) not broken down by race: &amp;ldquo;B19001_001E&amp;rdquo;. There is also median monthly housing cost (group B25105) with variable &amp;ldquo;B25105_001E&amp;rdquo;. Since the variable names can be a little difficult to parse, I recommend making a data dictionary as you prepare the list of variables to fetch.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dictionary = {
    &amp;quot;B01003_001E&amp;quot; : &amp;quot;Total Population&amp;quot;,
    &amp;quot;B19001_001E&amp;quot; : &amp;quot;Household Income (12 Month)&amp;quot;,
    &amp;quot;B25105_001E&amp;quot; : &amp;quot;Median Monthly Housing Cost&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This way, the list of variables can easily be created from the data dictionary:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;get_vars = &#39;,&#39;.join(data_dictionary.keys())
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;location-variables&#34;&gt;Location Variables&lt;/h3&gt;
&lt;p&gt;Which geographic variables are available for a particular data set can be found &lt;code&gt;{base_URL}/geography.html&lt;/code&gt;. The Census Bureau uses FIPS codes to reference the different geographies. To find the relevant codes, see 
&lt;a href=&#34;https://www.census.gov/geographies/reference-files.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Delaware for example has FIPS code 10 while North Carolina is 37. So to get information for these two states, we&amp;rsquo;d use &lt;code&gt;for=state:10,37&lt;/code&gt;. You can also use &amp;lsquo;*&amp;rsquo; as a wildcard. So to get all the states&#39; info you&amp;rsquo;d write &lt;code&gt;for=state:*&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Subdivisions for similarly. To get information for Orange County (FIPS 135) in North Carolina (FIPS 37), you could write &lt;code&gt;for=county:135&lt;/code&gt; with the keyword &lt;code&gt;in=state:37&lt;/code&gt;. Let&amp;rsquo;s get the information for Orange and Alamance counties in North Carolina.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;county_dict = {
    &amp;quot;001&amp;quot; : &amp;quot;Alamance County&amp;quot;,
    &amp;quot;135&amp;quot; : &amp;quot;Orange County&amp;quot;,
}
county_fips = &#39;,&#39;.join(county_dict.keys())

state_dict = {&amp;quot;37&amp;quot; : &amp;quot;North Carolina&amp;quot;}
state_fips = &#39;,&#39;.join(state_dict.keys())

query_str = f&amp;quot;get={get_vars}&amp;amp;for=county:{county_fips}&amp;amp;in=state:{state_fips}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-complete-call&#34;&gt;The Complete Call&lt;/h3&gt;
&lt;p&gt;The complete API call can now be easily assembled from the previous two pieces:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;api_call = base_URL + &amp;quot;?&amp;quot; + query_str
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we copy-and-paste this output into our browser, we can see the result looks as follows:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-result-of-our-sample-api-query&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/20-08-census-api/API_return_hu0ee03133ff661cb0ce10ab25f3a6aced_3573_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;The result of our sample API query.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/20-08-census-api/API_return_hu0ee03133ff661cb0ce10ab25f3a6aced_3573_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;454&#34; height=&#34;55&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The result of our sample API query.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;making-the-api-request&#34;&gt;Making the API Request&lt;/h2&gt;
&lt;p&gt;We can make the API request with Python&amp;rsquo;s requests package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests

r = requests.get(api_call)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it! We now have the response we wanted. To interpret the response as JSON, we would call the json method of the response object: &lt;code&gt;r.json()&lt;/code&gt;. The result can then be fed into Pandas to generate our data set.&lt;/p&gt;
&lt;h2 id=&#34;reading-the-json-into-pandas&#34;&gt;Reading the JSON into Pandas&lt;/h2&gt;
&lt;p&gt;We can use Pandas&#39; DataFrame method directly on our data, making sure to specify that the first row consists of column headers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd

data = r.json()

df = pd.DataFrame(data[1:], columns=data[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then do any renaming based on the dictionaries we have created previously.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.rename(columns=data_dictionary, inplace=True)
df[&#39;county&#39;] = df[&#39;county&#39;].replace(county_dict)
df[&#39;state&#39;]  = df[&#39;state&#39;].replace(state_dict)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last step is to make sure our numeric columns are interpreted as such. Since all of the requested variables are in fact numeric,
we can use the dictionary of variables to convert what we need to numeric variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for col in data_dictionary.values():
    df[col] = pd.to_numeric(df[col])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it! We&amp;rsquo;re now ready to work with our data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Porting Forward</title>
      <link>https://dmsenter89.github.io/post/porting-forward/</link>
      <pubDate>Mon, 27 Jul 2020 11:31:07 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/porting-forward/</guid>
      <description>&lt;p&gt;My website is back up and running! Some incompatabilities between my old site and updates to both Hugo and the Academic Theme have led to some downtime on this page as I didn&amp;rsquo;t have time to look through how to rebuild my site without loosing previous content. I&amp;rsquo;m currently in the process of updating everything and will try to bring back some material as well. Stay tuned!&lt;/p&gt;
&lt;p&gt;This page is currently using the Academic theme from Hugo. Docs and other templates are available at 
&lt;a href=&#34;https://wowchemy.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wowchemy&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
