[{"authors":["admin"],"categories":null,"content":"Fluid dynamics modeling and simulations, data analysis, and machine learning have been the major themes of Michael\u0026rsquo;s academic endeavors. The through line of his career has been the utilization of coding to solve computationally complex problems.\nMichael enjoys mentoring and teaching. His teaching philosophy is based on his own experience - that a passion for math and computer science can be cultivated through active learning and emphasizing small victories.\nMichael is originally from Nuremberg, Germany where he attended the Neues Gymnasium Nuernberg, a school following the humanist tradition of education. He now lives in Carrboro with his family, where he enjoys travelling, collecting books, and reading nerdy webcomics in his spare time.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dmsenter89.github.io/author/d.-michael-senter/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/d.-michael-senter/","section":"authors","summary":"Fluid dynamics modeling and simulations, data analysis, and machine learning have been the major themes of Michael\u0026rsquo;s academic endeavors. The through line of his career has been the utilization of coding to solve computationally complex problems.","tags":null,"title":"D. Michael Senter","type":"authors"},{"authors":[],"categories":[],"content":"One of the editors I use regularly is VS Code. I work a lot with Python, but when installing Anaconda using default settings on a Windows machine already having VSC installed there\u0026rsquo;s a good chance you\u0026rsquo;ll run into an issue. When attempting to run Python code straight from VSC you may get an error. This should be fixed on some newer versions of Anaconda, but I\u0026rsquo;ve needed to do something about it often enough I feel it\u0026rsquo;s useful to save the solution janh posted on StackExchange.\nSpecifically, the issue can be fixed by manually changing VSC\u0026rsquo;s default shell from PowerShell to CMD. Just open the command palette (CTRL+SHIFT+P), search \u0026ldquo;Terminal: Select Default Profile\u0026rdquo; and switch to \u0026ldquo;Command Prompt\u0026rdquo;. Everything should work as expected from now on!\n","date":1626871792,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626871792,"objectID":"f664ecba8b426abe9eaf090413bde694","permalink":"https://dmsenter89.github.io/post/21-07-vsc-python-fix/","publishdate":"2021-07-21T08:49:52-04:00","relpermalink":"/post/21-07-vsc-python-fix/","section":"post","summary":"One of the editors I use regularly is VS Code. I work a lot with Python, but when installing Anaconda using default settings on a Windows machine already having VSC installed there\u0026rsquo;s a good chance you\u0026rsquo;ll run into an issue.","tags":[],"title":"Making VS Code and Python Play Nice on Windows","type":"post"},{"authors":[],"categories":[],"content":"I am currently working with a database provided by the North Carolina Department of Public Safety that consists of several fixed-width files. Each of these has an associated codebook that gives the internal variable name, a label of the variable, its data type, as well as the start column and the length of the fields for each column. To import the data sets into SAS, I could copy and paste part of that data into my INPUT and LABEL statements, but that gets tedious pretty fast when dealing with dozens of lines. And since I have multiple data sets like that, I didn\u0026rsquo;t really want to do it that way. In this post I show how a simple command-line script can be written to deal with this problem.\nIntroducing AWK Here are the first few lines of one of these files:\nCMDORNUM OFFENDER NC DOC ID NUMBER CHAR 1 7 CMCLBRTH OFFENDER BIRTH DATE DATE 8 10 CMCLSEX OFFENDER GENDER CODE CHAR 18 30 CMCLRACE OFFENDER RACE CODE CHAR 48 30 CMCLHITE OFFENDER HEIGHT (IN INCHES) CHAR 78 2 CMWEIGHT OFFENDER WEIGHT (IN LBS) CHAR 80 3  We can see that the data is tabular and separated by multiple spaces. Linux programs often deal with column data and a tool is available for manipulating column-based data on the command-line: AWK, a program that can be used for complex text manipulation from the command-line. Some useful tutorials on AWK in general are available at grymoire.com and at tutorialspoint.\nFor our purposes, we want to know about the print and printf commands for AWK. To illustrate how this works, make a simple list of three lines with each term separated by a space:\ncat \u0026lt;\u0026lt; EOF \u0026gt; list.txt 1 one apple pie 2 two orange cake 3 three banana shake EOF  To print the whole file, you\u0026rsquo;d use the print statement: awk '{print}' list.txt. But I could do that with cat, so what\u0026rsquo;s the point? Well, what if I only want one of the columns? By default, $n refers to the nth column in AWK. So to print only the fruits I could write awk '{print $3}' list.txt.\nMultiple columns can be printed by listing multiple columns separated by a comma: awk '{print $2,$3}' list.txt. Note that if you omit the comma the two columns get concatenated into a single column.\nIf additional formatting is required, we can use the printf command. So to create a hyphenated fruit and food-item column, we could use awk '{printf \u0026quot;%s-%s\\n\u0026quot;, $3, $4}' list.txt. Note that we have to indicate the end-of line or else everything will be printed into a single line of text.\nNow we almost have all of the skills to create the label and input statements in SAS! Let\u0026rsquo;s create a comma-delimited list for practice:\ncat \u0026lt;\u0026lt; EOF \u0026gt; list.txt 1,one,apple pie 2,two,orange cake 3,three,banana shake EOF  The -F flag is used to tell AWK to use a different column separator. So to print the third column, we\u0026rsquo;d use awk -F ',' '{print $3}' list.txt.\nMaking the SAS statements Now we know everything we need to know about AWK to create code we want. First we note that our coding file uses multiple spaces as column separators as opposed to single spaces. If each item was a single word, this wouldn\u0026rsquo;t be a problem. Unfortunately, our second column reads \u0026ldquo;OFFENDER NC DOC ID NUMBER\u0026rdquo; which would be split into five columns by default. So we will need to use the column separator flag as -F '[[:space:]][[:space:]]+'.\nThe LABEL Statement A SAS label has the general form LABEL variable-1=label-1\u0026lt;...variable-n=label-n\u0026gt;;, so for example\nlabel score1=\u0026quot;Grade on April 1 Test\u0026quot; score2=\u0026quot;Grade on May 1 Test\u0026quot;;  is a valid label statement. In our file the variable names are given in column 1 and the appropriate labels in column 2. So an AWK script to print the appropriate labels can be written like this:\nawk -F '[[:space:]][[:space:]]+' '{printf \u0026quot;\\t%s=\\\u0026quot;%s\\\u0026quot;\\n\u0026quot;, $1, $2}' FILE.DAT  This is what everything looks like given our code:\nThe INPUT STATEMENT The INPUT statement can be made in a similar way, it just requires some minor tweaking as INPUT can be a bit more complex to handle a variety of data, see the documentation. In our case we are dealing with a fixed-width record. The fourth column gives the starting column of the data and the fifth gives us the width of that field. The third gives us the data type. The majority of ours are character, so it seems easiest to just have the AWK script print each line as though it were a character together with a SAS comment giving the name and \u0026ldquo;official\u0026rdquo; data type. Then the few lines that need adjustment can be manually adjusted. The corresponding code would look like this:\nawk -F '[[:space:]][[:space:]]+' '{printf \u0026quot;\\t@%s %s $%s. /*%s - %s*/\\n\u0026quot;,$4, $1, $5, $3, $2}' FILE.DAT  This is what is returned by our code (highlighted part has been manually edited):\nI hope you all find this useful and that it will save you some typing!\n","date":1625582307,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625582307,"objectID":"dd165a8d959242c48f55fcadb03f4439","permalink":"https://dmsenter89.github.io/post/2021-07-awk-for-sas/","publishdate":"2021-07-06T10:38:27-04:00","relpermalink":"/post/2021-07-awk-for-sas/","section":"post","summary":"I am currently working with a database provided by the North Carolina Department of Public Safety that consists of several fixed-width files. Each of these has an associated codebook that gives the internal variable name, a label of the variable, its data type, as well as the start column and the length of the fields for each column.","tags":[],"title":"Making INPUT and LABEL Statements with AWK","type":"post"},{"authors":[],"categories":[],"content":"I have been using both SAS and Python extensively for a while now. With each having great features, it was very useful to combine my skills in both languages by seamlessly moving between SAS and Python in a single notebook. In the video below, fellow SAS intern Ariel Chien and I show how easy it is to connect the SAS and Python kernels using the open-source SASPy package together with SAS OnDemand for Academics. I hope you will also find that this adds to your workflow!\n  The Jupyter notebook from the video can be viewed on GitHub. For installation instructions, check out the SASPy GitHub page. Configuration for SASPy to connect to ODA can be found at this support page. For more information on SAS OnDemand for Academics, click here.\n","date":1624978625,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624978625,"objectID":"43f341895d07f4930050c83450840876","permalink":"https://dmsenter89.github.io/post/2021-06-youtube-tutorial/","publishdate":"2021-06-29T10:57:05-04:00","relpermalink":"/post/2021-06-youtube-tutorial/","section":"post","summary":"I have been using both SAS and Python extensively for a while now. With each having great features, it was very useful to combine my skills in both languages by seamlessly moving between SAS and Python in a single notebook.","tags":[],"title":"SASPy Video Tutorial","type":"post"},{"authors":[],"categories":[],"content":"The Census Bureau has updated its population estimates for 2020 with county level data. This means any projects that have had to rely on the 2019 estimates can now switch to the 2020 estimates.\nThis is particularly useful for those of us who have been trying to track the development of COVID-19. The average incidence rates are typically rescaled to new cases per 100,000 people. Previous graphs and maps I have created used the 2019 estimates. I have now updated my code for mapping North Carolina developments to use the 2020 estimates.\n  County level data for North Carolina using the NYT COVID data set. Date set to June 8, 2021.   Below this post is my code for loading the necessary data using SAS. Note that I\u0026rsquo;m using a macro called mystate that can be set to the statecode abbreviation of your choice. The conditional County ne 0 is in the code because the county level CSV includes both the county data as well as the totals for each state.\nfilename popdat url 'https://www2.census.gov/programs-surveys/popest/datasets/2010-2020/counties/totals/co-est2020-alldata.csv'; data censusdata; infile POPDAT delimiter=',' MISSOVER DSD lrecl=32767 firstobs=2; informat SUMLEV REGION DIVISION State County best32. STNAME $20. CTYNAME $35. CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020 best32.; format SUMLEV REGION DIVISION STATE best32. COUNTY 5. STNAME $20. CTYNAME $35. CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020 COMMA12. StateCode $2.; input SUMLEV REGION DIVISION STATE COUNTY STNAME $ CTYNAME $ CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020; if (State ne 0) and (State ne 72) then do; FIPS=put(State, Z2.); Statecode=fipstate(FIPS); if Statecode eq \u0026amp;mystate and County ne 0 then output; end; keep STNAME CTYNAME County FIPS Statecode Popestimate2020; run;  The media release can be viewed here. The county-level data set can be downloaded at this page.\n","date":1623268834,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623268834,"objectID":"5354baad22577889f3cfc0a85b71f1ab","permalink":"https://dmsenter89.github.io/post/21-06-covid-county-incidence/","publishdate":"2021-06-09T16:00:34-04:00","relpermalink":"/post/21-06-covid-county-incidence/","section":"post","summary":"The Census Bureau has updated its population estimates for 2020 with county level data. This means any projects that have had to rely on the 2019 estimates can now switch to the 2020 estimates.","tags":[],"title":"Census 2020 Population Estimates Updated","type":"post"},{"authors":[],"categories":null,"content":"","date":1617807600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617807600,"objectID":"57d6d3d7994b3e5729ae4e32fb33b3d7","permalink":"https://dmsenter89.github.io/talk/dissertation-defense/","publishdate":"2021-03-07T16:24:53-05:00","relpermalink":"/talk/dissertation-defense/","section":"talk","summary":"All organisms must deal with fluid transport and interaction, whether it be internal, such as lungs moving air for the extraction of oxygen, or external, such as the expansion and contraction of a jellyfish bell for locomotion. Most organisms are highly deformable and their elastic deformations can be used to move fluid, move through fluid, and resist fluid forces. A particularly effective numerical method for biological fluid-structure interaction simulations is is the immersed boundary (IB) method. An important feature of this method is that the fluid is discretized separately from the boundary interface, meaning that the two meshes do not need to conform with each other. This thesis covers a software tool for the semi-automated creation of finite difference meshes of complex 2D geometries for use with 2D immersed boundary solvers IB2d and IBAMR, alongside two examples of locomotion - flight and swimming.","tags":[],"title":"Dissertation Defense","type":"talk"},{"authors":[],"categories":[],"content":"Metachronal paddling can be described as the sequential oscillation of appendages whereby adjacent paddles maintain a nearly constant phase difference. This mechanism is widely used in nature, both in locomotion such as swimming in crustaceans and in fluid transport such as the clearance of mucus in the mammalian lung. Aside from the wide range of applications, metachronal paddling can be observed across a wide range of Reynolds number regimes.\nI work on simulating the hydrodynamics of metachronal paddling in brine shrimp (Artemia). Brine shrimp are small aquatic crustaceans who lay dormat eggs and are widely used in aquaculture. Their thoracopods are spaced closely together and beat with a small phase difference. We are interested in the hydrodynamics and efficiency of this swimming pattern, which has not previously been rigorously explored.\n","date":1616432400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616432400,"objectID":"27e854c7eb907e3d28b622f58694fc8e","permalink":"https://dmsenter89.github.io/project/metachronal-paddling/","publishdate":"2021-03-22T13:00:00-04:00","relpermalink":"/project/metachronal-paddling/","section":"project","summary":"Metachronal paddling is a widely used mechanism for fluid transport and locomotion. We study the hydrodynamics of metachronal paddling in brine shrimp (*Artemia*)","tags":[],"title":"Metachronal Paddling","type":"project"},{"authors":[],"categories":[],"content":"Git is a widely used version control system that allows users to track their software development in both public and private repositories. It is also increasingly used to store data in text formats, see for example the New York Times COVID-19 data set. This post will briefly demonstrate how to clone and pull updates from a GitHub repository using the git functions that are built into SAS Studio.\nGit functionality has been built into SAS Studio for a little while, so there are actually two slightly different iterations of the git functions. The examples in this post will use the versions compatible with SAS Studio 3.8, which is the current version available at SAS OnDemand for Academics. All git functions use the same prefix. In older versions such as SAS Studio 3.8 the prefix is gitfn_, which is followed by a git command such as \u0026ldquo;clone\u0026rdquo; or \u0026ldquo;pull\u0026rdquo;. In SAS Studio 5, the prefix has been simplified to just git_. Most git functions have the same name between the\ntwo versions, so that the only difference is the prefix. A complete table of the old and new versions of the git functions is available in the documentation.\nWe use the git functions by calling them in an otherwise empty DATA step. In other words, we use the format\ndata _null_; /* use your git functions here */ run;  Cloning a Repo To clone a repo from github we use gitfn_clone. It takes two arguments - the URL of the repository of interest and the path to an empty folder. You can have SAS create the folder for you by using OPTIONS DLCREATEDIR. The basic syntax for the clone is as follows:\ndata _null_; rc = gitfn_clone ( \u0026quot;\u0026amp;repoURL.\u0026quot;, /* URL to repo */ \u0026quot;\u0026amp;targetDIR.\u0026quot;); /* folder to put repo in */ put rc=; /* equals 0 if successful */ run;  It doesn\u0026rsquo;t matter if the URL you use ends in \u0026ldquo;.git\u0026rdquo; or not. In other words, the following two macros would both work the same:\n%LET repoURL=https://github.com/nytimes/covid-19-data; /* works the same as */ %LET repoURL=https://github.com/nytimes/covid-19-data.git;  You can also use password based authentication to pull in private repositories:\ndata _null_; rc = gitfn_clone ( \u0026quot;\u0026amp;repoURL.\u0026quot;, \u0026quot;\u0026amp;targetDIR.\u0026quot;, \u0026quot;\u0026amp;githubUSER.\u0026quot;, /* your GitHub username */ \u0026quot;\u0026amp;githubPASSW.\u0026quot;); /* your GitHub password */ put rc=; /* equals 0 if successful */ run;  NOTE: GitHub is deprecating password-based authentication; you will need to switch to OAuth authentication or SSH keys if you are not already using them. To access a repository using an SSH key, use the following:\ndata _null_; rc = gitfn_clone( \u0026quot;\u0026amp;repoURL.\u0026quot;, \u0026quot;\u0026amp;targetDIR.\u0026quot;, \u0026quot;\u0026amp;sshUSER.\u0026quot;, \u0026quot;\u0026amp;sshPASSW.\u0026quot;, \u0026quot;\u0026amp;sshPUBkey.\u0026quot;, \u0026quot;\u0026amp;sshPRIVkey.\u0026quot;); put rc=; run;  Pull-ing in Updates It is just as easy to pull in updates to a local repository by using gitfn_pull(\u0026quot;\u0026amp;repoDIR.\u0026quot;). This also works with SSH keys for private repositories:\ndata _null_; rc = gitfn_pull( \u0026quot;\u0026amp;repoDIR.\u0026quot;, \u0026quot;\u0026amp;sshUSER.\u0026quot;, \u0026quot;\u0026amp;sshPASSW.\u0026quot;, \u0026quot;\u0026amp;sshPUBkey.\u0026quot;, \u0026quot;\u0026amp;sshPRIVkey.\u0026quot;); run;  Other Functions SAS also offers other built-in functions, such as _diff, _status, _push, _commit, and others. For a complete list, see the SAS documentation here.\n","date":1610394170,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610394170,"objectID":"75878ad228652307f5558a24409fff8a","permalink":"https://dmsenter89.github.io/post/git-with-sas-studio/","publishdate":"2021-01-11T14:42:50-05:00","relpermalink":"/post/git-with-sas-studio/","section":"post","summary":"Git is a widely used version control system that allows users to track their software development in both public and private repositories. It is also increasingly used to store data in text formats, see for example the New York Times COVID-19 data set.","tags":[],"title":"Using Git with SAS Studio","type":"post"},{"authors":[],"categories":[],"content":"A popular beginners machine learning problem is the prediction of housing prices. A frequently used data set for this purpose uses housing prices in California along some additional gathered through the 1990 Census. One such data set is available here at Kaggle. Unfortunately, that data set is rather old. And I live in North Carolina, not California! So I figured I might as well create a new housing data set, but this time with more up-to-date information and using North Carolina as the state to be analyzed. One thing that may be interesting about North Carlina as compared to California is the position of major populations centers. In California, major population centers are near the beach, while major population centers in North Carolina are in the interior of the state. Both large citites and proximity to the beach tend to correlate with higher housing prices. In California, unlike in North Carolina, both of these go together.\nThis post will describe the Kaggle data set with California housing prices and then walk you through how the relevant data can be acquired from the Census Bureau. I\u0026rsquo;ll also show how to clean the data. For those who just want to explore the complete data set, I have made it available for download here.\nTable of Contents  The Source Data Set Acquiring the Census Data Set  Census Variables Geography Considerations   Acquiring Location Data Data Merge with GEOID Matching Data Cleaning   The Source Data Set The geographic unit of the Kaggle data set is the Census block group, which means we will have several thousand data points for our analysis. For a good big-picture overview of Census geography divisions, see this post from the University of Pittsburgh library. The data set\u0026rsquo;s ten columns contain geographic, housing, and Census information that can be broken down as follows:\n geographic information  longitude latitude ocean proximity   housing information  median age of homes median value of homes total number of rooms in area total number of bedrooms in the area   Census information  population number of households median income    Most of these exist directly in the Census API data that we have covered previously. The ocean proximity variable is a categorical giving approximate distance from the beach. My data set will not include this last categorical variable.\nAcquiring the Census Data Set Census Variables The first, and most time consuming aspect, is to figure out where the data we want is located. We know that the US has a decennial census, so accurate information is available every ten years at every level of geography that the Census Bureau tracks. Since it is currently a census year 2020 and the newest information hasn\u0026rsquo;t been tabulated yet, that means the last census count that is available is from 2010. While this is 20 years more current than the California set from 1990, it still seems a bit outdated. Luckily, since the introduction of the American Community Survey (ACS) we have annually updated information available - but not for every level of geography. Only the 5-year ACS average gives us census block-level information for the whole state, making it comparable to the Kaggle data set. The most recent of these is the 2018.\nI start by creating a data dictionary from the groups and variables pages of the \u0026ldquo;American Community Survey: 1-Year Estimates: Detailed Tables 5-Year\u0026rdquo; data set. Note that median home age is not directly available. Instead, we will use the median year structures were built to calculate the median home age. Our data dictionary also does not include any data for the longitude and latitude of each row. We will get that data separately.\ndata_dictionary = { 'B01001_001E' : \u0026quot;population\u0026quot;, 'B11001_001E' : \u0026quot;households\u0026quot;, 'B19013_001E' : \u0026quot;median_income\u0026quot;, 'B25077_001E' : \u0026quot;median_house_value\u0026quot;, 'B25035_001E' : \u0026quot;median_year_structure_built\u0026quot;, 'B25041_001E' : \u0026quot;total_bedrooms\u0026quot;, 'B25017_001E' : \u0026quot;total_rooms\u0026quot;, }  Geography Considerations The next step is figuring out exactly what level of geography we want. Our data set goes down to the Census block level at its most granular. Unfortunately, the Census API won\u0026rsquo;t let us pull the data for all the Census blocks in a state at once. Census tracts on the other hand can be acquired in one go. If we were to shortcut and use only tract data, this would be a pretty quick API call build:\nprimary_geo = \u0026quot;tract:*\u0026quot; secondary_geo = \u0026quot;state:37\u0026quot; query = base_URL + \u0026quot;?get=\u0026quot; + \u0026quot;,\u0026quot;.join(data_dictionary.keys()) + f\u0026quot;\u0026amp;for={primary_geo}\u0026amp;in={secondary_geo}\u0026quot;  But let\u0026rsquo;s try and do it for the Census blocks instead. This will require us to build a sequence of API calls that loops over a larger geographic area, say the different counties in the state, and pull in the respective census block data for that geographic unit. While the FIPS codes for the state counties are sorted alphabetically, they are not contiguous. A full listing of North Carolina county FIPS codes is availalbe from NCSU here. It appears to be that the county FIPS codes are three digits long, starting at 001 and go up to 199 in increments of 2, meaning only odd numbers are in the county set. So it looks like we will be using range(1,200,2) with zero-padding to create the list of county FIPS codes. So we could use a loop similar to this:\nvars_requested = \u0026quot;,\u0026quot;.join(data_dictionary.keys()) for i in range(1,200,2): geo_request = f\u0026quot;for=block%20group:*\u0026amp;in=state:37%20county:{i:03}\u0026quot; query = base_URL + f\u0026quot;?get={vars_requested}\u0026amp;{geo_request}\u0026quot;  While practicing to write the appropriate API call, you may find it useful to give it frequent, quick tests using curl. If you are using Jupyter or IPython, you can use !curl \u0026quot;{query}\u0026quot; to test your API query. Don\u0026rsquo;t forget the quotation marks, since the ampersand has special meaning in the shell. It may be helpful to test the output of your call at the county or city level with that reported on the Census Quickfacts page, if your variable is listed there. This can help make sure you are pulling the data you actually want.\nNow that we have figured out the loop necessary for creation of the API calls, we can put everything together and create a list of Pandas DataFrames which we then concatenate to create our master list.\nimport pandas as pd import requests # create the base-URL host_name = \u0026quot;https://api.census.gov/data\u0026quot; year = \u0026quot;2018\u0026quot; dataset_name = \u0026quot;acs/acs5\u0026quot; base_URL = f\u0026quot;{host_name}/{year}/{dataset_name}\u0026quot; # build the api calls as a list query_vars = base_URL + \u0026quot;?get=\u0026quot; + \u0026quot;,\u0026quot;.join(list(data_dictionary.keys()) + [\u0026quot;NAME\u0026quot;,\u0026quot;GEO_ID\u0026quot;]) api_calls = [query_vars + f\u0026quot;\u0026amp;for=block%20group:*\u0026amp;in=state:37%20county:{i:03}\u0026quot; for i in range(1,200,2) ] # running the API calls will take a moment rjson_list = [requests.get(call).json() for call in api_calls] # create the data frame by concatenation df_list = [pd.DataFrame(data[1:], columns=data[0]) for data in rjson_list] df = pd.concat(df_list, ignore_index=True) # save the raw output to disk df.to_csv(\u0026quot;raw_census.csv\u0026quot;, index=False)  And now we have the data set! We do still have to address the issue of our values all being imported as strings as mentioned in my Census API post.\nAcquiring Location Data As mentioned above, we are still missing information regarding the latitude and longitude of the different block groups. The Census Bureau makes a lot of geographically coded data available on its TIGERweb page. You can interact with it both using a REST API and its web-interface. A page with map information exists here.\nDealing with shapefiles and the TIGERweb API can get a little complicated. Luckily, I know someone with expertise in GIS and shapefiles so we will be using a CSV file of the geographic data we need courtesy of Summer Faircloth, a GIS intern at the North Carolina Department of Transportation. She downloaded the TIGER/Line Shapefiles for the 20189 ACS Block Groups and Census Tracts and joined the data sets in ArcMap, from where she exported our CSV file, which is now available here.\nWe don\u0026rsquo;t need all of the columns in the CSV file, so we will limit the import to the parts we need with the usecols keyword.\ndf = pd.read_csv(\u0026quot;raw_census.csv\u0026quot;, dtype={})  shapedata = pd.read_csv(\u0026quot;BlockGroup_Tract2018.csv\u0026quot;, dtype={\u0026quot;GEOID\u0026quot;: str}, usecols=['GEOID','NAMELSAD','INTPTLAT','INTPTLON','NAMELSAD_1'] ) shapedata = shapedata.rename(columns={'INTPTLAT' : 'latitude', 'INTPTLON' : 'longitude' })  Data Merge with GEOID Matching At this stage we have two data frames - the first consists of all the Census information sans the geographic coordinates of the block groups, and a second data set containing the block groups' location. Both data sets contain a GEOID column that can be used for merging. The GEOID returned by the Census API includes additional information to the regular FIPS code based GEOID used in the TIGERweb system. For example, \u0026ldquo;1500000US370010204005\u0026rdquo; in the census data set is actually GEOID \u0026ldquo;370010204005\u0026rdquo; for purposes of the TIGERweb data set. We\u0026rsquo;ll use a string split to make our GEO_ID variable from the Census API compatible with the FIPS code based GEOID from the TIGERweb service.\ndf[\u0026quot;GEO_ID\u0026quot;] = df[\u0026quot;GEO_ID\u0026quot;].str.split('US').str[1] df = df.merge(shapedata, left_on='GEO_ID', right_on=\u0026quot;GEOID\u0026quot;)  Data Cleaning Now that our data set has been assembled, we can work on cleaning up the merged data set. We have the following tasks left:\n convert column data types to numeric drop unnecessary columns rename columns handle missing values calculate median age of homes  for col in data_dictionary.keys(): if col not in [\u0026quot;NAME\u0026quot;, \u0026quot;GEO_ID\u0026quot;]: df[col] = pd.to_numeric(df[col])  To indicate missing values, the Census API returns a value of \u0026ldquo;-666666666\u0026rdquo; in numeric columns. As all of our variables - except for longitude - ought to be positive, we can use the mask function to convert all negative values to missing. We\u0026rsquo;ll start by filtering out the string columns that are no longer necessary.\n# filter down to our numerical columns keeps = list(data_dictionary.keys()) +[\u0026quot;latitude\u0026quot;, \u0026quot;longitude\u0026quot;] df = df.filter(items=keeps) # replace vals \u0026lt; 0 with missing k = df.loc[:, df.columns != 'longitude'] k = k.mask(k \u0026lt; 0) df.loc[:, df.columns != 'longitude'] = k  Now that the missing values have been handled, we can go ahead and calculate our median home age.\ndf.rename(columns=data_dictionary, inplace=True) df[\u0026quot;housing_median_age\u0026quot;] = 2018 - df[\u0026quot;median_year_structure_built\u0026quot;] df.drop(columns=\u0026quot;median_year_structure_built\u0026quot;, inplace=True)  And now we\u0026rsquo;re done! We will save our output data set to disk for future analysis in a different post.\ndf.to_csv(\u0026quot;NC_Housing_Prices_2018.csv\u0026quot;, index=False)  ","date":1604675401,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606749001,"objectID":"f7e0af2bd9aee1557c21c03cb9e7ee1f","permalink":"https://dmsenter89.github.io/post/20-11-north-carolina-housing/","publishdate":"2020-11-06T10:10:01-05:00","relpermalink":"/post/20-11-north-carolina-housing/","section":"post","summary":"A popular beginners machine learning problem is the prediction of housing prices. A frequently used data set for this purpose uses housing prices in California along some additional gathered through the 1990 Census.","tags":[],"title":"North Carolina Housing Data","type":"post"},{"authors":[],"categories":[],"content":"After reading a news article about teacher pay in the US, I was curious and wanted to look into the source data myself. Unfortunately, the source that was mentioned was a publication by the National Education Association (NEA) which had the data as tables embedded inside a PDF report. As those who know me can attest, I don\u0026rsquo;t like hand-copying data. It is slow and error-prone. Instead, I decided to use the tabula package to extract the information from the PDFs directly into a Pandas dataframe. In this post, I will show you how to extract the data and how to clean it up for analysis.\nTable of Contents  The Data Source Loading the Data  Cleaning the Data Numeric Conversion   Table B-6   The Data Source Several years worth of data are available in PDF form on the NEA website. Reading through the technical notes, they highlight that they did not collect all of their own salary information. Some states' information is calculated from the American Community Survey (ACS) done by the Census Bureau - a great resource whose API I have covered in a different post. Each report includes accurate data for the previous school year, as well as estimates for the current school year. As of this post, the newest report is the 2020 report which includes data for the the 2018-2019 school year, as well as estimates of the 2019-2020 school year.\nThe 2020 report has the desired teacher salary information in two separate locations. One is in table B-6 on page 26 of the PDF, which shows a ranking of the different states' average salary in addition to the average salary:\nA second location is in table E-7 on page 46, which gives salary data for the completed school year as well as different states' estimates for the 2019-2020 school year:\nNote that table E-7 lacks the star-annotation marking NEA estimated values. This, and the lack of the ranking column, makes Table E-7 easier to parse. In the main example below, this will be the source of the five years of data. I will however also show how to parse table B-6 at the end of this post for completion.\nLoading the Data As of October 2020, the NEA site has five years worth of reports online. Unfortunately, these are not labeled consistently for all five years. Similarly the page numbers differ for each report. Prior to the 2018 report, inconsistent formats were used for the tables which require previous years to be parsed separately from the newer tables. For this reason, I\u0026rsquo;ll make a dictionary for the 2018-2020 reports only, which will simplify the example below.\nreport = { '2020' : { 'url' : \u0026quot;https://www.nea.org/sites/default/files/2020-10/2020%20Rankings%20and%20Estimates%20Report.pdf\u0026quot;, 'page' : 46, }, '2019' : { 'url' : \u0026quot;https://www.nea.org/sites/default/files/2020-06/2019%20Rankings%20and%20Estimates%20Report.pdf\u0026quot;, 'page' : 49, }, '2018' : { 'url' : \u0026quot;https://www.nea.org/sites/default/files/2020-07/180413-Rankings_And_Estimates_Report_2018.pdf\u0026quot;, 'page' : 51, }, }  We can now use dictionary comprehension to fill in a dictionary with all the source tables of interest. We will be using the tabula package to extract data from the PDFs. If you don\u0026rsquo;t have it installed, you can use pip install tabula-py to get a copy. The method that reads in a PDF is aptly called read_pdf. It\u0026rsquo;s first argument is a file path to the PDF. Since we want to use a URL, we will use the keyword argument stream=True and then name the specific page in each PDF that contains the information we are after. By default, read_pdf returns a list of dataframes, so we just save the first element from the list, which is the report we are interested in.\nNote: if you are using WSL, depending on your settings, you may get the error Exception in thread \u0026quot;main\u0026quot; java.awt.AWTError: Can't connect to X11 window server using 'XXX.XXX.XXX.XXX:0' as the value of the DISPLAY variable. error when running read_pdf. This is fixed by having an X11 server running.\nimport tabula import pandas as pd source_df = {year : tabula.read_pdf(report[year]['url'], stream=True, pages=report[year]['page'])[0] for year in report.keys()}  And that\u0026rsquo;s it in principle. How cool is that! Of course, we still need to clean our data a little bit.\nCleaning the Data Let\u0026rsquo;s take a look at the first and last few entries of the 2020 report:\npd.concat([source_df['2020'].head(), source_df['2020'].tail()])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Unnamed: 0 2018-19 2019-20 From 2018-19 to 2019-20 From 2010-11 to 2019-20 (%)     0 State Salary($) Salary($) Change(%) Current Dollar Constant Dollar   1 Alabama 52,009 54,095 4.01 13.16 -2.58   2 Alaska 70,277 70,877 0.85 15.36 -0.69   3 Arizona 50,353 50,381 0.06 8.03 -7.00   4 Arkansas 49,438 49,822 0.78 8.31 -6.75   48 Washington 73,049 72,965 -0.11 37.86 18.69   49 West Virginia 47,681 50,238 5.36 13.51 -2.28   50 Wisconsin 58,277 59,176 1.54 9.17 -6.02   51 Wyoming 58,861 59,014 0.26 5.19 -9.44   52 United States 62,304 63,645 2.15 14.14 -1.73     We see that each column is treated as a string object (which you can confirm by running source_df['2020'].dtypes) and that the first row of data is actually at index 1 due to the fact that the PDF report used a two-row header. This means we can safely drop the first row of every dataframe. We can also drop the last row of every dataframe since that just contains summary data of the US as a whole, which we can easily regenerate as necessary. So row indices 0 and 52 can go for all of our data sets.\nfor df in source_df.values(): df.drop([0, 52], inplace=True)  Next up I\u0026rsquo;d like to fix the column names. The fist column is clearly the name of the state (except in the case of Washington D.C.), while the next two columns give the years for which the salary information is given. Let\u0026rsquo;s rename the second and third columns according to the pattern Salary %YYYY-YY using Python\u0026rsquo;s f-string syntax.\nfor df in source_df.values(): df.rename(columns={ df.columns[0] : \u0026quot;State\u0026quot;, df.columns[1] : f\u0026quot;Salary {str(df.columns[1])}\u0026quot;, df.columns[2] : f\u0026quot;Salary {str(df.columns[2])}\u0026quot;, }, inplace=True) source_df[\u0026quot;2020\u0026quot;].head() # show the result of our edits so far   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  State Salary 2018-19 Salary 2019-20 From 2018-19 to 2019-20 From 2010-11 to 2019-20 (%)     1 Alabama 52,009 54,095 4.01 13.16 -2.58   2 Alaska 70,277 70,877 0.85 15.36 -0.69   3 Arizona 50,353 50,381 0.06 8.03 -7.00   4 Arkansas 49,438 49,822 0.78 8.31 -6.75   5 California 83,059 84,659 1.93 24.74 7.39     Looks like we\u0026rsquo;re almost done! Let\u0026rsquo;s drop the unnecessary columns and check our remaining column names:\nfor year, df in source_df.items(): df.drop(df.columns[3:], axis=1, inplace=True) print(f\u0026quot;{year}:\\t{df.columns}\u0026quot;)  2020:\tIndex(['State', 'Salary 2018-19', 'Salary 2019-20'], dtype='object') 2019:\tIndex(['State', 'Salary 2017-18', 'Salary 2018-19'], dtype='object') 2018:\tIndex(['State', 'Salary 2017', 'Salary 2018'], dtype='object')  We can see that the column naming scheme in 2018 was different than in the previous reports. To make them all compatible for our merge, we\u0026rsquo;re going to have to do some more editing. Based on the other reports, it appears as though the 2018 report used the calendar year of the end of the school year, while the others utilized a range. This can easily be solved using regex substitution. We\u0026rsquo;ll do that now.\nimport re for year, df in source_df.items(): if year != \u0026quot;2018\u0026quot;: df.rename(columns={ df.columns[1] : re.sub(r\u0026quot;\\d{2}-\u0026quot;, '', df.columns[1]), df.columns[2] : re.sub(r\u0026quot;\\d{2}-\u0026quot;, '', df.columns[2]), }, inplace=True) # print the output for verification print(f\u0026quot;{year}:\\t{df.columns}\u0026quot;)  2020:\tIndex(['State', 'Salary 2019', 'Salary 2020'], dtype='object') 2019:\tIndex(['State', 'Salary 2018', 'Salary 2019'], dtype='object') 2018:\tIndex(['State', 'Salary 2017', 'Salary 2018'], dtype='object')  Now that everything works, we can do our merge to create a single dataframe with the information for all of the school years we have downloaded.\nmerge_df = source_df[\u0026quot;2018\u0026quot;].drop([\u0026quot;Salary 2018\u0026quot;], axis=1).merge( source_df[\u0026quot;2019\u0026quot;].drop([\u0026quot;Salary 2019\u0026quot;], axis=1)).merge( source_df[\u0026quot;2020\u0026quot;]) merge_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  State Salary 2017 Salary 2018 Salary 2019 Salary 2020     0 Alabama 50,391 50,568 52,009 54,095   1 Alaska 6 8,138 69,682 70,277 70,877   2 Arizona 4 7,403 48,723 50,353 50,381   3 Arkansas 4 8,304 50,544 49,438 49,822   4 California 7 9,128 80,680 83,059 84,659     Numeric Conversion We\u0026rsquo;re almost done! Notice that we still have not dealt with the fact that every column is still treated as a string. Before we can use the to_numeric function, we still need to take care of two issues:\n The commas in the numbers. While they are nice for our human eyes, Pandas doesn\u0026rsquo;t like them. In the 2017 salary column, there appears to be extraneous white space after the first digit for some entries.  Luckily, both of these problems can be remedied with a simple string replacement operation.\nmerge_df.iloc[:,1:] = merge_df.iloc[:,1:].replace(r\u0026quot;[,| ]\u0026quot;, '', regex=True) for col in merge_df.columns[1:]: merge_df[col] = pd.to_numeric(merge_df[col])  Now we\u0026rsquo;re done! We have created an overview of annual teacher salaries from the 2016-17 school year until 2019-20 extracted from a series of PDFs published by the NEA. We have cleaned up the data and converted everything to numerical values. We can now get summary statistics and do any analysis of interest with this data.\nmerge_df.describe() # summary stats of our numeric columns   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Salary 2017 Salary 2018 Salary 2019 Salary 2020     count 51.000000 51.000000 51.000000 51.000000   mean 56536.196078 57313.039216 58983.254902 60170.647059   std 9569.444674 9795.914601 10286.843230 10410.259274   min 42925.000000 44926.000000 45105.000000 45192.000000   25% 49985.000000 50451.500000 51100.500000 52441.000000   50% 54308.000000 53815.000000 54935.000000 57091.000000   75% 61038.000000 61853.000000 64393.500000 66366.000000   max 81902.000000 84227.000000 85889.000000 87543.000000     Table B-6 As mentioned above, table B-6 in the 2020 Report presents slightly greater challenges. A lot of the cleaning is similar or identical, so I will not reproduce it in full. Instead, I have loaded a subsetted part of table B-6 and will show how this can be cleaned up as well. But first, let\u0026rsquo;s look at the first several entries:\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Unnamed: 0 2017-18 (Revised) 2018-19     0 State Salary($) Rank Salary($)   1 Alabama 50,568 36 52,009   2 Alaska 69,682 7 70,277   3 Arizona 48,315 45 50,353   4 Arkansas 49,096 44 49,438   5 California 80,680 2 83,059 *   6 Colorado 52,695 32 54,935   7 Connecticut 74,517 * 5 76,465 *   8 Delaware 62,422 13 63,662     We can see that there is an additional hurdle compared to the previous tables: the second column now contains data from two columns, both the Salary information as well as a ranking of the salary as it compares to the different states. For a few states, there is additionally a \u0026lsquo;*\u0026rsquo; to denote values that were estimated as opposed to received. We can again use a simple regex replace together with a capture group to parse out only those values that we are interested in, while dropping the extraneous information using the code below.\nb6.iloc[:,1:] = b6.iloc[:,1:].replace(r\u0026quot;([\\d,]+).*\u0026quot;, r\u0026quot;\\1\u0026quot;, regex=True)  And now we\u0026rsquo;re back to where we were above before we did the string conversion. This is what it looks like after also dropping the first row and renaming the columns:\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  State Salary 2018 Salary 2019     1 Alabama 50,568 52,009   2 Alaska 69,682 70,277   3 Arizona 48,315 50,353   4 Arkansas 49,096 49,438   5 California 80,680 83,059   6 Colorado 52,695 54,935   7 Connecticut 74,517 76,465   8 Delaware 62,422 63,662     From here on out, we can proceed as in the previous example.\n","date":1604025540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604419200,"objectID":"c5eca41aa0719561a38b4163c276f4ab","permalink":"https://dmsenter89.github.io/post/20-10-tabula/","publishdate":"2020-10-29T22:39:00-04:00","relpermalink":"/post/20-10-tabula/","section":"post","summary":"What do you do when your data table is in PDF format? Let's use tabula-py to extract teacher salary information from PDFs directly into Pandas dataframes. We'll also use some regex to clean up the results.","tags":[],"title":"Teacher Salaries","type":"post"},{"authors":[],"categories":[],"content":"The Census Bureau makes an incredible amount of data available online. In this post, I will summarize how to get access to this data via Python by using the Census Bureau\u0026rsquo;s API. The Census Bureau makes a pretty useful guide available here - I recommend checking it out.\nTable of Contents  API Basics Building an the Base URL Building the Query  The \u0026lsquo;Get\u0026rsquo; Variables Location Variables The Complete Call   Making the API Request Reading the JSON into Pandas   API Basics We can think of an API query of consisting of two main parts: a base URL (also called a root URL) and a query string. These two strings are joined together with the query character \u0026ldquo;?\u0026rdquo; to create an API call. The resulting API call can in theory be copy-and-pasted into the URL bar of your browser, and I recommend this when first playing around with a new API. Seeing the raw text returned in the browser can help you understand the structure of what is being returned. In the case of the Census Bureau\u0026rsquo;s API, it returns a string that essentially looks like a list of lists from a Python perspective. This can easily be turned into a Pandas dataset. Be aware that all values are returned as strings. You\u0026rsquo;ll have to convert number columns to numeric by yourself.\nTo get an overview of all available data sets, you can go to the data page which contains a long list of data sets. This data page is incredibly useful because it gives access to all of the information needed to build a correct API call, including the base URLs of all data sets and the variables available in each.\n  A snapshot of two datasets available as part of the 2018 American Community Survey (ACS).   Building an the Base URL Let\u0026rsquo;s build a sample API call for the 2018 American Community Survey 1-Year Detailed Table. While we could just copy the base URL from the data page, I like to assemble mine manually from its component parts. This makes it easier to write a wrapper for the API calls if you plan on scraping the same data from multiple years.\nhost_name = \u0026quot;https://api.census.gov/data\u0026quot; year = \u0026quot;2018\u0026quot; dataset_name = \u0026quot;acs/acs1\u0026quot; base_URL = f\u0026quot;{host_name}/{year}/{dataset_name}\u0026quot;  Building the Query Now that we have the base URL, we can work on building the query. For purposes of the Census Bureau, you will need two components: the variables of interest, which are listed after the get= keyword, and the geography for which you would like the data listed after the for= keyword. For certain subdivisions, like counties, you can specify two levels of geography by adding an in= keyword at at the end.\nThe \u0026lsquo;Get\u0026rsquo; Variables Since many of the data sets have a large amount of variables in them, it often makes sense to take a look at the \u0026ldquo;groups\u0026rdquo; page first. This page lists variables as groups, giving you a better overview of what data is available. This page is available at {base_URL}/groups.html. A complete list of all variables in the data set is available at {base_URL}/variables.html.\nLet\u0026rsquo;s find some variables. The most basic variable we\u0026rsquo;d expect to find here is total population. We can find this variable in group \u0026ldquo;B01003\u0026rdquo;. The total estimate is in sub-variable \u0026ldquo;001E\u0026rdquo;, meaning that the variable for total population is \u0026ldquo;B01003_001E\u0026rdquo;. Let\u0026rsquo;s also get household income (group \u0026ldquo;B19001\u0026rdquo;) not broken down by race: \u0026ldquo;B19001_001E\u0026rdquo;. There is also median monthly housing cost (group B25105) with variable \u0026ldquo;B25105_001E\u0026rdquo;. Since the variable names can be a little difficult to parse, I recommend making a data dictionary as you prepare the list of variables to fetch.\ndata_dictionary = { \u0026quot;B01003_001E\u0026quot; : \u0026quot;Total Population\u0026quot;, \u0026quot;B19001_001E\u0026quot; : \u0026quot;Household Income (12 Month)\u0026quot;, \u0026quot;B25105_001E\u0026quot; : \u0026quot;Median Monthly Housing Cost\u0026quot;, }  This way, the list of variables can easily be created from the data dictionary:\nget_vars = ','.join(data_dictionary.keys())  Location Variables Which geographic variables are available for a particular data set can be found {base_URL}/geography.html. The Census Bureau uses FIPS codes to reference the different geographies. To find the relevant codes, see here. Delaware for example has FIPS code 10 while North Carolina is 37. So to get information for these two states, we\u0026rsquo;d use for=state:10,37. You can also use \u0026lsquo;*\u0026rsquo; as a wildcard. So to get all the states' info you\u0026rsquo;d write for=state:*.\nSubdivisions for similarly. To get information for Orange County (FIPS 135) in North Carolina (FIPS 37), you could write for=county:135 with the keyword in=state:37. Let\u0026rsquo;s get the information for Orange and Alamance counties in North Carolina.\ncounty_dict = { \u0026quot;001\u0026quot; : \u0026quot;Alamance County\u0026quot;, \u0026quot;135\u0026quot; : \u0026quot;Orange County\u0026quot;, } county_fips = ','.join(county_dict.keys()) state_dict = {\u0026quot;37\u0026quot; : \u0026quot;North Carolina\u0026quot;} state_fips = ','.join(state_dict.keys()) query_str = f\u0026quot;get={get_vars}\u0026amp;for=county:{county_fips}\u0026amp;in=state:{state_fips}\u0026quot;  The Complete Call The complete API call can now be easily assembled from the previous two pieces:\napi_call = base_URL + \u0026quot;?\u0026quot; + query_str  If we copy-and-paste this output into our browser, we can see the result looks as follows:\n  The result of our sample API query.   Making the API Request We can make the API request with Python\u0026rsquo;s requests package:\nimport requests r = requests.get(api_call)  And that\u0026rsquo;s it! We now have the response we wanted. To interpret the response as JSON, we would call the json method of the response object: r.json(). The result can then be fed into Pandas to generate our data set.\nReading the JSON into Pandas We can use Pandas' DataFrame method directly on our data, making sure to specify that the first row consists of column headers.\nimport pandas as pd data = r.json() df = pd.DataFrame(data[1:], columns=data[0])  We can then do any renaming based on the dictionaries we have created previously.\ndf.rename(columns=data_dictionary, inplace=True) df['county'] = df['county'].replace(county_dict) df['state'] = df['state'].replace(state_dict)  The last step is to make sure our numeric columns are interpreted as such. Since all of the requested variables are in fact numeric, we can use the dictionary of variables to convert what we need to numeric variables.\nfor col in data_dictionary.values(): df[col] = pd.to_numeric(df[col])  And that\u0026rsquo;s it! We\u0026rsquo;re now ready to work with our data.\n","date":1598100835,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598100835,"objectID":"b15cce3c2e5806b0971c4615b9a1a94f","permalink":"https://dmsenter89.github.io/post/20-08-census-api/","publishdate":"2020-08-22T08:53:55-04:00","relpermalink":"/post/20-08-census-api/","section":"post","summary":"The Census Bureau makes an incredible amount of data available online. In this post, I will summarize how to get access to this data via Python by using the Census Bureau\u0026rsquo;s API.","tags":[],"title":"Accessing Census Data via API","type":"post"},{"authors":["D. Michael Senter","Dylan Ray Douglas","W. Christopher Strickland","Steven G. Thomas","Anne M Talkington","Laura Miller"],"categories":[],"content":"","date":1596468475,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596468475,"objectID":"f53c5f8137d2c17ac968dc30b8fb6a58","permalink":"https://dmsenter89.github.io/publication/senter-2020-meshmerizeme/","publishdate":"2020-08-05T11:27:55-04:00","relpermalink":"/publication/senter-2020-meshmerizeme/","section":"publication","summary":"Numerous fluid-structure interaction problems in biology have been investigated using the immersed boundary method. The advantage of this method is that complex geometries, e.g., internal or external morphology, can easily be handled without the need to generate matching grids for both the fluid and the structure. Consequently, the difficulty of modeling the structure lies often in discretizing the boundary of the complex geometry (morphology). Both commercial and open source mesh generators for finite element methods have long been established; however, the traditional immersed boundary method is based on a finite difference discretization of the structure. Here we present a software library for obtaining finite difference discretizations of boundaries for direct use in the 2D immersed boundary method. This library provides tools for extracting such boundaries as discrete mesh points from digital images. We give several examples of how the method can be applied that include passing flow through the veins of insect wings, within lymphatic capillaries, and around starfish using open-source immersed boundary software.","tags":[],"title":"A semi-automated finite difference mesh creation method for use with immersed boundary software IB2d and IBAMR","type":"publication"},{"authors":[],"categories":null,"content":"This workshop covers data acquisition and basic data preparation with a focus on using Python with Jupyter Notebooks. To avoid having to install Python locally during the workshop, we will be utilizing an Azure notebook project. The example files are located here.\nPlease note that the free Azure notebooks will only be available until early October. To continue using Python and Jupyter notebooks, you may want to consider using a local installation. For Windows and Mac users, I recommend using Anaconda. For continued cloud usage, you may consider Cocalc. Please note that you will need a subscription for your Cocalc notebooks to be able to download data from external sources.\nAdditional Links:\n  Engauge Digitizer (software to extract data points from graphs).  Markdown Cheatsheet.  ","date":1596128400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596128400,"objectID":"80d634f592a1f22b7ec5837df0dace9d","permalink":"https://dmsenter89.github.io/talk/webscraping-tutorial/","publishdate":"2020-07-27T14:10:17-04:00","relpermalink":"/talk/webscraping-tutorial/","section":"talk","summary":"Data acquisition is a key step in research. In this workshop, we will consider how to effectively access publicly available data sets. We will discuss how to find and load data published in CSV/Excel formats.  We will learn how to use Pandas to parse HTML tables. We will discuss some best practises for data acquisition and storage.","tags":[],"title":"Basics of Web Scraping with Python","type":"talk"},{"authors":[],"categories":[],"content":"Basics of Web Scraping with Python Michael Senter\n Goals for Today  Understand what tools and methods are available.  Be able to create a new project using Python and Jupyter.  Be able to edit existing code snippets to gather data.    Python  easy to learn, reads like \u0026ldquo;pseudocode\u0026rdquo; widely used in a variety of fields many books, websites, etc. to help you learn  print(\u0026quot;Hello, world!\u0026quot;)   Data Sources  CSV/Excel Downloads  COVID Related Data  Johns Hopkins Dashboard The Johns Hopkins data is published on GitHub and is updated regularly.\n Using SAS filename outfile \u0026quot;~/import-data-nyt.sas\u0026quot;; /* download official SAS script to above filename */ proc http url=\u0026quot;https://raw.githubusercontent.com/sassoftware/covid-19-sas/master/Data/import-data-nyt.sas\u0026quot; method=\u0026quot;get\u0026quot; out=outfile; run; /* run the downloaded script */ %include \u0026quot;~/import-data-nyt.sas\u0026quot;; /* state and county level data are now in memory */  ","date":1596126600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596126600,"objectID":"4e6dec377db9b7f6c1777b9311af259c","permalink":"https://dmsenter89.github.io/slides/webscraping-tutorial/","publishdate":"2020-07-30T12:30:00-04:00","relpermalink":"/slides/webscraping-tutorial/","section":"slides","summary":"Basics of Web Scraping with Python Michael Senter\n Goals for Today  Understand what tools and methods are available.  Be able to create a new project using Python and Jupyter.","tags":[],"title":"Webscraping Tutorial","type":"slides"},{"authors":[],"categories":[],"content":"My website is back up and running! Some incompatabilities between my old site and updates to both Hugo and the Academic Theme have led to some downtime on this page as I didn\u0026rsquo;t have time to look through how to rebuild my site without loosing previous content. I\u0026rsquo;m currently in the process of updating everything and will try to bring back some material as well. Stay tuned!\nThis page is currently using the Academic theme from Hugo. Docs and other templates are available at wowchemy.\n","date":1595863867,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625580900,"objectID":"92d904527521646e80dcda8e55bf841f","permalink":"https://dmsenter89.github.io/post/porting-forward/","publishdate":"2020-07-27T11:31:07-04:00","relpermalink":"/post/porting-forward/","section":"post","summary":"My website is back up and running! Some incompatabilities between my old site and updates to both Hugo and the Academic Theme have led to some downtime on this page as I didn\u0026rsquo;t have time to look through how to rebuild my site without loosing previous content.","tags":[],"title":"Porting Forward","type":"post"},{"authors":[],"categories":null,"content":"Please join me as I present the work I have done so far in my graduate career and discuss avenues for future study.\n","date":1587128400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587128400,"objectID":"d4925dbe1b1b4dd4dad7517dcbe7fbcf","permalink":"https://dmsenter89.github.io/talk/thesis-proposal/","publishdate":"2020-04-15T08:00:00-04:00","relpermalink":"/talk/thesis-proposal/","section":"talk","summary":"Please join me as I present the work I have done so far in my graduate career and discuss avenues for future study.","tags":[],"title":"Thesis Proposal","type":"talk"},{"authors":[],"categories":[],"content":"Insects are ubiquitious throughout the world. Most of us are familiar with winged insects such as butterflies and bees. Insect flight is an interesting topic from a biomechanics perspective. Unlike birds, most insects (with some eceptions, such as dragonflies and others) do not have flight muscles attached to their wings. Instead, their flight muscles oscillate their thorax, which in turn makes the wings move. Furthermore, they beat their wings at a very high speed. The aerodynamics of insect flight are also very interesting. Larger insects are able to fly by creating a leading edge vortex. This method does not work in the smallest insect fliers. Such insects include the thrips and chalcid wasps, some of which have wingspans as small as 1 mm. These insects have unusual wing structures, as can be seen in this image:\nThe solid part of the wing is rather small and narrow, with many large bristles projecting from the solid part of the wing. Insects such as thrips do not create a leading edge vortex; instead, they fly using the \u0026ldquo;clap and fling\u0026rdquo; method. This method is common amongst insects who fly in the intermediate Reynolds number regime, $1\\leq \\mathrm{Re} \\leq 100$.\n","date":1527866603,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527866603,"objectID":"047eb4aa355fddd02ce6989411f5488b","permalink":"https://dmsenter89.github.io/project/clap-and-fling/","publishdate":"2018-06-01T11:23:23-04:00","relpermalink":"/project/clap-and-fling/","section":"project","summary":"Simulating the aerodynamics of the smallest insect fliers.","tags":[],"title":"Clap and Fling","type":"project"},{"authors":[],"categories":[],"content":" IB2d and IBAMR are two software packages implementing the immersed boundary method (see below). These packages model fluid-structure interaction problems based on user given parameters and geometry. The manual creation of the initial geometry mesh can be difficult and time consuming, especially for the complex shapes encountered in biological applications. Oftentimes we have images of the geometry we wish to explore. I am developing software to help automate the creation of such CFD meshes for 2D simulations with a file-format suitable for use with IB2d and IBAMR from images. An initial prototype version is available on Github. A paper exploring the use of MeshmerizeMe in conjuction with IB2d for simulations is in preparation.\nUsage MeshmerizeMe needs two input files per experimental geometry: an SVG image file with the geometry of interest and an input2d file with the experiment parameters. When selecting an SVG for use with MeshmerizeMe it will automatically look for the input2d file in the same folder. It will then parse the paths, transform them into the correct coordinate system and appropriately sample the paths based on the size of the Cartesian grid set in the input2d file. The geometry will be exported as a vertex file. This file is readable by both IB2d and IBAMR.\nSVGs were chosen as the image source as the are an open, text-based format making them very accesible to work with. They are standardized for web use and many tools exist for creating and manipulating SVG images. They can be created from source images such as photographs or scans by means of edge detection tools and by manually tracing the outline of a shape of interest Consider optimizing the SVG prior to processing to save time.\nAs the current version of MeshmerizeMe only handles a subset of SVG, tools that optimize the SVG files created by your editor are very useful. Examples of such software include SVGO, which also offers a webapp called SVGOMG. Another software is svgcleaner.\nIBM Background One aspect of computational fluid dynamics is the investigation of fluid-structure interactions. One method developed for the study of such interactions is the immersed boundary method (IBM) developed by Peskin1. It is well known that fluids can be studied from both a Eulerian and a Lagrangian view. The IBM combines these - the domain of the problem is resolved as a Cartesian grid on which Eulerian equations are solved for fluid velocity and pressure. In the case of Newtonian fluids the incompressible Navier-Stokes equations comprising of\n$$ \\rho \\left( \\frac{\\partial \\mathbf{u}}{\\partial t} + \\mathbf{u} \\cdot \\nabla \\mathbf{u} \\right) = - \\nabla \\mathbf{p} + \\mu \\nabla^2 \\mathbf{u} + \\mathbf{f}$$\nand\n$$\\nabla \\cdot \\mathbf{u} = 0$$\nneed to be solved.\nThe immersed structures are modeled as fibers in the form of parametric curves $X(s,t)$, where $s$ is a parameter and $t$ is time. The fiber experiences force distributions $F(s,t)$, and we can derive the force the fiber exerts on the fluid from the momentum equation. For the fibers we then solve\n$$\\mathbf{f} = \\int_\\Gamma \\mathbf{F}(s,t),\\delta\\left(\\mathbf{x}-\\mathbf{X}(s,t)\\right),ds$$\nand\n$$\\frac{\\partial \\mathbf{X}}{\\partial t} = \\int_\\Omega \\mathbf{u}(\\mathbf{x},t), \\delta \\left( \\mathbf{x}-\\mathbf{X}(s,t)\\right),d\\mathbf{x}.$$\nHere, $\\Gamma$ is the immersed structure and $\\Omega$ is the fluid domain.\nThe immersed structures are discretized not on a Cartesian grid but on a separate Lagrangian grid on the fiber itself. Of import to CFD software users is that the initial discretization of the immersed structure has to be supplied by the user. While this is not too difficult for simple geometries, the often complex structures encountered in mathematical biology can present a significant time investment. This is the part where MeshmerizeMe comes in handy.\n  Charles S Peskin. 2002. \u0026ldquo;The immersed boundary method.\u0026rdquo; Acta numerica 11:479-517. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1505921706,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505921706,"objectID":"75d971656dea94655081afc1899e8ab1","permalink":"https://dmsenter89.github.io/project/meshmerizeme/","publishdate":"2017-09-20T11:35:06-04:00","relpermalink":"/project/meshmerizeme/","section":"project","summary":"Automatic mesh generation from 2D images for use with immersed boundary solvers.","tags":[],"title":"MeshmerizeMe","type":"project"},{"authors":["C Hohenegger","R Durr","DM Senter"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"424a48dfbcfb4647e011e675044fd04a","permalink":"https://dmsenter89.github.io/publication/hohenegger-2017-mean/","publishdate":"2020-07-27T15:40:59.097696Z","relpermalink":"/publication/hohenegger-2017-mean/","section":"publication","summary":"The motion of a passive spherical particle in a fluid has been widely described via a balance of force equations known as a Generalized Langevin Equation (GLE) where the covariance of the thermal force is related to the time memory function of the fluid. For viscous fluids, this relationship is simply a delta function in time, while for a viscoelastic fluid it depends on the constitutive equation of the fluid memory function. In this paper, we consider a general setting for linear viscoelasticity which includes both solvent and polymeric contributions, and a family of memory functions known as the generalized Rouse kernel. We present a statistically exact algorithm to generate paths which allows for arbitrary large time steps and which relies on the numerical evaluation of the covariance of the velocity process. As a consequence of the viscoelastic properties of the fluid, the particle exhibits subdiffusive behavior, which we verify as a function of the free parameters in the generalized Rouse kernel. We then numerically compute the mean first passage time of a passive particle through layers of different widths and establish that, for the generalized Rouse kernel, the mean first passage time grows quadratically with the layers width independently of the free parameters. Along the way, we also find the linear scaling of the mean first passage time for a layer of fixed width as a function of the particles radius.","tags":null,"title":"Mean first passage time in a thermally fluctuating viscoelastic fluid","type":"publication"}]