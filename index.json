[{"authors":["admin"],"categories":null,"content":"Fluid dynamics modeling and simulations, data analysis, and machine learning have been the major themes of Michael\u0026rsquo;s academic endeavors. The through line of his career has been the utilization of coding to solve computationally complex problems.\nMichael enjoys mentoring and teaching. His teaching philosophy is based on his own experience - that a passion for math and computer science can be cultivated through active learning and emphasizing small victories.\nMichael is originally from Nuremberg, Germany where he attended the Neues Gymnasium Nuernberg, a school following the humanist tradition of education. He now lives in Carrboro with his family, where he enjoys travelling, collecting books, and reading nerdy webcomics in his spare time.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dmsenter89.github.io/author/d.-michael-senter/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/d.-michael-senter/","section":"authors","summary":"Fluid dynamics modeling and simulations, data analysis, and machine learning have been the major themes of Michael\u0026rsquo;s academic endeavors. The through line of his career has been the utilization of coding to solve computationally complex problems.","tags":null,"title":"D. Michael Senter","type":"authors"},{"authors":[],"categories":[],"content":"After reading a news article about teacher pay in the US, I was curious and wanted to look into the source data myself. Unfortunately, the source that was mentioned was a publication by the National Education Association (NEA) which had the data as tables embedded inside a PDF report. As those who know me can attest, I don\u0026rsquo;t like hand-copying data. It is slow and error-prone. Instead, I decided to use tabula to extract the information from the PDFs directly into a Pandas dataframe. In this post, I will show you how I did that and how I cleaned up the data for analysis.\nThe Data Source Several years worth of data are available in PDF form on the NEA website. Reading through the technical notes, they highlight that they did not collect all of their own salary information. Some states' information is calculated from the American Community Survey (ACS) done by the Census bureau - a great resource whose API I have covered in a different post. Each report includes accurate data for the previous school year, as well as estimates for the current school year. As of this post, the newest report is the 2020 report which includes data for the the 2018-2019 school year, as well as estimates of the 2019-2020 school year.\nThe 2020 report has the desired teacher salary information in two separate locations. One is in table B-6 on page 26 of the PDF, which shows a ranking of the different states' average salary in addition to the average salary:\nA second location is in table E-7 on page 46, which gives salary data for the completed school year as well as different states' estimates for the 2019-2020 school year:\nNote that table E-7 lacks the star-annotation marking NEA estimated values. This, and the lack of the ranking column, makes Table E-7 easier to parse. In the main example below, this will be the source of the five years of data. I will however also show how to parse table B-6 at the end of this post for completion.\nLoading the Data As of October 2020, the NEA site has five years worth of reports online. Unfortunately, these are not labeled consistently for all five years. Similarly the page numbers differ for each report. Prior to the 2018 report, inconsistent formats were used for the tables which require previous years to be parsed separately from the newer tables. For this reason, I\u0026rsquo;ll make a dictionary for the 2018-2020 reports only, which will simplify the example below.\nreport = { '2020' : { 'url' : \u0026quot;https://www.nea.org/sites/default/files/2020-10/2020%20Rankings%20and%20Estimates%20Report.pdf\u0026quot;, 'page' : 46, }, '2019' : { 'url' : \u0026quot;https://www.nea.org/sites/default/files/2020-06/2019%20Rankings%20and%20Estimates%20Report.pdf\u0026quot;, 'page' : 49, }, '2018' : { 'url' : \u0026quot;https://www.nea.org/sites/default/files/2020-07/180413-Rankings_And_Estimates_Report_2018.pdf\u0026quot;, 'page' : 51, }, }  We can now use dictionary comprehension to fill in a dictionary with all the source tables of interest. We will be using the tabula package to extract data from the PDFs. If you don\u0026rsquo;t have it installed, you can use pip install tabula-py to get a copy. The method that reads in a PDF is aptly called read_pdf. It\u0026rsquo;s first argument is a file path to the PDF. Since we want to use a URL, we will use the keyword argument stream=True and then name the specific page in each PDF that contains the information we are after. By default, read_pdf returns a list of dataframes, so we just save the first element from the list, which is the report we are interested in.\nNote: if you are using WSL, depending on your settings, you may the Exception in thread \u0026quot;main\u0026quot; java.awt.AWTError: Can't connect to X11 window server using '172.17.16.17:0' as the value of the DISPLAY variable. error when running read_pdf. This is fixed by having an X11 server running.\nimport tabula import pandas as pd source_df = {year : tabula.read_pdf(report[year]['url'], stream=True, pages=report[year]['page'])[0] for year in report.keys()}  And that\u0026rsquo;s it in principle. How cool is that! Of course, we still need to clean our data a little bit.\nCleaning the Data Let\u0026rsquo;s take a look at the first and last few entries of the 2020 report:\npd.concat([source_df['2020'].head(), source_df['2020'].tail()])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Unnamed: 0 2018-19 2019-20 From 2018-19 to 2019-20 From 2010-11 to 2019-20 (%)     0 State Salary($) Salary($) Change(%) Current Dollar Constant Dollar   1 Alabama 52,009 54,095 4.01 13.16 -2.58   2 Alaska 70,277 70,877 0.85 15.36 -0.69   3 Arizona 50,353 50,381 0.06 8.03 -7.00   4 Arkansas 49,438 49,822 0.78 8.31 -6.75   48 Washington 73,049 72,965 -0.11 37.86 18.69   49 West Virginia 47,681 50,238 5.36 13.51 -2.28   50 Wisconsin 58,277 59,176 1.54 9.17 -6.02   51 Wyoming 58,861 59,014 0.26 5.19 -9.44   52 United States 62,304 63,645 2.15 14.14 -1.73     We see that each column is treated as a string object (which you can confirm by running source_df['2020'].dtypes) and that the first row of data is actually at index 1 due to the fact that the PDF report used a two-row header. This means we can safely drop the first row of every dataframe. We can also drop the last row of every dataframe since that just contains summary data of the US as a whole, which we can easily regenerate as necessary. So row indices 0 and 52 can go for all of our data sets.\nfor df in source_df.values(): df.drop([0, 52], inplace=True)  Next up I\u0026rsquo;d like to fix the column names. The fist column is clearly the name of the state (except in the case of Washington D.C.), while the next two columns give the years for which the salary information is given. Let\u0026rsquo;s rename the second and third columns according to the pattern Salary %YYYY-YY using Python\u0026rsquo;s f-string syntax.\nfor df in source_df.values(): df.rename(columns={ df.columns[0] : \u0026quot;State\u0026quot;, df.columns[1] : f\u0026quot;Salary {str(df.columns[1])}\u0026quot;, df.columns[2] : f\u0026quot;Salary {str(df.columns[2])}\u0026quot;, }, inplace=True) source_df[\u0026quot;2020\u0026quot;].head() # show the result of our edits so far   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  State Salary 2018-19 Salary 2019-20 From 2018-19 to 2019-20 From 2010-11 to 2019-20 (%)     1 Alabama 52,009 54,095 4.01 13.16 -2.58   2 Alaska 70,277 70,877 0.85 15.36 -0.69   3 Arizona 50,353 50,381 0.06 8.03 -7.00   4 Arkansas 49,438 49,822 0.78 8.31 -6.75   5 California 83,059 84,659 1.93 24.74 7.39     Looks like we\u0026rsquo;re almost done! Let\u0026rsquo;s drop the unnecessary columns to and check our remaining column names:\nfor year, df in source_df.items(): df.drop(df.columns[3:], axis=1, inplace=True) print(f\u0026quot;{year}:\\t{df.columns}\u0026quot;)  2020:\tIndex(['State', 'Salary 2018-19', 'Salary 2019-20'], dtype='object') 2019:\tIndex(['State', 'Salary 2017-18', 'Salary 2018-19'], dtype='object') 2018:\tIndex(['State', 'Salary 2017', 'Salary 2018'], dtype='object')  We can see that the column naming scheme in 2018 was different than in the previous reports. To make them all compatible for our merge, we\u0026rsquo;re going to have to do some more editing. Based on the other reports, it appears as though the 2018 report used the calendar year of the end of the school year, while the others utilized a range. This can easily be solved using regex substitution. We\u0026rsquo;ll do that now.\nimport re for year, df in source_df.items(): if year != \u0026quot;2018\u0026quot;: df.rename(columns={ df.columns[1] : re.sub(r\u0026quot;\\d{2}-\u0026quot;, '', df.columns[1]), df.columns[2] : re.sub(r\u0026quot;\\d{2}-\u0026quot;, '', df.columns[2]), }, inplace=True) # always print the output for verification print(f\u0026quot;{year}:\\t{df.columns}\u0026quot;)  2020:\tIndex(['State', 'Salary 2019', 'Salary 2020'], dtype='object') 2019:\tIndex(['State', 'Salary 2018', 'Salary 2019'], dtype='object') 2018:\tIndex(['State', 'Salary 2017', 'Salary 2018'], dtype='object')  Now that everything works, we can do our merge to create a single dataframe with the information for all of the school years we have downloaded.\nmerge_df = source_df[\u0026quot;2018\u0026quot;].drop([\u0026quot;Salary 2018\u0026quot;], axis=1).merge( source_df[\u0026quot;2019\u0026quot;].drop([\u0026quot;Salary 2019\u0026quot;], axis=1)).merge( source_df[\u0026quot;2020\u0026quot;]) merge_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  State Salary 2017 Salary 2018 Salary 2019 Salary 2020     0 Alabama 50,391 50,568 52,009 54,095   1 Alaska 6 8,138 69,682 70,277 70,877   2 Arizona 4 7,403 48,723 50,353 50,381   3 Arkansas 4 8,304 50,544 49,438 49,822   4 California 7 9,128 80,680 83,059 84,659     Numeric Conversion We\u0026rsquo;re almost done! Notice that we still have not dealt with the fact that every column is still treated as a string. Before we can use the to_numeric function, we still need to take care of two issues:\n The commas in the numbers. While they are nice for us human eyes, Pandas doesn\u0026rsquo;t like them. In the 2017 salary column, there appears to be extraneous white space after the first digit for some entries.  Luckily, both of these problems can be remedied with a simple string replacement operation.\nmerge_df.iloc[:,1:] = merge_df.iloc[:,1:].replace(r\u0026quot;[,| ]\u0026quot;, '', regex=True) for col in merge_df.columns[1:]: merge_df[col] = pd.to_numeric(merge_df[col])  Now we\u0026rsquo;re done! We have created an overview of annual teacher salaries from the 2016-17 school year until 2019-20 extracted from a series of PDFs published by the NEA. We have cleaned up the data and converted everything to numerical values. We can now get summary statistics and do any analysis of interest with this data.\nmerge_df.describe() # summary stats of our numeric columns   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Salary 2017 Salary 2018 Salary 2019 Salary 2020     count 51.000000 51.000000 51.000000 51.000000   mean 56536.196078 57313.039216 58983.254902 60170.647059   std 9569.444674 9795.914601 10286.843230 10410.259274   min 42925.000000 44926.000000 45105.000000 45192.000000   25% 49985.000000 50451.500000 51100.500000 52441.000000   50% 54308.000000 53815.000000 54935.000000 57091.000000   75% 61038.000000 61853.000000 64393.500000 66366.000000   max 81902.000000 84227.000000 85889.000000 87543.000000     Table B-6 As mentioned above, table B-6 in the 2020 Report presents slightly greater challenges. A lot of the cleaning is similar or identical, so I will not reproduce it in full. Instead, I have created loaded and subsetted part of table B-6 and I will show how this can be cleaned up as well. But first, let\u0026rsquo;s look at the first several entries:\nb6   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Unnamed: 0 2017-18 (Revised) 2018-19     0 State Salary($) Rank Salary($)   1 Alabama 50,568 36 52,009   2 Alaska 69,682 7 70,277   3 Arizona 48,315 45 50,353   4 Arkansas 49,096 44 49,438   5 California 80,680 2 83,059 *   6 Colorado 52,695 32 54,935   7 Connecticut 74,517 * 5 76,465 *   8 Delaware 62,422 13 63,662     We can see that there is an additional hurdle compared to the previous tables: the second column now contains data from two columns, both the Salary information as well as a ranking of the salary as it compares to the different states. For a few states, there is additionally a \u0026lsquo;*\u0026rsquo; to denote values that were estimated as opposed to received. We can again use a simple regex replace together with a capture group to parse out only those values that we are interested in, while dropping the extraneous information using the code below.\nb6.iloc[:,1:] = b6.iloc[:,1:].replace(r\u0026quot;([\\d,]+).*\u0026quot;, r\u0026quot;\\1\u0026quot;, regex=True)  And now we\u0026rsquo;re back to where we were above before we did the string conversion. This is what it looks like after also dropping the first row and renaming the columns:\nb6   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  State Salary 2018 Salary 2019     1 Alabama 50,568 52,009   2 Alaska 69,682 70,277   3 Arizona 48,315 50,353   4 Arkansas 49,096 49,438   5 California 80,680 83,059   6 Colorado 52,695 54,935   7 Connecticut 74,517 76,465   8 Delaware 62,422 63,662     From here on out, we can proceed as in the previous example.\n","date":1604025540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604025540,"objectID":"c5eca41aa0719561a38b4163c276f4ab","permalink":"https://dmsenter89.github.io/post/20-10-tabula/","publishdate":"2020-10-29T22:39:00-04:00","relpermalink":"/post/20-10-tabula/","section":"post","summary":"What do you do when your data table is in PDF format? Let's use tabula-py to extract teacher salary information from PDFs directly into Pandas dataframes. We'll also use some regex to clean up the results.","tags":[],"title":"Teacher Salaries","type":"post"},{"authors":["D. Michael Senter","Dylan Ray Douglas","W. Christopher Strickland","Steven G. Thomas","Anne M Talkington","Laura Miller"],"categories":[],"content":"","date":1596468475,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596468475,"objectID":"f53c5f8137d2c17ac968dc30b8fb6a58","permalink":"https://dmsenter89.github.io/publication/senter-2020-meshmerizeme/","publishdate":"2020-08-05T11:27:55-04:00","relpermalink":"/publication/senter-2020-meshmerizeme/","section":"publication","summary":"Numerous fluid-structure interaction problems in biology have been investigated using the immersed boundary method. The advantage of this method is that complex geometries, e.g., internal or external morphology, can easily be handled without the need to generate matching grids for both the fluid and the structure. Consequently, the difficulty of modeling the structure lies often in discretizing the boundary of the complex geometry (morphology). Both commercial and open source mesh generators for finite element methods have long been established; however, the traditional immersed boundary method is based on a finite difference discretization of the structure. Here we present a software library for obtaining finite difference discretizations of boundaries for direct use in the 2D immersed boundary method. This library provides tools for extracting such boundaries as discrete mesh points from digital images. We give several examples of how the method can be applied that include passing flow through the veins of insect wings, within lymphatic capillaries, and around starfish using open-source immersed boundary software.","tags":[],"title":"A semi-automated finite difference mesh creation method for use with immersed boundary software IB2d and IBAMR","type":"publication"},{"authors":[],"categories":null,"content":"This workshop covers data acquisition and basic data preparation with a focus on using Python with Jupyter Notebooks. To avoid having to install Python locally during the workshop, we will be utilizing an Azure notebook project. The example files are located here.\nPlease note that the free Azure notebooks will only be available until early October. To continue using Python and Jupyter notebooks, you may want to consider using a local installation. For Windows and Mac users, I recommend using Anaconda. For continued cloud usage, you may consider Cocalc. Please note that you will need a subscription for your Cocalc notebooks to be able to download data from external sources.\nAdditional Links:\n  Engauge Digitizer (software to extract data points from graphs).  Markdown Cheatsheet.  ","date":1596128400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596128400,"objectID":"80d634f592a1f22b7ec5837df0dace9d","permalink":"https://dmsenter89.github.io/talk/webscraping-tutorial/","publishdate":"2020-07-27T14:10:17-04:00","relpermalink":"/talk/webscraping-tutorial/","section":"talk","summary":"Data acquisition is a key step in research. In this workshop, we will consider how to effectively access publicly available data sets. We will discuss how to find and load data published in CSV/Excel formats.  We will learn how to use Pandas to parse HTML tables. We will discuss some best practises for data acquisition and storage.","tags":[],"title":"Basics of Web Scraping with Python","type":"talk"},{"authors":[],"categories":[],"content":"Basics of Web Scraping with Python Michael Senter\n Goals for Today  Understand what tools and methods are available.  Be able to create a new project using Python and Jupyter.  Be able to edit existing code snippets to gather data.    Python  easy to learn, reads like \u0026ldquo;pseudocode\u0026rdquo; widely used in a variety of fields many books, websites, etc. to help you learn  print(\u0026quot;Hello, world!\u0026quot;)   Data Sources  CSV/Excel Downloads  COVID Related Data  Johns Hopkins Dashboard The Johns Hopkins data is published on GitHub and is updated regularly.\n Using SAS filename outfile \u0026quot;~/import-data-nyt.sas\u0026quot;; /* download official SAS script to above filename */ proc http url=\u0026quot;https://raw.githubusercontent.com/sassoftware/covid-19-sas/master/Data/import-data-nyt.sas\u0026quot; method=\u0026quot;get\u0026quot; out=outfile; run; /* run the downloaded script */ %include \u0026quot;~/import-data-nyt.sas\u0026quot;; /* state and county level data are now in memory */  ","date":1596126600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596126600,"objectID":"4e6dec377db9b7f6c1777b9311af259c","permalink":"https://dmsenter89.github.io/slides/webscraping-tutorial/","publishdate":"2020-07-30T12:30:00-04:00","relpermalink":"/slides/webscraping-tutorial/","section":"slides","summary":"Basics of Web Scraping with Python Michael Senter\n Goals for Today  Understand what tools and methods are available.  Be able to create a new project using Python and Jupyter.","tags":[],"title":"Webscraping Tutorial","type":"slides"},{"authors":[],"categories":[],"content":"My website is back up and running! Some incompatabilities between my old site and updates to both Hugo and the Academic Theme have led to some downtime on this pageas I didn\u0026rsquo;t have time to look through how to rebuild my site without loosing previous content. I\u0026rsquo;m currently in the process of updating everything and will try to bring back some material as well. Stay tuned!\n","date":1595863867,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595863867,"objectID":"92d904527521646e80dcda8e55bf841f","permalink":"https://dmsenter89.github.io/post/porting-forward/","publishdate":"2020-07-27T11:31:07-04:00","relpermalink":"/post/porting-forward/","section":"post","summary":"My website is back up and running! Some incompatabilities between my old site and updates to both Hugo and the Academic Theme have led to some downtime on this pageas I didn\u0026rsquo;t have time to look through how to rebuild my site without loosing previous content.","tags":[],"title":"Porting Forward","type":"post"},{"authors":[],"categories":null,"content":"","date":1587128400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587128400,"objectID":"d4925dbe1b1b4dd4dad7517dcbe7fbcf","permalink":"https://dmsenter89.github.io/talk/thesis-proposal/","publishdate":"2020-04-15T08:00:00-04:00","relpermalink":"/talk/thesis-proposal/","section":"talk","summary":"","tags":[],"title":"Thesis Proposal","type":"talk"},{"authors":[],"categories":[],"content":"Insects are ubiquitious throughout the world. Most of us are familiar with winged insects such as butterflies and bees. Insect flight is an interesting topic from a biomechanics perspective. Unlike birds, most insects (with some eceptions, such as dragonflies and others) do not have flight muscles attached to their wings. Instead, their flight muscles oscillate their thorax, which in turn makes the wings move. The aerodynamics of insect flight are also very interesting. Larger insects are able to fly by creating a leading edge vortex. This method does not work in the smallest insect fliers. Such insects include the thrips and chalcid wasps, some of which have wingspans as small as 1 mm. These insects have unusual wing structures, as can be seen in this image:\nThe solid part of the wing is rather small and narrow, with many large bristles projecting from the solid part of the wing. Insects such as thrips do not create a leading edge vortex; instead, they fly using the \u0026ldquo;Clap-and-Fling\u0026rdquo; method. This method is common amongst insects who fly in the intermediate Reynolds number regime, $1\\leq \\mathrm{Re} \\leq 100$.\n","date":1527866603,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527866603,"objectID":"047eb4aa355fddd02ce6989411f5488b","permalink":"https://dmsenter89.github.io/project/clap-and-fling/","publishdate":"2018-06-01T11:23:23-04:00","relpermalink":"/project/clap-and-fling/","section":"project","summary":"Simulating the aerodynamics of the smallest insect fliers.","tags":[],"title":"Clap and Fling","type":"project"},{"authors":[],"categories":[],"content":" IB2d and IBAMR are two software packages implementing the immersed boundary method (see below). These packages model fluid-structure interaction problems based on user given parameters and geometry. The manual creation of the initial geometry mesh can be difficult and time consuming, especially for the complex shapes encountered in biological applications. Oftentimes we have images of the geometry we wish to explore. I am developing software to help automate the creation of such CFD meshes for 2D simulations with a file-format suitable for use with IB2d and IBAMR from images. An initial prototype version is available on Github. A paper exploring the use of MeshmerizeMe in conjuction with IB2d for simulations is in preparation.\nUsage MeshmerizeMe needs two input files per experimental geometry: an SVG image file with the geometry of interest and an input2d file with the experiment parameters. When selecting an SVG for use with MeshmerizeMe it will automatically look for the input2d file in the same folder. It will then parse the paths, transform them into the correct coordinate system and appropriately sample the paths based on the size of the Cartesian grid set in the input2d file. The geometry will be exported as a vertex file. This file is readable by both IB2d and IBAMR.\nSVGs were chosen as the image source as the are an open, text-based format making them very accesible to work with. They are standardized for web use and many tools exist for creating and manipulating SVG images. They can be created from source images such as photographs or scans by means of edge detection tools and by manually tracing the outline of a shape of interest Consider optimizing the SVG prior to processing to save time.\nAs the current version of MeshmerizeMe only handles a subset of SVG, tools that optimize the SVG files created by your editor are very useful. Examples of such software include SVGO, which also offers a webapp called SVGOMG. Another software is svgcleaner.\nIBM Background One aspect of computational fluid dynamics is the investigation of fluid-structure interactions. One method developed for the study of such interactions is the immersed boundary method (IBM) developed by Peskin1. It is well known that fluids can be studied from both a Eulerian and a Lagrangian view. The IBM combines these - the domain of the problem is resolved as a Cartesian grid on which Eulerian equations are solved for fluid velocity and pressure. In the case of Newtonian fluids the incompressible Navier-Stokes equations comprising of\n$$ \\rho \\left( \\frac{\\partial \\mathbf{u}}{\\partial t} + \\mathbf{u} \\cdot \\nabla \\mathbf{u} \\right) = - \\nabla \\mathbf{p} + \\mu \\nabla^2 \\mathbf{u} + \\mathbf{f}$$\nand\n$$\\nabla \\cdot \\mathbf{u} = 0$$\nneed to be solved.\nThe immersed structures are modeled as fibers in the form of parametric curves $X(s,t)$, where $s$ is a parameter and $t$ is time. The fiber experiences force distributions $F(s,t)$, and we can derive the force the fiber exerts on the fluid from the momentum equation. For the fibers we then solve\n$$\\mathbf{f} = \\int_\\Gamma \\mathbf{F}(s,t),\\delta\\left(\\mathbf{x}-\\mathbf{X}(s,t)\\right),ds$$\nand\n$$\\frac{\\partial \\mathbf{X}}{\\partial t} = \\int_\\Omega \\mathbf{u}(\\mathbf{x},t), \\delta \\left( \\mathbf{x}-\\mathbf{X}(s,t)\\right),d\\mathbf{x}.$$\nHere, $\\Gamma$ is the immersed structure and $\\Omega$ is the fluid domain.\nThe immersed structures are discretized not on a Cartesian grid but on a separate Lagrangian grid on the fiber itself. Of import to CFD software users is that the initial discretization of the immersed structure has to be supplied by the user. While this is not too difficult for simple geometries, the often complex structures encountered in mathematical biology can present a significant time investment. This is the part where MeshmerizeMe comes in handy.\n  Charles S Peskin. 2002. \u0026ldquo;The immersed boundary method.\u0026rdquo; Acta numerica 11:479-517. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1505921706,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505921706,"objectID":"75d971656dea94655081afc1899e8ab1","permalink":"https://dmsenter89.github.io/project/meshmerizeme/","publishdate":"2017-09-20T11:35:06-04:00","relpermalink":"/project/meshmerizeme/","section":"project","summary":"Automatic mesh generation from 2D images for use with immersed boundary solvers.","tags":[],"title":"MeshmerizeMe","type":"project"},{"authors":["C Hohenegger","R Durr","DM Senter"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"424a48dfbcfb4647e011e675044fd04a","permalink":"https://dmsenter89.github.io/publication/hohenegger-2017-mean/","publishdate":"2020-07-27T15:40:59.097696Z","relpermalink":"/publication/hohenegger-2017-mean/","section":"publication","summary":"The motion of a passive spherical particle in a fluid has been widely described via a balance of force equations known as a Generalized Langevin Equation (GLE) where the covariance of the thermal force is related to the time memory function of the fluid. For viscous fluids, this relationship is simply a delta function in time, while for a viscoelastic fluid it depends on the constitutive equation of the fluid memory function. In this paper, we consider a general setting for linear viscoelasticity which includes both solvent and polymeric contributions, and a family of memory functions known as the generalized Rouse kernel. We present a statistically exact algorithm to generate paths which allows for arbitrary large time steps and which relies on the numerical evaluation of the covariance of the velocity process. As a consequence of the viscoelastic properties of the fluid, the particle exhibits subdiffusive behavior, which we verify as a function of the free parameters in the generalized Rouse kernel. We then numerically compute the mean first passage time of a passive particle through layers of different widths and establish that, for the generalized Rouse kernel, the mean first passage time grows quadratically with the layer’s width independently of the free parameters. Along the way, we also find the linear scaling of the mean first passage time for a layer of fixed width as a function of the particle’s radius.","tags":null,"title":"Mean first passage time in a thermally fluctuating viscoelastic fluid","type":"publication"}]