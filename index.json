[{"authors":["admin"],"categories":null,"content":"Fluid dynamics modeling and simulations, data analysis, and machine learning have been the major themes of Michael\u0026rsquo;s academic endeavors. The through line of his career has been the utilization of coding to solve computationally complex problems.\nMichael enjoys mentoring and teaching. His teaching philosophy is based on his own experience - that a passion for math and computer science can be cultivated through active learning and emphasizing small victories.\nMichael is originally from Nuremberg, Germany where he attended the Neues Gymnasium Nuernberg, a school following the humanist tradition of education. He now lives in Carrboro with his family, where he enjoys travelling, collecting books, and reading nerdy webcomics in his spare time.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dmsenter89.github.io/author/d.-michael-senter/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/d.-michael-senter/","section":"authors","summary":"Fluid dynamics modeling and simulations, data analysis, and machine learning have been the major themes of Michael\u0026rsquo;s academic endeavors. The through line of his career has been the utilization of coding to solve computationally complex problems.","tags":null,"title":"D. Michael Senter","type":"authors"},{"authors":[],"categories":["missing-data"],"content":"I\u0026rsquo;m excited to announce that the new SAPy v4.6.0 release includes a pull request of mine that adds PROC MI to the SAS/STAT procedures directly exposed in SASPy. This procedure allows you to analyze missing data patterns and create imputations for missing data.\nSyntax PROC MI is accessed via the mi function that has been added to the SASstat class. Like other procedures, the SAS statements in MI are called as keyword arguments to the function whose name matches the SAS syntax:1\nPROC MI options; BY variables; CLASS variables; EM \u0026lt;options\u0026gt;; FCS \u0026lt;options\u0026gt;; FREQ variable; MCMC \u0026lt;options\u0026gt;; MNAR options; MONOTONE \u0026lt;options\u0026gt;; TRANSFORM transform (variables\u0026lt;/ options\u0026gt;) \u0026lt;â€¦transform (variables\u0026lt;/ options\u0026gt;)\u0026gt;; VAR variables;  Here is the corresponding function signature in Python:\ndef mi(self, data: ('SASdata', str) = None, by: (str, list) = None, cls: (str, list) = None, em: str = None, fcs: str = None, freq: str = None, mcmc: str = None, mnar: str = None, monotone: str = None, transform: str = None, var: str = None, procopts: str = None, stmtpassthrough: str = None, **kwargs: dict) -\u0026gt; 'SASresults':  Statements like EM or MCMC, which can stand alone in SAS, are called with an empty string argument in Python.\nBasic Example    # ending the SAS session sas.endsas() ``` -- To use the new MI functionality, make sure you have updated to the newest SASPy release. In addition to starting a SAS Session as per usual, you will also want to enable access to the SAS/STAT procedures:\nsas = saspy.SASsession() # loads a session using your default profile stat = sas.sasstat() # gives access to SAS/STAT procedures  Once these session objects are loaded, you can start using the mi function with stat.mi. The simplest possible call is to invoke MI with a built-in data set and all defaults as stat.mi(data='sashelp.heart'). For best results, store the output in a SASResults object. From there you can access the SAS log associated with the function call (LOG) as well as all ODS Output using the ODS table names in all caps. The default uses the EM method with 25 imputations.\nA more realistic use might look something like this:\nods = stat.mi(data='sashelp.heart', em=\u0026quot;outem=outem\u0026quot;, var=\u0026quot;Cholesterol Height Smoking Weight\u0026quot;, procopts=\u0026quot;simple nimpute=20 out=imp\u0026quot;)  This is equivalent to running\nproc mi data=sashelp.heart simple nimpute=20 out=imp; em outem=outem; var Cholesterol Height Smoking Weight; run;  in SAS. This call uses the EM procedure to impute values for the cholesterol, height, smoking, and weight variables. The simple option displays univariate statistics and correlations. The outem option saves a data set containing the computed MLE to work.outem. The imputed data sets are saved to work.imp, which contains the additional variable _IMPUTATION_ with the imputation number. This can be used as a by variable in other procedures, and the results can later be pooled using PROC MIANALYZE.\nThe resulting ods object for our example exposes the following ODS outputs to your Python instance, in addition to the log:\n['CORR', 'EMESTIMATES', 'EMINITESTIMATES', 'EMPOSTESTIMATES', 'MISSPATTERN', 'MODELINFO', 'PARAMETERESTIMATES', 'UNIVARIATE', 'VARIANCEINFO']  See the SAS documentation for details. To use the imputed data with Python tools, create a SAS data object. We\u0026rsquo;ll also print the first few entries so we can see what it looks like:\nimputed = sas.sasdata(table=\u0026quot;imp\u0026quot;, libref=\u0026quot;work\u0026quot;) imputed.head()    One exception is the SAS class statement, which is implemented as cls due to class being a reserved keyword in Python. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1675712700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675712700,"objectID":"587616efd4cb247ecebc436c90516dfd","permalink":"https://dmsenter89.github.io/post/23-02-proc-mi-added-to-saspy/","publishdate":"2023-02-06T14:45:00-05:00","relpermalink":"/post/23-02-proc-mi-added-to-saspy/","section":"post","summary":"I\u0026rsquo;m excited to announce that the new SAPy v4.6.0 release includes a pull request of mine that adds PROC MI to the SAS/STAT procedures directly exposed in SASPy. This procedure allows you to analyze missing data patterns and create imputations for missing data.","tags":["missing-data","python","sas","saspy"],"title":"PROC MI Added to SASPy","type":"post"},{"authors":[],"categories":[],"content":"ChatGPT has been all over my newsfeed lately, with a considerable amount of hype. In particular, many are wondering or even worrying whether the emergence of this technology will threaten jobs with moderate to high education requirments. See for example \u0026ldquo;How ChatGPT Will Destabilize White-Collar Work\u0026rdquo; (The Atlantic), where Annie Lowrey leads with \u0026ldquo;In the next five years, it is likely that AI will begin to reduce employment for college-educated workers.\u0026rdquo; I do not share these views. In fact, I am somewhat underwhelmed by the threat of ChatGPT for a number of reasons. Since this topic has come up a few times for me lately, I will write down my thoughts here so I can reference them more easily.\nChatGPT Cannot Think The first issue I take with many of the AI hype articles is that despite what the news coverage may imply, ChatGPT cannot think. To be honest, when I see articles talking about ChatGPT as \u0026ldquo;intelligent\u0026rdquo; or \u0026ldquo;thinking,\u0026rdquo; the first thing that comes to mind is this SMBC from 2011-08-17:\nIn my view, ChatGPT is a lot like this parrot - except that I do think it is fundamentally different, and ChatGPT is not \u0026ldquo;conscious\u0026rdquo; and does not \u0026ldquo;think\u0026rdquo; in a meaningful way. Despite the many advances made, artificial intelligence (AI) functions differently than a natural intelligence (NI), and in any ChatGPT is not designed to \u0026ldquo;think.\u0026rdquo;\nBefore giving a big picture view of how a large language model like ChatGPT works, I want to illustrate the limited flexibility of AI with an example from image recognition. An NI can readily distinguish between what is in the foreground and background of an image. Think of an image like this:\nA human will have no problem distinguishing between the insect in the image, the flower it is on, and the foliage in the background as distinct objects in different planes. This holds true even if the individual is not familiar with the particular plant or insect in the image. If given additional images of either the insect on a different background or the same background without the insect, we would not mistake the the plant for the insect or the other way around.\nNow consider an AI model trained to recognize insects. The algorithm doesn\u0026rsquo;t have a concept of \u0026ldquo;insect\u0026rdquo; or \u0026ldquo;plant,\u0026rdquo; per se. Rather, it notices patterns in images that are labeled \u0026ldquo;insect\u0026rdquo; or labeled with a particular insect. The pattern it learns does not depend on it having a concept of \u0026ldquo;insect.\u0026rdquo; What that means in practice, is that our model might learn that the background is equally or even more important than the foreground. If we train our data set with bees on flowers, but not flowers without bees, we may end up with a model that declares flower photos \u0026ldquo;bees.\u0026rdquo; This phenomeon is known in image recognition, and people are actively working on methods around this problem. But it nicely illustrates how AI is not \u0026ldquo;smart,\u0026rdquo; and humans need to do a lot of heavy lifting to get the AI algorithm to perform as intended, even if the application domain is relatively limited. For more information on this application, see this article from GradientScience.\nHow does it Work? With this background, let\u0026rsquo;s get an overview of how models like ChatGPT work. A good summary of the techniques involved is detailed in this post by AssemblyAI. In simple terms, a model is exposed to large amounts of data in order to learn about the structure of words and how the are aligned in sentences. In principle, this is not too different from the text prediction feature you have on your phone while texting. But this methodology only works to help produce coherent or seemingly coherent sentences by completion. Marked language modeling is a method use to help the model learn about syntax as well to improve the output.\nWhat is new with ChatGPT is that in addition labeled training material, it utilizes human feedback to improve its output. Deep down, AI models can be thought of as optimizing some (very complicated) function. This goal function need not necessarily be written down explicitly. OpenAI uses a method where a model gives two possible outputs for a prompt, and then a human judges which is \u0026ldquo;better,\u0026rdquo; somewhat similar to when an optometrist asks you if \u0026ldquo;1\u0026rdquo; or \u0026ldquo;2\u0026rdquo; is better. It then uses this feedback to improve its output iteratively. See this blog post from OpenAI where they use this methodology to animate a backflip.\nChatGPT uses three steps for human feedback based reinforcement learning. You can already imagine some of the issues that can arise from using this method. For one, if human feedback is used to train the model, then we can expect the model to reflect the thoughts and opinions of the labelers to some degree. Labelers may be mistaken and might not be experts in whatever topic they are reviewing. They may be fundamentally mistaken or biased about what we would consider high school-level knowledge.1 This is on top of the issues of the large amount of source text used in the initial training phase. These source texts may vary wildly in style and accuracy. Even humans reviewing an article may not be able to distinguish facts from opinion, let alone a language model using many source texts as input. Which leads us to what I see as a main problem for ChatGPT.\nFactual Inaccuracies Despite the confidence exuded by ChatGPTs output, it will readily produce a number of factual inaccuracies or give bad advice when explaining how to do tasks. See for example Avram Piltch\u0026rsquo;s \u0026ldquo;I Asked ChatGPT How to Build a PC. It Told Me to Break My CPU\u0026rdquo; (Tom\u0026rsquo;s Hardware), where ChatGPT gives instructions for a computer assembly that is potentially damaging to the hardware.\nOr this article (ToolGuyd) where Stuart asked ChatGPT to recommend a cordless powerdrill. ChatGPT made three recommendations. In explaining its recommendations, it gave several tech specs about the recommended products. The only problem is that it got several of these items wrong. It made mistakes about what type of drill a particular model was, whether the battery is included in the particular SKU it listed or not, and how many BPM the model delivers. It also recommended a discontinued model.\nAs a third example, consider this post where economics professor Bryan Caplan attempts to let ChatGPT take one his more recent midterms. It\u0026rsquo;s quite detailed and includes the questions, answers, and grading rubric Bryan used. He gave ChatGPT a D on this exam, substantially below the average grade human students in the class received.\nI would like to highlight that my argument isn\u0026rsquo;t that ChatGPT gets everything wrong - it doesn\u0026rsquo;t. It can even perform exceptionally well at certain tasks. See this white paper by Christian Terwiesch grading ChatGPT\u0026rsquo;s attempt at the final exam Wharton Business School MBA core course for just one example. A little googling will quickly lead to other examples, such as it passing law school exams or giving decent answers to tech sector interview questions.\nMy concern is that it sounds very confident in its answers, but it is not always trivial for the average person to verify whether or not ChatGPT\u0026rsquo;s output is trustworthy. As Rupert Goodwin put it, ChatGPT is \u0026ldquo;a Dunning-Kruger effect knowledge simulator par excellence.\u0026rdquo; And that\u0026rsquo;s a problem if people decide to just trust it to produce truth, when ChatGPT has no idea what \u0026ldquo;truth\u0026rdquo; is. It\u0026rsquo;s important to know that OpenAI is aware of this and it even says so on it\u0026rsquo;s FAQ page:\n Can I trust that the AI is telling me the truth?  a. ChatGPT is not connected to the internet, and it can occasionally produce incorrect answers. It has limited knowledge of world and events after 2021 and may also occasionally produce harmful instructions or biased content.\nWe\u0026rsquo;d recommend checking whether responses from the model are accurate or not. If you find an answer is incorrect, please provide that feedback by using the \u0026ldquo;Thumbs Down\u0026rdquo; button.\n In my opinion this is reasonable and to be expected. I think some people may get too excited and feel too confident in this technology when it just isn\u0026rsquo;t as reliable as many would wish at this stage. And for those reasons, I don\u0026rsquo;t think it\u0026rsquo;s coming for our jobs any time soon.\nNote:: If you use ChatGPT, be careful to not give it any sensitive information. OpenAI isn\u0026rsquo;t making this very expensive model available to you for free out of the goodness of their hearts. They\u0026rsquo;re using your interaction with it to further train the model.\n  For a good review of the many ways in which typical adults are uninformed and mistaken about issues contra accepted expert opinion, see: B. Caplan, The Myth of the Rational Voter: Why Democracies Choose Bad Policies, Princeton University Press, Princeton, NJ, 2007. And B. Caplan, The Case against Education: Why the Education System Is a Waste of Time and Money, Princeton University Press, Princeton, NJ, 2019. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1674838620,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674838620,"objectID":"93805e416dd9894b7c4bdb27611e348d","permalink":"https://dmsenter89.github.io/post/23-01-why-im-not-worried-about-chatgpt/","publishdate":"2023-01-27T16:57:00Z","relpermalink":"/post/23-01-why-im-not-worried-about-chatgpt/","section":"post","summary":"ChatGPT has been all over my newsfeed lately, with a considerable amount of hype. In particular, many are wondering or even worrying whether the emergence of this technology will threaten jobs with moderate to high education requirments.","tags":["ai","chatgpt","machine-learning","opinion"],"title":"Why I'm Not Worried About ChatGPT","type":"post"},{"authors":[],"categories":[],"content":"During December 2022, SAS ODA received substantial updates - see the upgrade page for details. It\u0026rsquo;s really nice to see that ODA is now using SAS 9.4M7. If you are a SASPy user, you may now bump into an error while logging in with your existing configuration. The specific error I encountered was \u0026ldquo;An exception was thrown during the encryption key exchange.\u0026rdquo; Nothing is wrong with your password, however. Due to changes with the AES encryption, SASPy will now need access to 3 encrpytion JARs in its classpath. See this note in the official SASPy docs. Download the required JAR files here (requires login) and add them to your SASPy package\u0026rsquo;s path here:\npath/to/python/site-packages/saspy/java/iomclient/  Make sure your JAR files are set to executable and you\u0026rsquo;ll be good to go again.\n","date":1673277600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673277600,"objectID":"c89fe56249592039012972b3d5a7608e","permalink":"https://dmsenter89.github.io/post/23-01-sas-oda-update-saspy-impact/","publishdate":"2023-01-09T15:20:00Z","relpermalink":"/post/23-01-sas-oda-update-saspy-impact/","section":"post","summary":"During December 2022, SAS ODA received substantial updates - see the upgrade page for details. It\u0026rsquo;s really nice to see that ODA is now using SAS 9.4M7. If you are a SASPy user, you may now bump into an error while logging in with your existing configuration.","tags":["oda","saspy"],"title":"Dec22 SAS ODA Update - Impact on SASPy Users","type":"post"},{"authors":[],"categories":[],"content":"Dealing with computer resources in a modern lab can be tricky. Even if all participating researchers have laptops, a central location for storage or to host licensed software is desirable. While a physical computer can be setup for such a use, that is not always the most desirable solution. We want multiple people to have concurrent access to our resources while providing safe, sandboxed environments. Sometimes lab members want/need root access to learn certain tasks, but we don\u0026rsquo;t want them to accidentally take down our carefully configured systems. This leads us to the idea of containerization which can provide various failsafes. In this post, we will be setting up the LXD (ArchWiki) environment as a virtual lab computer. This solution gives an entire working system, including systemd, similar to but more lightweight than VirtualBox.\nSetting up a successful virtual computer is very similar to setting up a regular Linux machine, with some minor LXD overhead. Note that only the individual administering the containers will need to deal with that LXD overhead. From the point of view of the end user, it\u0026rsquo;ll look the same as if they were interacting with a \u0026ldquo;regular\u0026rdquo; computer. This post deals with the setup from the point of view of the admin. The lab members should be setup as users inside the container and can then SSH into the container or use VNC if a GUI is needed, similar to how they interact with a regular remote computer.\nTable of Contents  Install and Setup of LXD Container Setup  Getting the Image Basic Container Management Setting up the Container   Networking  Giving the Container Access to the Internet Network Forwarding   Getting a GUI Running   Install and Setup of LXD LXD can be installed from a snap with sudo snap install lxd, but that requires you to have snap running. On Arch, LXD is available in the repos with pacman -S lxd. To get a RPM install in Fedora, you\u0026rsquo;ll need to use an additional COPR repository like this:\ndnf copr enable ganto/lxc4 dnf install lxd  Once installed, you\u0026rsquo;ll need to either enable the lxd.socket or lxd.service (if you want instances to be able to autostart). You\u0026rsquo;ll want to modify the subuid and subgid files so you can run unpriviliged containers (recommended), e.g.:\n# for root user and systemd: usermod -v 1000000-1000999999 -w 1000000-1000999999 root  With this done, run lxd init to go through a configuration guide for your new setup. If this is your first time using LXD, you will likely be fine just using the default settings, except maybe the size of the storage pool - but you can always attach other storage to your containers later, so if you will run tasks producing a lot of data you might want to consider just mounting a dedicated filesystem later. If you have multiple computers available in your lab, you might want to consider turning on clustering (documentation).\nFor more details, see the ArchWiki or the Official Getting Started Guide.\nContainer Setup Getting the Image The first step in setting up a container is picking a suitable image to start from. Similar to Docker, many distributions are available to chose from. There are also arm and amd64 images available, so you can pick what works with your platform. To list available images on the image server, use the syntax lxc image list images:\u0026lt;keyword\u0026gt;.\n# ArchLinux images: lxc image list images:archlinux amd64 # Fedora images, using key/value pairs: lxc image list images:fedora arch=amd64 type=container  To create a new image without starting it, use lxc init \u0026lt;image\u0026gt; \u0026lt;container-name\u0026gt;. To both initialize and start a new container, use lxc init \u0026lt;image\u0026gt; \u0026lt;container-name\u0026gt;. For example:\n# create a base image called myarch without starting: lxc init images:archlinux myarch # you can also specify version and arch, e.g. Fedora 36 / 64bit: lxc init images:fedora/36/amd64 myfedora # create and launch an image: lxc init images:rockylinux/9 myrocky  Note that you can have a large number of concurrent containers in use, which may but need not share the same base image. This can be useful for larger teams, where you can setup systems for particular tasks or projects. For example, you could have a main machine for your graduate students, a separate one for people moving in and out of the lab like REU students, and a third container for a class that you\u0026rsquo;re teaching to use.\nBasic Container Management # Starting, stopping etc. is intuitive lxc start \u0026lt;container\u0026gt; # starts container lxc stop \u0026lt;container\u0026gt; [--force] # stops the container lxc restart \u0026lt;container\u0026gt; [--force] # restart lxc pause \u0026lt;container\u0026gt; # send SIGSTOP to all container processes # what containers do I have? lxc list lxc info myarch # get detailed info about this container lxc copy \u0026lt;name1\u0026gt; \u0026lt;name2\u0026gt; # make a copy of an existing container lxc delete \u0026lt;container\u0026gt; [--force] # edit container configuration lxc config edit \u0026lt;container\u0026gt; # launches config in VISUAL editor lxc config set \u0026lt;container\u0026gt; \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; # change a single config item lxc config device add \u0026lt;container\u0026gt; \u0026lt;dev\u0026gt; \u0026lt;type\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; lxc config show [--expanded] \u0026lt;container\u0026gt;  In addition to these commmands, you can also snapshot your containers. This creates a restorable copy of your container in case something bad happens - like someone typing rm -rf * into the wrong root shell. By default, snapshots are named in a numbered pattern snapX where X is an integer.\nlxc snapshot \u0026lt;container\u0026gt; \u0026lt;snap\u0026gt; # create new snapshot lxc restore \u0026lt;container\u0026gt; \u0026lt;snap\u0026gt; # restore container to snapshot lxc copy \u0026lt;container\u0026gt;/\u0026lt;snap\u0026gt; \u0026lt;new-container\u0026gt; # new container from snapshot lxc delete \u0026lt;container\u0026gt;/\u0026lt;snap\u0026gt; # delete the snapshot lxc info \u0026lt;container\u0026gt; # lists available snapshots, plus other info lxc move \u0026lt;container\u0026gt;/\u0026lt;snap\u0026gt; \u0026lt;container\u0026gt;/\u0026lt;new-snap\u0026gt; # rename snapshot  Setting up the Container You can enter your container immediately with a root shell with lxc shell \u0026lt;container\u0026gt; and proceed with your regular setup, such as updating and installing packages, setting up new users, etc. To make this process more repeatedly, you can also just move a setup script from the host to the container first, and then execute that script inside the container. That way you can have a record of what you did when you first set up the container.\n# drop into root shell lxc shell \u0026lt;container\u0026gt; # execute arbitrary command in container lxc exec \u0026lt;container\u0026gt; -- \u0026lt;program\u0026gt; [\u0026lt;options\u0026gt;] # move a file from host to container lxc file push /host/file \u0026lt;container\u0026gt;/path/on/container # move a file from container to host lxc file pull \u0026lt;container\u0026gt;/path/to/file /path/on/host # edit a file inside container lxc file edit \u0026lt;container\u0026gt;/etc/passwd  See my earlier blog post for a list of some CLI tools I like to install on new systems.\nNetworking Giving the Container Access to the Internet The first step in container networking is to make sure your container can access the network. This may require your firewall to let traffic through on the default bridge. On an ArchLinux host, use\nufw route allow in on lxdbr0 ufw allow in on lxdbr0  while on a Fedora host you might use\nfirewall-cmd --zone=trusted --change-interface=lxdbr0 --permanent  Network Forwarding At this point your container is available from the host on wich the LXD service is running. But the whole point of the exercise is to make the container accessible from the lab members' various devices. I\u0026rsquo;ll present two options here for setting this up, depending on whether you need access from outside of your local network or not. Either way, make sure SSH is set up inside your container and you can SSH into the container from the host shell. Both methods rely on using the network forward feature built into LXD. See the documentation for details.\nFor network forwarding to work we need to know two things about our container: what device our container is using to connect to the internet; on a default setup, this will be lxdbr0 but check with lxc network list to be sure. The second item we need is the IP address of our container, which can be displayed with lxc list \u0026lt;container\u0026gt;.\nThe next item we need is an IP address to forward from. We can either get an IP address dedicated to the container, or hijack some ports from our host for re-routing.\nTo add a second IP to your existing network device, use the ip a command to find the device name (on your host) of the network device connected to your network. If you use wifi, this might be something like wlp4s0 or similar. Then pick an IP not otherwise assigned by the router and assign it to this device - in addition to the existing IP - using the following command:\nip -4 a add dev \u0026lt;device-name\u0026gt; \u0026lt;free-ip\u0026gt;/24  Note that this will only persist until the host reboots. You can then create a network forward on the container\u0026rsquo;s device (e.g., lxdbr0) with the newly assigned IP as the listening address. Using this command will let the container handle all incoming traffic to the new IP:\nlxc network forward create lxdbr0 \u0026lt;listening_address\u0026gt;  You can then edit the target address with\nlxc network forward edit lxdbr0 \u0026lt;listening_address\u0026gt;  and specify the container\u0026rsquo;s IP as the target_address.\nThe alternative method is to use the host\u0026rsquo;s IP as the listening address and then just forward particular ports to the container, e.g. port 22 for SSH or 590x for VNC servers. This way you skip creating the second IP above, and just start by creating and editing a network forward with the the host IP as listening address. The edit can then list the ports you want forwarded. Here\u0026rsquo;s an example of a valid file:\ndescription: Sample Forward config: {} ports: - description: ssh protocol: tcp listen_port: \u0026quot;10022\u0026quot; # any unused host port target_port: \u0026quot;22\u0026quot; target_address: \u0026lt;container-ip\u0026gt; - description: VNC servers protocol: tcp listen_port: 105901-105904 # any unused host port target_port: 5901-5904 target_address: \u0026lt;container-ip\u0026gt; listen_address: \u0026lt;host-ip\u0026gt;  Aside from these forwards, you may consider setting up a postfix server and associated forward so you can use the mail command to programmatically send emails to users. One great use case for this is the sending of log files after completion of long running jobs. This keeps your users from needing to manually log in and check the status of their jobs. If you have used HPC services at your campus, you may have experienced the utility of this first hand.\nNetwork forwarding options are explained in more detail in the documentation, which also contains a link to a short YouTube video demonstring these commands in a shell session.\nGetting a GUI Running First, think about whether the tools you use require a GUI. A lot of research work can be done entirely within the command line or by using servers with particular software. So instead of installing a regular RStudio instance, you could install RStudio Server. Jupyter is already designed around the client/server model, as are RDBMS systems. If your team doesn\u0026rsquo;t feel comfortable with ViM and prefers VS Code, use the remote extension to use a VS Code server that can be opened up from your teams' local computers using SSH.\nIf you only need a GUI to use one GUI app at a time, say Mathematica/Matlab, then the simplest option will be to use X-forwarding via SSH. Make sure that X11Forwarding yes is set in your sshd_config file and restart the sshd service to turn it on. You\u0026rsquo;ll also need to install xorg-xauth on an ArchLinux container. From then on, connecting via SSH with the -X flag should work as desired.\nIf you need an entire desktop environment available, you can set up VNC or NoMachine the same way you would for a regular system. I have seen a lot of comments arguing for NoMachine being more performant, but the default TigerVNC on Arch/Fedora has worked sufficiently well for most of my needs.\n","date":1673031600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673031600,"objectID":"9c66d42a9f0ed0de845d8e374541b3fc","permalink":"https://dmsenter89.github.io/post/23-01-virtual-lab/","publishdate":"2023-01-06T19:00:00Z","relpermalink":"/post/23-01-virtual-lab/","section":"post","summary":"Dealing with computer resources in a modern lab can be tricky. Even if all participating researchers have laptops, a central location for storage or to host licensed software is desirable. While a physical computer can be setup for such a use, that is not always the most desirable solution.","tags":["containers","linux","networking","virtual-lab"],"title":"Setting up a Virtual Lab Computer","type":"post"},{"authors":[],"categories":["Missing Data"],"content":"Understanding whether a variable\u0026rsquo;s missingness from a dataset is related to the underlying value of the data is a key concept in the field of missing data analysis. We distinguish three broad categories: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). In his book Statistical Rethinking, McElreath1 gives an amusing example to illustrate this concept: he considers variants of a dog eating homework and how the dog chooses - if at all - to eat the homework. The examples he give show substantial shifts in observed values, which make for a good illustration of the types of problems you might encounter. A lecture corresponding to the example from the book can be found on YouTube. In this post, I will first briefly review the different missing data mechanisms before implementing McElreath\u0026rsquo;s examples in SAS.\nOverview of Missing Data Mechanisms My presentation here follows van Buuren2. Let $Y$ be a $n \\times p$ matrix representing a sample of $p$ variables for $n$ units of the sample and $R$ be a corresponding $n \\times p$ indicator matrix, so that\n$$r_{i,j} = \\begin{cases} 1 \u0026amp; y_{i,j} \\text{ is observed} \\\\ 0 \u0026amp; y_{i,j} \\text{ not observed.}\\end{cases} $$\nWe denote the observed data by $Y_\\text{obs}$ and the missing data that $Y_\\text{miss}$ so that $Y=(Y_\\text{obs},Y_\\text{miss})$.\nWe distinguish three main categories for how the distribution of $R$ may depend on $Y$. This relationship is described as the missing data model. Let $\\psi$ contain the parameters of this model. The general expression of the missing data model is $\\mathrm{Pr}(R|Y_\\text{obs}, Y_\\text{miss}, \\psi)$, where $\\psi$ consists of the parameters of the missing data model.\nMissing Completely at Random (MCAR). This implies that the cause of the missing data is unrelated to the data itself. In this case,\n$$ \\mathrm{Pr}(R=0| Y_\\text{obs}, Y_\\text{miss}, \\psi) = \\mathrm{Pr}(R=0|\\psi).$$\nThis is the ideal case, but unfortunately rare in practice. Many researchers implicitly assume this when using methods such as list-wise deletion, otherwise known as complete case analysis, which can produce unbiased estimates of sample means if the data are MCAR, although the reported standard error will be too large.\nMissing at Random (MAR). Missingness is the same within groups defined by the observed data, so that\n$$ \\mathrm{Pr}(R=0| Y_\\text{obs}, Y_\\text{miss}, \\psi) = \\mathrm{Pr}(R=0|Y_\\text{obs},\\psi).$$\nThis is a often a more reasonable assumption in practice and the starting point for modern missing data methods.\nMissing not at Random (MNAR). If neither the MCAR or MAR assumptions hold, then we may find that missingness depends on the missing data itself, in which case there is no simplification and $$ \\mathrm{Pr}(R=0| Y_\\text{obs}, Y_\\text{miss}, \\psi) = \\mathrm{Pr}(R=0| Y_\\text{obs}, Y_\\text{miss}, \\psi).$$\nAs you can imagine, this is the most tricky case to deal with.\nDogs Eating Homework Consider dogs (D) eating students' homework. Each student\u0026rsquo;s homework score (H) is graded on a 10-point scale and each student\u0026rsquo;s score varies in proportion to how much they study (S). We assume the amount of time they study is normally distributed. A binomial is used to generate homework scores from the normed time spent studying. McElreath uses the following code to simulate the full data set:\nN \u0026lt;- 100 S \u0026lt;- rnorm( N ) H \u0026lt;- rbinom( N, size=10, inv_logit(S) )  where inv_logit(x) = exp(x)+(1+exp(x)), the definition used by the LOGISTIC function in SAS. With a data step, this can be represented in SAS as follows:\ndata full; DO i=1 to 100; S=RAND('NORM'); H=RAND('BINO', LOGISTIC(S), 10); output; END; label S='Amount of Studying' H='Homework Score'; drop i; run;  We can get closer in form to the R code by using PROC IML, but that\u0026rsquo;s a story for a different post.\nSay we are interested in estimating the relationship between $S$ and $H$. In our example, we assume that $H$ is not directly observable. Instead, $H^*$ is observed - a subset of the full data set $H$ with some homework values missing. We can now look at why some of those values are missing. Specifically, in McElreath\u0026rsquo;s example each student has a dog $D$ and sometimes the dog eats the homework. But here we can again ask, why is the dog eating the homework? McElreath uses directed acyclic graphs (DAGs) to represent different missing data models, reproduced below. As we will see, these are some intuitive examples for our three missing data mechanism categories.\n The directed acyclic graphs corresponding to McElreath\u0026rsquo;s examples of missing data models. $S$ represents the amount of time spent studying, which in turn influences the homework score $H$, which is only partially observed (indicated by the circle). Alas, dogs $D$ eat some of the homework. The actually observed scores - those not eaten - are indicated by $H^*$. Adapted from figure 15.4 in Statistical Rehinking.   Missing Completely At Random (MCAR) In the first example, the dogs eat homework completely at random. This is the most basic and benign case, and corresponds to DAG 1) in Figure 1. McElreath\u0026rsquo;s R code is given by\nD \u0026lt;- rbern( N ) Hm \u0026lt;- H # H*, but * is not a valid char for varnames in R Hm[D==1] \u0026lt;- NA  We can implement this in SAS by using the RAND function with the Bernoulli argument in an if/else clause:\nif RAND('BERN', 0.5) then Hm = .; else Hm = H;  This causes about half of our data to be hidden, but not in a biased way.\nMissing at Random (MAR) In the second example, we assume the amount of time a student spends studying decreases the amount of time they have to play with and exercise their dog. This, in turn, influences whether the homework gets eaten. Or, as McElreath puts it, the \u0026ldquo;dog eats conditional on the cause of homework.\u0026rdquo; In his particular example, the homework is eaten whenever a student spends more time studying than the average $S=0$. This corresponds to DAG 2) in Figure 1.\nD \u0026lt;- ifelse( S\u0026gt;0 , 1 , 0 ) Hm \u0026lt;- H Hm[D==1] \u0026lt;-NA  In SAS:\nif S\u0026gt;0 then Hm = .; else Hm = H;  Missing not at Random (MNAR) In this case, we have some correspondence between the missing variable\u0026rsquo;s value and whether or not it is missing from the data set. Here, the \u0026ldquo;dog eats conditional on the homework itself.\u0026rdquo; Suppose that dogs prefer to eat bad homework. In such a case, the value of $H$ is directly related to whether or not $H$ is observed in the particular unit or not. His example R code is as follows:\n# dogs prefer bad homework D \u0026lt;- ifelse( H\u0026lt;5 , 1 , 0 ) Hm \u0026lt;- H Hm[D==1] \u0026lt;- NA  And in SAS:\nif H\u0026lt;5 then Hm = .; else Hm = H;  The Full SAS Code We can now build a SAS data set that contains a full copy of the original data set, together with our various examples of missing data mechanisms. I have added a seed to the data step for reproducibility.\ndata full; CALL streaminit( 451 ); LABEL Type = 'Missing Data Mechanism' S = 'Amount of Studying' H = 'Homework Score' Hm = 'Observed Homework Score' ; DO i=1 to 100; TYPE = 'FULL'; S = RAND('NORM'); H = RAND('BINO', LOGISTIC(S), 10); Hm = H; output; /* Example 1) MCAR */ TYPE = 'MCAR'; if RAND('BERN', 0.5) then Hm = .; else Hm = H; output; /* Example 2) MAR */ TYPE = 'MAR'; if S\u0026gt;0 then Hm = .; else Hm = H; output; /* Example 3) MNAR */ TYPE = 'MNAR'; if H\u0026lt;5 then Hm = .; else Hm = H; output; END; drop i; run;  You may want to run a PROC SORT or PROC SQL afterwards to group the different categories together, as they will be alternating in this data set.\nResults  Boxplot of our example data. Note that the MCAR data looks very similar to the original data set, unlike the MAR and MNAR versions.   We can see that MCAR leads to minimal bias in our example data, while both the MAR and MNAR variations lead to substantial differences in observed vs actual homework scores for our synthetic population. For a more subtle example, see section 2.2.4 in van Buuren,2 available online here.\n  R. McElreath, Statistical Rethinking, 2nd ed, Chapman and Hall/CRC, Boca Raton, FL, 2020. \u0026#x21a9;\u0026#xfe0e;\n S. van Buuren, Flexible Imputation of Missing Data, 2nd ed, Chapman and Hall/CRC, Boca Raton, FL, 2019. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1672758000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672758000,"objectID":"2917851f663dcaa5eeaaddf31350ca76","permalink":"https://dmsenter89.github.io/post/23-01-missing-data-mechanisms/","publishdate":"2023-01-03T10:00:00-05:00","relpermalink":"/post/23-01-missing-data-mechanisms/","section":"post","summary":"Understanding whether a variable\u0026rsquo;s missingness from a dataset is related to the underlying value of the data is a key concept in the field of missing data analysis. We distinguish three broad categories: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR).","tags":["missing-data","simulation","sas"],"title":"Missing Data Mechanisms","type":"post"},{"authors":[],"categories":[],"content":"There are certain CLI tools that I find myself installing whenever I set up a new system. I\u0026rsquo;m not talking about the general system setup, like installing vim or Python, but some drop-in replacements for older Linux tools and some cli solutions that I use quite regularly. I thought I would collect them here for convenience. The headers are sorted alphabetically, except that \u0026ldquo;Other\u0026rdquo; is last because that seems most sensible.\nTable of Contents  \u0026ldquo;Better\u0026rdquo; Drop-Ins Data and File Wrangling     Development Resource Management     Other   \u0026ldquo;Better\u0026rdquo; Drop-Ins The following tools I use as \u0026ldquo;better\u0026rdquo; drop-ins for other commands. Instead of ls, I typically now use exa. Instead of grep, I tend to use ripgrep. Instead of find, I tend to use fd. For quick viewing of text-files with syntax highlighting, I like bat over cat. As a git/diff pager with syntax highlighting I use delta. All of these tools are written in Rust. The old df command can be improved with duf, a Go implementation with output that\u0026rsquo;s nicer to read (and alternatively, outputs as JSON).\nData and File Wrangling Calculator A great CLI calculator implemented in C++ is Qalculate!. It is mainly intended to be run with a Qt or GTK GUI, but does include a CLI version that can be invoked with qalc. A Rusty alternative is rink. A benefit of rink is that if you are trying to convert incompatible units, it will make a suggestion for a transformation that makes each side compatible, which can help with dimensional analysis. A good Rusty calculator app without units but wide support of operations, including functions, is kalker.\nCSV and JSON CSV files are ubiquitous, and being able to manipulate them and get an overview of what is contained without needing to actually load them in Excel/Python/SAS/etc is very useful. I used to really like csvkit for that. The main drawback here is speed for large CSV files, due to it being implemented in Python. A must faster program written in Rust is qsv, successor to BurntSushi\u0026rsquo;s xsv. It has more features than csvkit, is faster, and seems more flexible.\nAnother cool Python program for interacting with CSV files is visidata. It is a CSV viewer that doubles as a spreadsheet program, allows you to make plots and statistics and just do a ton of different things with your file in-memory.\nThe C program jq aims to be the sed of working with JSON files.\nFile Managers There are a lot of file managers to choose from these days. There are lots of popular options like mc, ranger, nnn, but I tend to keep falling back on vifm.\nDevelopment If you need to benchmark something, try Rusty old Hyperfine. There is an interesting make alternative called just, which looks promising but I haven\u0026rsquo;t played with it yet.\nResource Management Disk Space There are several excellent tools here. One that can be found in most repos I\u0026rsquo;ve encountered is Ncdu, a disk usage analyzer with an ncurses interface written in C. It is reasonably fast and let\u0026rsquo;s you interactively delete folders while you\u0026rsquo;re at it. A parallel implementation of the same idea but written in Go can be found with gdu. For non-interactive use, dust (du + rust) is available.\nSystem Status - General One of the first tools I usually install is htop. It is a widely available and fast process viewer written in C. You can use it to kill or renice a process interactively without needing to find its PID. An alternative to this is glances, \u0026ldquo;an eye on your system\u0026rdquo; written in Python. It has a lot more information, including disk usage, sensor temperatures, battery information (on laptops), etc. and can be extended with plugins. It can be used interactively on the CLI, but it also gives the option of running in client/server mode which is nifty.\nSimilar to glances, bottom is a Rust program giving general system information including plots, but it does not have quite the same range of information to it as glances does.\nSystem Status - Networking To see what is clogging up your internet pipes, try nethogs written in C++. A nice rusty alternative is bandwhich.\nOther Trying to figure out when it\u0026rsquo;s a good time to speak with a colleague in a different time zone? Install the Python package undertime.\nDon\u0026rsquo;t want to remeber different package manager\u0026rsquo;s syntax? Install pacapt and use ArchLinux' pacman syntax on your system instead.\n","date":1670427496,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672759800,"objectID":"2466d18fb8ff203cf468b4cfc25c413b","permalink":"https://dmsenter89.github.io/post/22-12-some-cli-tools/","publishdate":"2022-12-07T15:38:16Z","relpermalink":"/post/22-12-some-cli-tools/","section":"post","summary":"A few convenience CLI tools I find myself installing on new systems regularly.","tags":["cli","linux"],"title":"Some CLI Tools","type":"post"},{"authors":[],"categories":[],"content":"Creating a minimum working example (MWE) is a relatively frequent task. It is no problem to share an MWE for a feature in SAS because a large number of example data sets are shipped and installed by default. But sometimes you need an MWE because you are having trouble accomplishing a particular task with particular input data. At that point, you will need to share the data or a subset thereof together with your code. In SAS Forums, the preferred way to do this is with a datastep using a datalines/cards statement. Writing these by hand can be tedious since the data source is not typically a datalines statement to begin with. I have previously seen a SAS macro that can be used to generate a datalines statement from a SAS data set, but can\u0026rsquo;t seem to locate it at the moment. The data source I personally encounter the most often in my work is either in CSV or Excel formats. Since the latter can easily be exported to CSV, I decided to write a program that generates a SAS data step given a CSV file.\nFor the implementation language I chose to use Go. I started learning about Go back in May when I implemented a simple CLI version of Wordle. Since then I have increasingly used Go to write various small tools at work. It has been a very enjoyable language to write in and distribution via GitHub is easy. If you have the Go toolchain installed, you can get the latest copy of csv2ds using\ngo install github.com/dmsenter89/csv2ds@latest  The tool is very simple to use. Give it a CSV file or list of CSV files and it will generate a data step for each file using the CSV\u0026rsquo;s base name as the data set name. To ensure compatibility, variable names and the data set name are processed to be compatible with SAS' naming scheme. The tool will attempt to guess if a particular column is numeric or not. If a column is determined to not be numeric, the longest cell will be used to set that variable\u0026rsquo;s length via a length statement to prevent truncation.\nI often work with the csvkit suite of command-line tools. It\u0026rsquo;s a wonderful collection of Python programs that can import data into CSV, generate basic column statistics, and use grep and SQL to extract data from a CSV file, amongst other things. This collection is designed to allow you to pipe the output from one as input to the next. Consider this example. Csvcut is used to extract only certain columns from the file data.csv. Then csvgrep is used to subset to use only the data pertaining to one particular county. Then the data is sorted by the total_cost variable and displayed. I wanted my tool to be compatible with this suite, so if - is passed as the filename, csv2ds will read the contents of STDIN instead. Changing the above csvkit example by replacing csvlook with my tool will generate the corresponding SAS data set:\ncsvcut -c county,item_name,total_cost data.csv | csvgrep -c county -m LANCASTER | csvsort -c total_cost -r | csv2ds -  At this point csv2ds is quite simple, but sufficient for my needs. Some minor intervention may be needed to make the data step template work for your data. Informats like DOLLAR are not recognized as numeric and minor edits would need to be made to the produced template.\nCheckout my new tool over on GitHub.\n","date":1669221808,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669221808,"objectID":"3ac5794344680382d0d4b97c33b78bbe","permalink":"https://dmsenter89.github.io/post/22-11-csv2ds/","publishdate":"2022-11-23T16:43:28Z","relpermalink":"/post/22-11-csv2ds/","section":"post","summary":"CSV2DS is a new program I wrote in Go to help me create minimum working examples for SAS that can be shared as a single SAS script.","tags":["csv","data-step","go","sas"],"title":"CSV2DS","type":"post"},{"authors":[],"categories":[],"content":" Windows Subsystem for Linux (WSL) is an important part of my daily work flow. Unfortunately, the main distro supplied by Windows is Ubuntu, which - for a variety of reasons - is not exactly my favorite distro. Luckily, WSL2 allows you to import an arbitrary Linux distro to use instead. I got the idea from an article (Dev.to) by Jonathan Bowman explaining how to get Fedora up and running in WSL2. This article summarizes the key points of Bowman\u0026rsquo;s post and includes information for my long time daily driver, Arch Linux.\nThe short of it is that you can import a root filesystem tarball into WSL2 from Windows terminal using the following command:\nwsl --import \u0026lt;distro-name\u0026gt; \u0026lt;distro-target-location\u0026gt; \u0026lt;path-to-tarball\u0026gt;  Once imported, you can launch into your distro using wsl -d \u0026lt;distro-name\u0026gt;. The only question is how to get the root filesystem for this import step.\nThere are two options we can go with: using a pre-fabricated root filesystem (rootfs), or creating our own using Docker.\nUsing an Existing Root Filesystem Some distros publish these. For Arch Linux, you can find them on GitLab. Two main images are available: base and base-devel. The latter has the base-devel package group pre-installed.\nFor Fedora, you can head over to GitHub to get a copy of the rootfs. Note that for Fedora, the rootfs is merely part of the repo and not a separate release page. You\u0026rsquo;ll be able to pick your base version of Fedora by switching branches in the repository.\nThese rootfs images are usually compressed. Before you can use them with WSL2, the tarball needs to be extracted. The Arch Linux rootfs can be extracted with zstd and the Fedora rootfs can be extracted using 7z.\nMaking your Own Root Filesystem Docker allows you to export a container to a root filesystem tarball:\ndocker export -o \u0026lt;rootfs-name\u0026gt;.tar \u0026lt;container-or-image-name\u0026gt;  The neat thing here is that you can use either an image or a container name.\nArch Linux images are available from DockerHub. Available tags include the above mentioned base and base-devel. Fedora is also available on DockerHub and its tags include version numbers (e.g., 37 or 36).\nAdditional Setup Once you have imported the distro you only have a barebones system available. Likely only the root user is available, which is not ideal. You\u0026rsquo;ll want to install the packages you want to use and set up your own user in addition to root. If you are building your own rootfs using Docker, you can build everything interactively in your container by running docker run -it \u0026lt;image-name\u0026gt;:\u0026lt;tag\u0026gt; to drop into a shell and do all your setup there. Alternatively, you can create a Dockerfile with the basic setup and build an image from that.\nArch Linux Pacman won\u0026rsquo;t work out-of-the-box because it doesn\u0026rsquo;t ship with keys. You\u0026rsquo;ll need to run pacman-keys --init first. Install your favorite software using pacman, e.g.\npacman -Syu exa htop vim  User management and other common setup tasks are covered in the Arch Wiki\u0026rsquo;s General Recommendations. Key tasks include adding a new user:\nuseradd -m -G wheel $username  Fedora Make sure to run dnf upgrade to get the latest version of your packages. You may need to install either the util-linux or util-linux-core packages in order to get the mount command working (used by WSL to mount the Windows filesystem). To be able to add a non-root user with a password you\u0026rsquo;ll need to make sure that passwd is installed.\nTo add a non-root user in Fedora, use\nuseradd -G wheel $username passwd $username # in interactive mode, you'll type in your password here  General Case In order to actually start the WSL instance as your non-root user, you\u0026rsquo;ll need to edit /etc/wsl.conf inside of your distro. If the user section doesn\u0026rsquo;t exist yet, you can just run\necho -e \u0026quot;\\n[user]\\ndefault = ${username}\\n\u0026quot; \u0026gt;\u0026gt; /etc/wsl.conf  Those are the basics to get you up and running. Not everything will necessarily work smoothly out-of-the box as you may be missing some packages that you\u0026rsquo;re not aware of until you need them, but overall I\u0026rsquo;ve had a positive experience with this setup.\n","date":1668483265,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668483265,"objectID":"8a65f8768593fb32a0594b11829a2c6e","permalink":"https://dmsenter89.github.io/post/22-11-setup-an-arbitrary-wsl2-distro/","publishdate":"2022-11-14T22:34:25-05:00","relpermalink":"/post/22-11-setup-an-arbitrary-wsl2-distro/","section":"post","summary":"Windows Subsystem for Linux (WSL) is an important part of my daily work flow. Unfortunately, the main distro supplied by Windows is Ubuntu, which - for a variety of reasons - is not exactly my favorite distro.","tags":["archlinux","fedora","linux","windows","wsl"],"title":"Setup an Arbitrary WSL2 Distro","type":"post"},{"authors":[],"categories":[],"content":"One of the coolest packages for R is knitr. Essentially, it allows you to combine explanatory writing, such as a paper or blog post, directly with your analysis code in a Markdown document. When the target document is compiled (\u0026lsquo;knitted\u0026rsquo;), the R code in the document is run and the results inserted into the final document. The target document could be an HTML or a PDF file, for example. This is great for many reasons. You have a regular report you want to run, but the data updates? Just re-knit and your entire report is updated. No more separate running of the code followed by copying the results into whatever software you use to build the report itself. This makes it not just less cumbersome, but less error prone. It also improves reproducibility. Somebody wants to see your work, perhaps because they are unsure of your results or they want to extend your work? You can share the markdown file and the other party can see exactly what code was used to generate what part of your report or paper.\nWhile knitr is certainly not the first package that allows for this workflow, and also not the only one, I have found it to be the most consistent and easy to use.1 Luckily, knitr supports a variety of languages, including SAS. And you can even mix and match multiple languages in one document.2\nYou might think that this sounds similar to Jupyter notebooks. While that is true, and there is a Jupyter kernel for SAS as well, knitr has some advantages over Jupyter for report-generation. Without additional tools, you have the option to execute but not display the code that generates your results, making a cleaner report. You can also elect to only show part of the code, with manual setup code running behind the scenes without being printed to the report itself. Additionally, the entire document is executed linearly. That means that if you update a code chunk towards the beginning of your document, it affects the code chunks following it, while in Jupyter you easily get in the habit of executing the chunks independently which can lead to inconsistencies if you don\u0026rsquo;t pay attention to the cell numbers.3\nIn this post, I\u0026rsquo;ll demonstrate the basics of setting up a reproducible report using the SAS engine in knitr.\nInstall Perhaps the easiest way to get started for beginners is to use RStudio and Anaconda. With that you can create a sample R Markdown document (File -\u0026gt; New File -\u0026gt; R Markdown). Press the knit button. If any packages required by knitr are missing, RStudio will install them for you. This way you can be sure that all the R parts are set up correctly. Additionally, I recommend installing the SASmarkdown package with\n# from CRAN: install.packages(\u0026quot;SASmarkdown\u0026quot;) # from GitHub: devtools::install_github(\u0026quot;Hemken/SASmarkdown\u0026quot;)  Once install is complete, load the package (library(SASmarkdown)) and check the output. If you see a message that SAS was found, you are good to go. If not, you will either need to add SAS to your PATH or simply provide the path to SAS as an option in your document (see below).\nA Basic Markdown File The important thing is to load the SASMarkdown package in your document. I recommend making a setup chunk at the very top of your document and setting include to FALSE. That way the setup chunk is executed, but not printed to your final document.\n```{r setup, include=FALSE} library(SASMarkdown) # if SAS is not in your path, define it manually: saspath \u0026lt;- \u0026quot;C:/Program Files/SASHome/SASFoundation/9.4/sas.exe\u0026quot; knitr::opts_chunk$set(engine=\u0026quot;sashtml\u0026quot;, engine.path=saspath) ```  With that, we\u0026rsquo;re ready to run a basic SAS chunk using just the SAS option. This produces the typewriter-style output that is familiar from Enterprise Guide for example:\n```{sas example1} proc print data=sashelp.class; run; ```  If we want to take advantage of the modern HTML output that is standard in SAS Studio, we use the sashtml engine instead:\n```{sashtml example2} /* if you want, you can set an ODS style for HTML output: */ ods html style=journal; proc print data=sashelp.class; run; ```  If you want graphical output, for example from SGPLOT, you\u0026rsquo;ll need to use the sashtml engine. To get the default blue look from SAS Studio, use the HTMLBLUE style:\n```{sashtml example3} ods html style=HTMLBLUE; proc sgplot data=sashelp.cars; scatter x=EngineSize y=MPG_CITY; run; ```  Some Additional Comments The first thing that is important to note is that each chunk is processed separately. That means each chunk should be written so as to be capable of being executed independent of the others. It is possible to get around this using the collectcode=TRUE chunk option. This chunk will then subsequently be executed prior to the code from a following chunk. So for example:\n```{sashtml save1, collectcode=TRUE} data sample; set sashelp.class; run; ``` And now use it again: ```{sashtml save2} proc means data=sample; run; ```  This is particularly useful for libnames and setting the preferred ODS style, so you don\u0026rsquo;t have to keep doing it again in each cell.\nThe other thing to note is that knitr for SAS works best with HTML output. It can use SAS styles and produce output looking like what you would expect running in SAS Studio. If you want PDF output, you can get nicer output using LaTeX Tagsets for ODS and the StatRep System.\n  knitr itself was based on Sweave, but uses Markdown instead of LaTeX code. Other languages have similar packages, for example pweave for Python or Weave for Julia. \u0026#x21a9;\u0026#xfe0e;\n The chunks from different languages do not have access to each other\u0026rsquo;s data. To move data between the different engines, more setup work is needed. \u0026#x21a9;\u0026#xfe0e;\n If you code in Julia, there is an interesting new reactive notebook called Pluto that promises to always keep your cells in sync, while being geared towards a Jupyter-style workflow. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1668178800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668178800,"objectID":"a99be060e2ba5cf9395d042423acd6e2","permalink":"https://dmsenter89.github.io/post/22-11-sas-markdown-for-reproducibility/","publishdate":"2022-11-11T15:00:00Z","relpermalink":"/post/22-11-sas-markdown-for-reproducibility/","section":"post","summary":"One of the coolest packages for R is knitr. Essentially, it allows you to combine explanatory writing, such as a paper or blog post, directly with your analysis code in a Markdown document.","tags":["knitr","reproducibility","rstudio","sasmarkdown","sas"],"title":"SAS Markdown for Reproducibility","type":"post"},{"authors":[],"categories":[],"content":"In a first semester probability course, students encounter combinatorics and point estimates such as the mean and median of a data set. A common example is the low odds of winning the lottery. When discussing the topic of point estimates, students are exposed to the idea of a \u0026ldquo;fair bet\u0026rdquo; or \u0026ldquo;fair game\u0026rdquo; - one in which the expected value of the random variable associated with the game is equal to the cost of participation or zero, depending on if a fixed cost is included in the game or tracked separately. This year, the Mega Millions had a jackpot in excess of one billion dollars. This had me thinking - mathematically, this is likely a fair game. But I still would expect to loose out playing it. In this article, I want to explore this idea further using the Mega Millions lottery as a particular example.\nMega Millions is played by a choosing five numbers from 1 to 70 (the white balls) and one number from 1 to 25 (the golden \u0026ldquo;Mega Ball\u0026rdquo;). Five white balls (W) and one golden ball (G) are drawn without replacement twice per week. Prizes are earned by matching the drawn numbers. Payouts generally follow a fixed schedule for everything but the jackpot, at least outside of California where the payouts for all prizes are pari-mutual instead. Below is a table of all possible events as given on the Mega Millions website, sorted by increasing odds.\n   Event Variable Value Odds     5 W + G $x_1$ Jackpot 1/302,575,350   5 W $x_2$ $1,000,000 1/12,607,306   4 W + G $x_3$ $10,000 1/931,001   4 W $x_4$ $500 1/38,792   3 W + G $x_5$ $200 1/14,547   2 W + G $x_6$ $10 1/693   3 W $x_7$ $10 1/606   1 W + G $x_8$ $4 1/89   G $x_9$ $2 1/37   No Match $x_{10}$ $0 24/1    The Fair Bet Analysis A Fair Bet or Fair game is one in which the expected value of the random variable doesn\u0026rsquo;t favor either the player or the house. Given a cost of 2 USD per game, we can say that Mega Millions is fair when $E[X]=2$, or more specifically when\n$$E[X] = \\sum_{i=1}^{10} x_i P(X=x_i) = \\frac{n}{302,575,351} + \\sum_{i=2}^{10} x_i P(X=x_i) = 2$$\nI use Maxima to solve for the jackpot representing a fair game and to print a few representative values of the expected value for some jackpot options.\n/* Define Expectation as dependent on n */ E(n) := n/302575351 + 1000000/12607307 + 10000/931002 + 500/38793 + 200/14548 + 10/694 + 10/607 + 4/90 + 2/38; /* solve for fair game */ float(solve(E(n)=2,n)); /* give expected return for different jackpot values */ jackpots : [5e7, 1e8, 2.5e8, 5e8, 7.5e8, 1e9, 2e9]; for i in jackpots do printf(true, \u0026quot;E(~:D) = $~$ ~%\u0026quot;, i, E(i))$  From this we learn that to have a fair jackpot, we require $n = 531,123,698.80$. Even with a fair pet, the expected value is very modest. For example, a 2 billion USD jackpot has $E[X]~=6.85$ - less than 5 USD above the ticket price.\nHow long until we Profit? Most people don\u0026rsquo;t play the lottery to win small amounts like 5 USD. They want to become millionaires. Given that our expected values are so low, let\u0026rsquo;s take a look at how long it will take us to become rich if we take the lottery game route.\nThe Geometric Distribution and Our Lottery Let\u0026rsquo;s start by considering only the events that would result in a win of a million dollars or more. In other words, events $x_1$ and $x_2$. We have\n$$P(x_1 \\vee x_2) = \\frac{315,182,658}{3,814,660,340,689,757} \\approx 8.262\\times 10^{-8}. $$\nIf we are only interested in this outcome, we can treat our outcome as a Bernoulli variable with $p=P(x_1 \\vee x_2)$. Then the expected number of games we need to play to win a million dollars or more is distributed like a geometric with $E[G] = 1/p$. For our specific case:\n$$ E[G] = \\frac 1 p = \\frac{3,814,660,340,689,757}{315,182,658} \\approx 12,103,014.$$\nRecall that two games are played per week. Converting this expected number of games to years, it would take approximately $115,977$ years for us to win. Even if one drawing were held each day, we would expect to take more than $33,000$ years to win.\nSince the CDF of the geometric distribution is well defined, we can use it to estimate the number of games required for a certain likelihood of having a win of at least a million dollars. To have roughly 50% odds of winning, we need to play about $8,400,000$ games of Mega Millions. Note that in this case you would still likely be in the hole since the $1,000,000$ USD jackpot is nearly 24 times more likely than the main jackpot and each game costs 2 USD to play.\nSimulating a Lifetime of Playing At this point you might agree that the lottery is not a good get-rich-quick scheme. That alone doesn\u0026rsquo;t mean that you are all but guaranteed to loose money over a lifetime of playing. So let\u0026rsquo;s run some simulations and see what the distribution of our net worth is after taking everything into account. To make things as fair as possible, we will assume a constant jackpot of 750 million USD.\nLet\u0026rsquo;s say you spend 50 years playing the Mega Millions at 2 USD for one ticket at each of the two weekly drawings. That comes out to just about $2,609$ weeks or $5,218$ games for a total price of $10,436$ USD. I simulated $50,000$ individuals each playing $5,218$ games for a constant jackpot of $750,000,000$ USD - much higher than the typical jackpot and advantageous to the players. This will cost them each $10,436$ USD in ticket costs over the 50 years they play. Yet, despite the simulated lottery being rigged in the players' favor, 99% of my players win less than 600 USD total over this 50 year time period.\n   Statistic Value ($)     Mean -9,169.43   SD 20,985.05   Min -9,974.00   25% -9,912.00   50% -9,898.00   75% -9,890.00   99% -9,884.00   Max 1,990,204.00    From these results it is clear that for all but the luckiest few, even just saving the money under a mattress outperforms playing the lottery. You can explore a distribution plot of my simulation with Plotly here. Note that this page may take a moment to load due to the many data points. You will need to zoom in on the left-hand side to be able really make anything out.\nImplementation Note The number of simulations grows quickly given the $5,218$ games we are using. Doing $50,000$ simulations of that many games requires over 260 million random draws. Prototyping in Python often makes sense because of the many features available for analysis and plotting, but this seems like an example where a compiled language might outperform by a considerable amount. I decided to try this out (GitHub).\nAll of the programs were written with an emphasis on simplicity over performance so as to avoid biasing the results. Since the different individuals play their games independently, I wrote both a single-threaded C++ version as well as one utilizing OpenMP\u0026rsquo;s parallel for loop. As alternative compiled languages I added implementations in Go and Rust.\nFor scripting languages I included Python and Julia. In Julia the main loop can trivially be set to run concurrently by prepending Threads.@threads to the for loop, so inlcuded that as an option as well. This instructs the Julia interpreter to run this loop with the available threads. By default this is one, but can be set higher using an environment variable or by starting Julia with the -t flag and specifying the desired number of threads.\nI used hyperfine (GitHub) to benchmark the performance of my programs in WSL; see output below for details.\n   Command Mean [s] Min [s] Max [s] Relative     C++ (Single Thread) 4.602 Â± 0.087 4.514 4.828 1.88 Â± 0.06   C++ (OpenMP) 2.653 Â± 0.170 2.506 3.092 1.08 Â± 0.07   Go 9.362 Â± 0.058 9.258 9.417 3.83 Â± 0.10   Julia 28.606 Â± 0.372 28.198 29.520 11.69 Â± 0.33   Julia (4 Threads) 19.016 Â± 0.274 18.673 19.511 7.77 Â± 0.23   Python 57.727 Â± 0.530 59.783 Â± 3.811 57.833 70.242   Rust 2.447 Â± 0.062 2.391 2.579 1.00    I was surprised by Rust\u0026rsquo;s performance. I only looked up enough Rust to be able to implement this simple example, so I find it surprising that it can keep up with a multi-threaded C++ implementation.\nUpdates: This blogpost has been updated with new benchmark values. The original post did not include results in Rust.\n","date":1664566800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666286400,"objectID":"9076db55a05c4a582d5c09cea44af64d","permalink":"https://dmsenter89.github.io/post/22-09-lottery/","publishdate":"2022-09-30T15:40:00-04:00","relpermalink":"/post/22-09-lottery/","section":"post","summary":"In a first semester probability course, students encounter combinatorics and point estimates such as the mean and median of a data set. A common example is the low odds of winning the lottery.","tags":["analysis","performance","probability","simulation"],"title":"Does it ever make sense to play the Lottery?","type":"post"},{"authors":[],"categories":[],"content":"Most people can guess the current life expectancy for Americans at birth as being in the high 70s or around 80. In fact, given the current mortality table published by the Social Security Administration (SSA), males have a life expectancy of about 76 compared to a female life expectancy of about 81. Of course that is only an expected value. Guessing the distribution of a person\u0026rsquo;s life expectancy is somewhat more difficult. In this post, we\u0026rsquo;ll take a look at some simulated lives to get a feel for the distribution of life expectancy and its implications for retirement planning.\nLet\u0026rsquo;s begin by looking at our mortality table. The rows indicate an individual\u0026rsquo;s current age. For both a male and a female, three values are then given: the probability of death in a given year, the \u0026ldquo;Number of Lives\u0026rdquo;, and the life expectancy for this individual. The probability of death in a given year is somewhat self-explanatory. The \u0026ldquo;Number of Lives\u0026rdquo; variable starts with 100,000 individuals and gives the number of survivors at a given age. So for example, of the 100,000 males \u0026ldquo;born\u0026rdquo; at age 0, we expect 99,392 to be alive at age 1. The life expectancy is the expected number of years of life remaining for an individual. We can start by plotting this to get a feel for the data.\n  Survival curve for 100,000 males and females given the 2019 SSA mortality tables. The dashed line indicates the typical retirement age of 67.   To get a feeling for the distribution of age at death, I ran 10,000 simulations each for males and females starting at ages 0, 25, 40, 60, and 80. These ages were chosen to represent the full range of possibilities at birth, followed by early, mid- and late career individuals. Age 80 was included for comparison as an older retiree value. Since the probability of death by age 50 is so low, we expect very little difference for the first three ages, with differences becoming more pronounced as age progresses, but it is still useful to visualize.\n  Overview of the distribution of age at death by sex for different ages at the beginning of the simulation. Outliers are are not represented. Note how the results for ages 0 to 40 are nearly indistinguishable.   As expected, we see relatively little variation between birth and age 40, with some recognizable changes beginning at age 60. Given that, I will visualize the distribution for an individual starting at age 40. A 40 year old is about 25-30 years away from retirement and has probably at least started thinking about saving and how much they\u0026rsquo;ll need to put away to last through retirement.\n  Distribution of age at death for males and females given a starting age of 40. Half of the starting population is expected to make it to at least 81/85 (Male/Female), and a quarter will make it at least to 88/91.   So now that have seen the distribution, let\u0026rsquo;s consider how long we\u0026rsquo;ll live past the typical retirement age of 67. The table below lists the ages by sex for the top percentiles given a starting age of 40.\n   Top Percentiles - Age at Death Males Females     5% 95 98   10% 93 95   20% 89 92   30% 86 90   40% 84 87   50% 81 85    We see that 40% of females and 30% of males are expected to live at least 20 years past retirement age. A little more than 5% of females will make it thirty years past retirement, but only 2.5% of males will. While only a small minority of retirees will need to fund their retirement for thirty or more years, it is not unreasonable to target retirement funds to last until we reach age 90.\nUnfortunately, a large share of Americans have insufficient 401k balances to cover their expected longevity (see here, here, and here for some estimates of savings by age group). Many are likely relying on social security benefits to cover some of the difference. This system may not last that long, or at least not with current benefit levels. Social Security outlays have exceeded allocated revenues since 2010 and are currently expected to continue to do so well into the 2090s (see Table A-1). Social security trust fund balances for old-age and survivor benefits are rapidly declining. Between 2020 and 2030, the CBO expects a drop of 80% in this fund. Curiously, over the same time period the trust fund for military personnel is expected to grow by more than 70%, while the fund for civilian government employees is expected to grow by more than 20% (see CBO report). As such, younger Americans not working for the government will need to consider how to fund a multi-decade retirement in the face of potentially large reductions in social security benefits.\n","date":1662124290,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662124290,"objectID":"94a074f9096d57719205f775e2b877a9","permalink":"https://dmsenter89.github.io/post/22-09-life-expectancy/","publishdate":"2022-09-02T13:11:30Z","relpermalink":"/post/22-09-life-expectancy/","section":"post","summary":"A look at the distribution of age at death based on social security mortality tables to see how long we can expect to be in retirement for.","tags":["analysis","life-expectancy","mortality","probability","r","retirement","simulation"],"title":"Life Expectancy Data","type":"post"},{"authors":[],"categories":[],"content":"This post is a follow-up to my post on how to load data from Zillow. Housing prices have soared through the COVID-19 pandemic, leading to a lot of discussion about housing affordability. The quickly growing home values coupled with the subsequent raising of interest rates on mortgages are seeing more and more people priced out of the ability to purchase a home. While rent prices have increased as well, they haven\u0026rsquo;t increased as sharply as home prices.\nIn this post, I will look at the Zillow data set and consider a popular question for millennials - is it better to buy or rent in the current market? For this analysis, I will use the zip code level data of the Zillow Home Value Index (ZHVI) and the Zillow Observed Rent Index (ZORI) for North Carolina metropolitan areas in the years 2019 through June 2022. To evaluate the monthly costs of owning a home, I will utilize the 30 year fixed rate mortgage average from the FRED database, a comprehensive database aggregating various economic time series maintained by the St. Louis Federal Reserve.\nDisclaimer: This post is not financial advice. We are only exploring some aggregate data sets. Past performance is not indicative of future performance.\nTable of Contents  Initial Thoughts Crafting a Mortgage-to-Rent Index  A Naive Implementation A Slightly Better Implementation   Further Thoughts  Risk and Other Costs Housing as an Investment Last Thoughts     Initial Thoughts Both Zillow data sets are available with monthly data published at the zip code level. As such, they are equally well-spaced in time. One issue we notice at initial inspection is that the ZHVI observations are dated to the last day of every month, while the ZORI observations are dated to the first of the month. For merging and comparison, we will set the ZHVI to the first of the month.\nBoth data sets are available for download at the zip code level. The ZORI data set is much smaller, however. It covers 2,453 distinct zip codes compared to the ZHVI\u0026rsquo;s 27,366 distinct zip codes.\nAn initial graph of average ZHVI and ZORI for North Carolina shows the dramatic growth in home values and the degree to which rent is lagging behind. Both y-axes have been scaled proportionally.\n Average Zillow Home Value Index (ZHVI) and Zillow Observed Rent Index (ZORI) values for North Carolina. The left and right axes have been scaled proportionally.   The FRED data set is published weekly on Thursdays and provides a seasonally unadjusted look at the average, actual mortgage rates that homebuyers have received throughout the US. So while this data set has the largest number of data points in time, it is the least granular on a geographic level. I don\u0026rsquo;t have detailed information about the geographic variability in mortgage rates and will ignore this for now. To allow merging with the Zillow data, the FRED data set will need to be averaged in some form. For this post, I will use simple averages by month. It is important to note that the FRED data is not in decimal notation. That means that 3.5% is written as 3.5 as opposed to 0.035.\nOne thing that is interesting about the FRED data is that it goes back as far as 1971. Looking at the overall historic values, we see that the past decade\u0026rsquo;s very low interest rates (less than 5%) are an anomaly.\n  Crafting a Mortgage-to-Rent Index I will create two indices, one being a very naive implementation focusing just on the actual monthly payment on the mortgage loan and the monthly rent payments. This is followed by an improved index that takes into account additional monthly expenses. After both indices have been created, I will discuss some of their shortcomings.\nA Naive Implementation For monthly cost comparisons, we don\u0026rsquo;t actually care about the value of the home directly. What matters is the monthly mortgage cost. There is a lot of variability here and I will make the following assumptions:\n The mortgage originates in the month given by Date at the rate given by the monthly average of the Mortgage30US time series. A 20% down payment was made, so that the mortgage amount is for 80% of the ZHVI. A 20% down payment is often recommended because it avoids the need for mortgage insurance, which would create an additional monthly expense. Closing costs are handled separately and not rolled into the mortgage.  Monthly mortgage payments can be calculated with the PMT function in SAS. The general syntax is PMT( rate, number-of-periods, principal-amount [, future-amount] [, type]). The rate is the APR divided by the number of periods in a year, in our case 12. The number of periods is 12 payments over 30 years, i.e. 360. The principal amount is 80% of ZHVI, while the future amount is zero (default value). SAS allows us to pick of we want to use end of period (type=0) or beginning of period (type=1) payments. For the monthly amount the difference is small, so I will use the default end of period scheme. With that, our formula for the monthly payment is PMT(M30Rate/12, 360, 0.8*ZHVI).\nWe can now construct a simple unit-less ratio of monthly mortgage costs over monthly rent. If this index is greater than 1, it is cheaper to rent while if it is less than 1 it is cheaper to purchase a home. Below is a figure demonstrating the distribution of this index in North Carolina for 2019 through June 2022.\n Mean index value for North Carolina. The shaded region represents the 25th through 75th percentile of index values. Note that the index is \u0026lt;1 until 2022, despite the sharp increase in ZHVI across the state seen above.   The index indicates that the situation was favorable to home buyers until 2022, when it flips to being generally favorable to renters. FRED data indicate that the average mortgage rate didn\u0026rsquo;t substantially begin growing until winter 2021. This lends credence to the view that historically low interest rates have helped buffer would-be homeowners from increases in home value.\nIndividual zip codes' index values behave similar in pattern to the the figure above, making it a good approximation to observed behavior.\nA Slightly Better Implementation The previous implementation underestimates the true monthly cost of home ownership. For one, homeowners are required to pay property taxes each year. While an individual doesn\u0026rsquo;t directly pay the local government on a monthly basis, funds for this purpose are typically collected in an escrow account so it is an ongoing cost. Property tax rates are set at the county and municipal levels and mapping them directly to the ZIP codes requires some work. The rates themselves are fixed fees for every $100 of assessed home value. For the purpose of taxation, the home value used is not the ZHVI, but the value assigned to the home by the county. In North Carolina these assessments must take place at least once every 8 years, but individual counties may choose to do more frequent assessments. As such, there can be a large discrepancy between the ZHVI and the assessed value. I have seen cases where the ZHVI is roughly double that of the county\u0026rsquo;s assessed value. Finding the average assessed value is easy for individual homes (Zillow displays it on its website), but finding an aggregate value that can be used for analysis is tricky. For ease of use I\u0026rsquo;ll assume that assessed value is 70% of ZHVI. I expect this measure to underestimate the assessed values of homes in 2019 but to be close to target in 2022.\nBased on the most recent effective tax rates published by the North Carolina Department of Revenue, the average tax rate is approximately 1.02%, with a minimum of 0.33% and a maximum of 1.7%.\nIn addition to taxes, escrow accounts will typically collect monthly insurance fees as well. The two main types of insurance included are homeowners insurance and mortgage insurance. Mortgage insurance is mandatory when purchasing a home with less than a 20% down payment and protects the lender from the borrower defaulting on the loan repayment. In the above example we assumed a 20% down payment, so mortgage insurance would be optional.\nThe cost of homeowners insurance varies wildly as can be seen in this article from NerdWallet. Their calculated average monthly cost for North Carolina is $142. Renters may be required to purchase renters insurance, but this is not a requirement for all properties and the cost is substantially lower than homeowners insurance. NerdWallet\u0026rsquo;s analysis gives a monthly average cost of $12 for North Carolina.\n Mean adjusted index value for North Carolina. The shaded region represents the 25th through 75th percentile of index values.   As expected, the adjusted index shows the same overall behavior but is shifted up. This index gives a more complicated picture, in which the zip code starts mattering somewhat more in whether renting or buying is cheaper on a monthly basis.\nFurther Thoughts The above analysis was relatively simple, but a good excuse to take a look at the data made available by Zillow. This is not the end of the story, however. I have made several simplifications here, although on average I would argue they were in favor of home purchases. There additional matters to consider, which I will take up below. We also only discussed aggregate data. Particular housing may offer other benefits, especially if you have knowledge of some likely future events. For example, a large tech company planning to move to an area tends to increase both rent and housing prices, so purchasing now may be beneficial by locking in today\u0026rsquo;s prices. We also haven\u0026rsquo;t considered whether there are differences in housing stock and location that may outweigh purely financial considerations.\nRisk and Other Costs Another item to consider is the question of risk in either scenario. A homeowner incurs somewhat larger risks of on-going expenses that we have conveniently ignored. Roofs, HVAC systems, etc. need maintenance and eventually will need to be repaired. Neither last forever either, so they will each need to be replaced at least once, if not more often, during the 30 years the house is being paid down. A majority of new construction and a large number of existing homes are part of a Home Owners Association (HOA). Being part of an HOA not only increases monthly cost due to HOA fees, but also exposes a homeowner to the risk of special assessments. These are one-time financial obligations on homeowners to cover some expense for the HOA.\nWhile a renter may be immune to costs such as these, they are not immune to changing rent prices. By having a fixed-rate mortgage on the other hand, monthly housing expenses stay locked at current rates. Furthermore, while rent is not tax deductible, mortgage interest is. Under current guidelines, up to 750,000 USD in mortgage interest on your primary home can be deducted from your income tax. This is especially valuable during the first 15 years of the loan, when interest accounts for the majority of the mortgage payment.\nHousing as an Investment One issue that we have ignored so far is the idea of home purchases as investments. Home prices have historically increased. Each year lived and mortgage paid on a property increases your share in the home\u0026rsquo;s value, otherwise known as equity. Some would argue that even if the monthly cost of homeownership is in excess of the cost of renting an equivalent home, it\u0026rsquo;s worth it in the long run as an investment in your financial future. Proponents may point to homeownership being a key component in long term wealth accrual.\nAssessing the value of equity growth in wealth building needs to be viewed in opposition to investing the potential price differential. We are only talking about the difference in housing cost being utilized here, not in using additional funds to pay down a mortgage early. This has not been an effective strategy for nearly 30 years.1 The S\u0026amp;P 500\u0026rsquo;s average annual return during the period of 1975-2021 has been 10.2%. Home values on the other hand increased annually by an average of only 4.5% between 1975 and today.2 This would indicate investment in the S\u0026amp;P 500 would be expected to outperform real estate investment for the average person over the long run.\nThe variability of the two is rather different, however. For comparison, we can show a distribution of annual returns from the S\u0026amp;P 500 compared to FRED\u0026rsquo;s All-Transactions House Price Index.3\n Histograms showing the distribution of annual returns of the All-Transactions House Price Index for North Carolina and the S\u0026amp;P 500 for 1976-2021.   While this shows that we can expect greater payoff from stock market investments compared to real estate, this is not helpful if we have to pay for housing and don\u0026rsquo;t have excess funds to throw at the stock market. If the actual monthly cost of homeownership and rent are about equal and max out our available funds, then purchasing a home builds at least some equity, no matter how small, compared to no investment being made. Only when rent is lower than mortgage costs and fees does it make sense to start comparing rates of return.\nWe should never underestimate the power of compounding, however. Consider the following example: you have the option of purchasing a 400,000 USD home with a 80,000 USD (20%) down payment at 5% interest as a 30 year fixed rate mortgage. Assume no other costs. In total, this housing purchase will cost you approximately 698,000 USD over the course of 30 years. Assuming 4.53% annual increase in housing value, this leaves you with about 1,553,000 USD in equity at the end. Given your total cost you have made a profit of approximately 855,000 USD. Now compare this to stock market returns, assuming the 10.22% annual growth we found above. Investing the down payment only, with no further payments made, would leave you with approximately 1,695,000 USD after 30 years. To match the amount of equity in our example home after 30 years would require only a 73,500 USD in initial investment. If we only wanted to match the profit of about 855,000 USD with a one-time investment, we would require an initial investment of approximately 42,500 USD.\nDespite these superior returns from investment, we still need to live somewhere. I have already mentioned that unlike a mortgage payment, rent is not fixed over the 30 year timespan. How much should I expect renting to cost me? Let\u0026rsquo;s assume we invest the entire down payment and rent a home that costs exactly the same as the mortgage would have cost. Assuming once annual increases in rent of 4.17%, we spend a total of nearly 1,190,000 USD on rent over the course of 30 years. Given these expenses, the profit from investing the 80,000 USD shrinks to a mere 500,000 USD.\nThis suggests a strategy of minimizing the down payment and investing the difference, if possible. One downside of this is that having a smaller down payment may lead to less favorable mortgage terms. In this second scenario, let\u0026rsquo;s assume that we make a 40,000 USD down payment on a 400,000 USD home with an increased, but still fixed, mortgage rate of 5.25% over 30 years coupled with a 40,000 USD initial investment in the S\u0026amp;P 500 at 10.22% increase annually with no additional investments made. Here we invest a total of approximately 766,000 USD into our home, reducing our profit on the home purchase to approximately 797,000 USD. But our initial 40,000 USD investment has accumulated to about 847,000 USD. This brings our total profit to approximately 1,644,000 USD in this scenario - nearly double the 855,000 USD profit in the 80,000 USD down payment scenario.\nLast Thoughts Overall, we see that the more precise we want to be, the more difficult it becomes to estimate an ideal strategy as there are a lot of moving pieces, not all of which can best be approximated by a geographic average value. Given the very different variability in housing value increase compared to stock market returns, it may make sense to generate a large number of random walks to get a better feel for the distribution of outcomes after 30 years. Finally, this entire post has assumed that we remain living in the same home for the entire 30 year life of the mortgage. Many will want to move at some point during their next thirty years, perhaps to make room for a larger family, to down-size, or to follow a job opportunity. This creates additional costs and considerations for deciding between renting and owning a home.\n  Average mortgage rates have been below average annual return of the S\u0026amp;P 500 since the 1990s. \u0026#x21a9;\u0026#xfe0e;\n Annual returns of S\u0026amp;P 500 were calculated from data provided by Investopedia. Annual returns of housing prices were calculated from the All-Transactions House Price Index for North Carolina (FRED time series NCSTHPI). \u0026#x21a9;\u0026#xfe0e;\n I have chosen this measure for historic house price increases as this index is available on a quarterly basis starting in 1975, unlike Zillow data which only goes back a little more than 20 years. Both US wide data (USSTHPI) and state specific data (e.g., for North Carolina: NCSTHPI) are available from FRED. I will use the North Carolina data set for my comparison. An alternative measure to consider is the median sales price of houses sold in the South census region, available on a quarterly basis starting in 1965 from FRED (MSPS). \u0026#x21a9;\u0026#xfe0e;\n   ","date":1661026898,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661026898,"objectID":"08ab9bcbe6c63dd6afef4b9eebf76c94","permalink":"https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/","publishdate":"2022-08-20T16:21:38-04:00","relpermalink":"/post/22-08-should-you-buy-or-rent/","section":"post","summary":"This post is a follow-up to my post on how to load data from Zillow. Housing prices have soared through the COVID-19 pandemic, leading to a lot of discussion about housing affordability.","tags":["analysis","housing","mortgage","zillow"],"title":"Is it better to buy or rent housing?","type":"post"},{"authors":[],"categories":[],"content":"A big difference between using compiled languages like C/C++ compared to scripting languages like Javascript or Python is that prior to execution, compiled languages require an explicit compilation step where the human readable code is translated to machine code for execution, the so-called \u0026ldquo;binary\u0026rdquo; of the code. Typically, a separate binary needs to be compiled for each target operating system and architecture. Compiling for your own machine is not a problem. The difficulty lies in creating binaries for machines that you don\u0026rsquo;t normally use, so you might not have an extra Mac lying around just to compile your program on for other Mac users. Compiling programs for an operating system or architecture other than the one you are working with is called cross-compiling. This would allow a Linux developer to create binaries for Windows and Mac computers, for example.\nFor most languages, this requires installing additional development tools and increases the complexity of the compilation workflow. I have found Go to be a pleasant exception to this, because cross-compilation is built into the standard Go tools. There is no need to learn any additional build-tools. All you need to learn about are some system variables that you need to set when compiling.\nGo build tools know which system you are building for by checking the GOOS and GOARCH environment variables. If they are unset, the tools fall back to GOHOSTOS and GOHOSTARCH. In other words, to change the target OS/architecture for your build, all you have to do is set the GOOS and GOARCH variables during the build. So say you want to build a simple program hello.go for a Windows computer with the same architecture as your development machine. All you have to do is write\nGOOS=windows go build hello.go  instead of just go build hello.go and you\u0026rsquo;re good to go. This would produce a hello.exe binary you could copy to a Windows machine to run.\nTo check what combinations of GOOS and GOARCH are valid, run go tool dist list. To see which environment variables Go is currently seeing, run go env.\n","date":1660223700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660223700,"objectID":"4a27a4c46b028954ec07dc0c83aac30a","permalink":"https://dmsenter89.github.io/post/22-08-cross-compiling-with-go/","publishdate":"2022-08-11T09:15:00-04:00","relpermalink":"/post/22-08-cross-compiling-with-go/","section":"post","summary":"When using compiled languages like C/C++, Go, Rust, etc. a separate binary needs to be created for every OS and architecture the software is meant to run on. Cross-compilation can be daunting at first. In Go, it is built-in and straightforward to use.","tags":["cross-compiling","go"],"title":"Cross Compiling With Go","type":"post"},{"authors":[],"categories":[],"content":"Zillow is a well-known website widely used by those searching for a home or curious to find out the value of their current home. What you may not know is that Zillow has a dedicated research page. To make their website work optimally, they churn through tons of data on the American housing market. They share insights they gleaned via zillow.com/research. If you visit their research website you\u0026rsquo;ll notice they have a data page where you can download some really cool data sets for your own research. They even have an API with which you can load data directly, but you\u0026rsquo;ll have to register for access. In this post, we\u0026rsquo;ll look at how to load the CSV files that are available for direct download into SAS for analysis.\nThe CSV files can be downloaded here. In the example below, I\u0026rsquo;m working with the Zillow Home Value Index file for all homes, seasonally adjusted at the ZIP code level. Tha file is fairly large. It has data going from January 2000 through June 2022 in more than 27,000 rows of data and about 280 columns. Below is an image of the beginning of this file.\nWhen working with large CSV files, I find it useful to get a feel for it in the CLI with csvkit. This is especially important when importing with a SAS data step, because we need to know the number of columns and their order, amongst other things, for our code. To get an overview of the total number of columns and their contents, run\ncsvcut -n Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv  The output is fairly long, so you may prefer piping to a pager. I don\u0026rsquo;t need all the different identifiers in the file, so I\u0026rsquo;m going to exclude those I won\u0026rsquo;t need and put them into a separate, smaller CSV.\n# ignore these four columns which I won't need csvcut -C RegionID,SizeRank,RegionType,StateName Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv \u0026gt; Zip_zhvi_small.csv # alternatively, also cut down on date columns to only 2022 for debugging csvcut -C RegionID,SizeRank,RegionType,StateName,10-273 Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv \u0026gt; Zip_zhvi_small.csv  You can also reduce the file size by using csvgrep to filter any of the columns. For example, if we only wanted the data for North Carolina we could run csvgrep -c State -m NC in the pipe.\nFor SAS, we need to know the maximum length of string columns so we can allocate the appropriate length to the corresponding SAS variables. This is easily done with the csvstat tool:\ncsvcut -c Metro,City,CountyName Zip_zhvi_small.csv | csvstat --len  You can also specify the list of columns in csvstat directly, but in my experience that tends to be slower.\nAlright, now we have everything we need to start on our DATA step! We start with the attribute statement. One problem with importing this file is that everyhing is in wide format, with the dates used as headers. We will get around this shortly. I have seen people use transpose etc for similar problems online, but this is unnecessary if we feel comfortable with the DATA step. We\u0026rsquo;ll start by naming the identifying columns just as in the CSV file. For the date columns, we will use a numeric range prefixed by date (date1-date270). You can use csvcut to find the exact number of date columns you have. We will also allocate the same number of columns for the ZHVI values, so we\u0026rsquo;ll need to add a val1-val270. This and the date variable are temporary and will be dropped later, in favor of the Date and ZHVI variables.\nattrib ZIP informat=best12. format=z5. State informat=$2. City informat=$30. Metro informat=$42. CountyName informat=$29. date1-date270 informat=YYMMDD10. format=DATE9. val1-val270 informat=best16. Date format=Date9. ZHVI format=Dollar16. ;  Now we will allocate an array to hold all of the date and ZHVI values during the processing of each row. Since the date column won\u0026rsquo;t change, we\u0026rsquo;ll tell SAS to retain its values.\nretain date1-date270; array d(270) date1-date270; array v(270) val1-val270;  This is where the magic happens now. You may not know it, but you are not limited to a single INPUT statement in a DATA step. We use this and start by reading in only the first row. Because we use an OUTPUT statement later, this reading of row 1 will be processed, but not saved into the output data set.\nif _n_ = 1 then do; input ZIP $ State $ City $ Metro $ CountyName $ date1-date270; PUT _ALL_; /* if you want to see what that looks like */ end;  With this if clause, the date1 through date270 variables will be populated, and because we used a retain statement earlier, these values remain available to us during the processing of every other row. You can probably guess where this is going now: we will process each row, and then OUTPUT one line per date which we have access to now thanks to our array and the retain statement.\ninput ZIP $ State $ City $ Metro $ CountyName $ val1-val270; do i=1 to 270; Date = d(i); /* look up date for column i */ ZHVI = v(i); /* use the corresponding i-th value for ZHVI */ OUTPUT; /* This output creates one line per date column */ end;  At the end of your data step, don\u0026rsquo;t forget to\ndrop i date1-date270 val1-val270;  so those variables don\u0026rsquo;t clutter your data set. And that\u0026rsquo;s it! You now have the data set loaded and available in SAS.\n","date":1659388898,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659388898,"objectID":"880e34ba184c2fff20cd2b6f2d36fa30","permalink":"https://dmsenter89.github.io/post/22-08-zillow-data/","publishdate":"2022-08-01T17:21:38-04:00","relpermalink":"/post/22-08-zillow-data/","section":"post","summary":"Zillow is a well-known website widely used by those searching for a home or curious to find out the value of their current home. What you may not know is that Zillow has a dedicated research page.","tags":["csv","data-step","finding-data","housing","import","sas","zillow"],"title":"Loading Zillow Housing Data in SAS","type":"post"},{"authors":["Anusha Doshi","Elizabeth Senter, MPH","Owen Queen","D. Michael Senter, PhD","Susan Keen, MD, MSCR","Kaitlin Shartle, MA","Ross J. Simpson Jr., MD, PhD"],"categories":[],"content":"Background Sudden death accounts for 10% of deaths in the United States. Prior research has focused on sudden death in older victims, leaving much unknown about risk factors for younger, working age adults. Compared to older adults, younger adults may be more vulnerable to genetic factors. Understanding age related differences in sudden death risk factors may guide future prevention efforts, as the factors contributing to sudden death in younger patients may warrant different types of prevention than those affecting older adults.\nMethods From 2013-2015, out-of-hospital deaths among adults aged 18-64 in Wake County, NC were screened and adjudicated to identify 306 sudden death victims. A comparison group of 1,113 patients matched for age, gender, and residence was formed by randomly sampling individuals from the Carolina Data Warehouse. For sudden death victims and controls, the prevalence of sudden death risk factors, comprising comorbidities, mental illnesses, and family history variables, were assessed in three age groups (18-41, 42-54, and 55-64 years old). Hypothesis testing was conducted for each variable across all pairwise combinations of age groups using an unpaired, two-sample proportion Z-test with $p\u0026lt;0.05$ as statistically significant.\nResults None of the variables analyzed were more prevalent in younger sudden death victims compared to older victims. However, family history variables were largely missing (\u0026gt;80%) compared to comorbidities and mental illnesses (\u0026lt;10%), with younger adults being disproportionately affected.\nConclusions Underreporting of family history in medical records leaves younger adults as a poorly understood subset of sudden death victims who are currently unable to be appropriately screened for lifesaving preventive measures.\nPublic Health Implications Better family history documentation may identify younger adults at higher risk of sudden death, allowing more comprehensive population level screening and implementation of prevention measures appropriate for addressing genetic factors instead of the environmental factors more common in older adults.\n","date":1655301600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655301600,"objectID":"19ab7286622efe35b7d874c41270c248","permalink":"https://dmsenter89.github.io/publication/doshi-2022-family-history/","publishdate":"2022-08-15T11:27:55-04:00","relpermalink":"/publication/doshi-2022-family-history/","section":"publication","summary":"Background Sudden death accounts for 10% of deaths in the United States. Prior research has focused on sudden death in older victims, leaving much unknown about risk factors for younger, working age adults.","tags":[],"title":"Family History And Chronic Medical Conditions Associated With Sudden Death Among Working Age Adults","type":"publication"},{"authors":[],"categories":[],"content":"Recently I\u0026rsquo;ve needed to capture the entries of a datalines statement in SAS for editing. Generally, this is a straight forward problem if I only need to do it with one file or all of the files that I am using are formatted identically. But then I started thinking about the more general case. SAS doesn\u0026rsquo;t care about the case of my keywords, so I need a case insensitive match. I need to account for possible extra whitespace. So far so good. But what if I have two different keywords that can start my data section, and the end of the data section is indicated with different characters depending on the chosen keyword? Could I still use a single regular expression?\nSAS does in fact allow a number of different keywords to enter data in a data step. In my experience, the most common are the datalines and datalines4 statements. The main difference between them is how the end of the data is indicated. For datalines, a single semicolon is used, while datalines4 uses a sequence of four semicolons, thereby allowing the use of semicolons in the data itself. There are some aliases for these commands that can be used: cards/lines and cards4/lines4 with matching behavior. A simple data step with these statements could look like this:\ndata person; input name $ sex $ age; datalines; /* or `cards` or `lines` */ Alfred M 14 Alice F 13 ; data person4; input name $ sex $ age; datalines4; /* or `cards4` or `lines4` */ Alfred M 14 Alice F 13 ;;;;  We could write two separate RegEx expressions, one for the datalines/cards/lines statement and a second one for the datalines4/cards4/lines4 statement. But, if the RegEx engine we are using allows conditionals, e.g. the Python RegEx engine, then we can write a single statement that can capture both types of statements. The basic format of the conditonal capture is (?(D)A|B), which can be read as \u0026ldquo;if capture group D is set, then match A, otherwise match B.\u0026rdquo; For more details, see here.\nUsing this technique, we can capture both types of statements in one go. The short form of the solution I found is this regular expression: r\u0026quot;(?:(?:(?:data)?lines)|cards)(4)?\\s*;(.*?)(?(1);{4}|;)\u0026quot; with two flags set: case insensitive and dot-all. If we utilize Python\u0026rsquo;s verbose flag, we can format this a bit nicer as well:\nre.compile( r\u0026quot;\u0026quot;\u0026quot;(?:(?: # mark groups as non-capture groups (?:data)? # maybe match `data`, but don't capture lines) # matches `lines` |cards) # alternatively, matches `cards` (4)? # a `4` may be present \\s*; # there might be whitespace before the ; (.*?) # lazy-match data content (?(1) # check if capture group 1 is set, if so ;{4} # match `;;;;` |;) # otherwise, match a single ; \u0026quot;\u0026quot;\u0026quot;, flags=re.DOTALL | re.X | re.I)  A great website to help you build up a regular expression is regex101.com. It allows you to copy a sample text and regular expression. It then explains your expression and lists the capture groups by number, which can be convenient. It also allows you to try out different RegEx engines. Try setting it to Python with the flags we mentioned, and see how it works!\n","date":1653011146,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653011146,"objectID":"2ed2889b7cb3cf52a773fd08f0e4fd16","permalink":"https://dmsenter89.github.io/post/22-05-conditional-regex-python/","publishdate":"2022-05-19T21:45:46-04:00","relpermalink":"/post/22-05-conditional-regex-python/","section":"post","summary":"When the endpoint of your match depends on an earlier term, try conditional regex matching in Python.","tags":["datalines","python","regex"],"title":"Conditional RegEx Matching with Python","type":"post"},{"authors":[],"categories":[],"content":"Lately I\u0026rsquo;ve been playing around with Go. I\u0026rsquo;ve read about Go for a few years and have been using some software written in Go (this website is built with Hugo), but never tried it before. So what better way to give Go a shake than to write some code. Since Wordle has been popular, I thought I\u0026rsquo;d write a very simple Wordle implementation in Go; you can check it out on GitHub. It\u0026rsquo;s been a good way for me to get familiar with some of the basisc of Go, such as variables and their types, functions, etc. So far I\u0026rsquo;ve been enjoying it.\nThe Go website has a very nicely written documentation and package page. The Go Playground let\u0026rsquo;s you test out Go in your browser without needing to install anything. I\u0026rsquo;ve also found Bodner\u0026rsquo;s \u0026ldquo; Learning Go\u0026rdquo; to be helpful.\nGo is a compiled language with a pretty picky compiler. It won\u0026rsquo;t let you compile code with unnecessary imports and variable declarations, which help keeps your code clean. Cross-compilation is built-in. While Go is not a common language in scientific computing, the gonum package has implemented a number of important functions and seems to be well developed. I look forward to learning more about Go in the future.\n","date":1651782600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651782600,"objectID":"26fdc91186679e7948b3a64a1f0fb8e2","permalink":"https://dmsenter89.github.io/post/22-05-go-wordle/","publishdate":"2022-05-05T16:30:00-04:00","relpermalink":"/post/22-05-go-wordle/","section":"post","summary":"Learning the basics of Go with a Wordle app.","tags":["go","wordle"],"title":"Wordle in Golang","type":"post"},{"authors":[],"categories":[],"content":"I frequently find myself needing to concatenate data sets but also wanting to be able to distinguish which row came from which data set originally. Introductory SAS courses tend to teach the in keyword, for a workflow similar to this:\ndata Concat1; set data1(in = ds0) data2(in = ds1); if ds0 then source = \u0026quot;data1\u0026quot;; else if ds1 then source = \u0026quot;data2\u0026quot;; run;  With more than two input data sets, this can get unwieldy and repetitive. In an old blog post on Rick Wicklin\u0026rsquo;s DO LOOP, a better method is introduced - the indsname option. Using this method, the above code looks much nicer:\ndata Concat2; set data1-data2 indsname = source; /* the INDSNAME= option is on the SET statement */ libref = scan(source,1,'.'); /* extract the libref */ dsname = scan(source,2,'.'); /* extract the data set name */ run;  As long as your input data sets are reasonably named, you\u0026rsquo;ll now have access to all the information needed.\n","date":1650469322,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650469322,"objectID":"c8fba1fdad656c07292e0b4bae144aa5","permalink":"https://dmsenter89.github.io/post/22-04-sas-indsname-option/","publishdate":"2022-04-20T11:42:02-04:00","relpermalink":"/post/22-04-sas-indsname-option/","section":"post","summary":"I frequently find myself needing to concatenate data sets but also wanting to be able to distinguish which row came from which data set originally. Introductory SAS courses tend to teach the in keyword, for a workflow similar to this:","tags":["data-step","merging","sas"],"title":"The INDSNAME Option in SAS","type":"post"},{"authors":[],"categories":[],"content":"In a previous post, I have shown how to connect to the Census API and load data with Python. In this post, I will do the same using SAS instead. Before we get started, two important links from last time: a guide to the API can be found here and a list of the available data sets can be accessed here.\nPicking the Data For this post, I\u0026rsquo;ll use the same data as last time. There we used the 2018 American Community Survey 1-Year Detailed Table and asked for three variables - total population, household income, and median monthly cost for Alamance and Orange counties in North Carolina (FIPS codes 37001 and 37135). The variable names are not very intuitive, so I highly recommend starting your code with a comment section that includes a markdown-style table of the variables that you want to use. Here is an example table for our data:\n   Variable Label     B01003_001E Total Population   B19001_001E Household Income (12 Month)   B25105_001E Median Monthly Housing Cost    Building the Query The next step is to build the query. Like last time, the API consists of a base URL that points us to the data set we are looking for, a list of the variables we want to request, and a description of the geography for which we want to request those variables. Just like last time, I\u0026rsquo;ll build the query using several macros for flexibility purposes. Note that since \u0026amp; has a special meaning in SAS, we need to use %str(\u0026amp;) when referring to it to avoid having the log clobbered with warnings about unresolved macros.\n%let baseurl=https://api.census.gov/data/2018/acs/acs1; %let varlist=NAME,B01003_001E,B19001_001E,B25105_001E; %let geolist=for=county:001,135%str(\u0026amp;)in=state:37; %let fullurl=\u0026amp;baseurl.?get=\u0026amp;varlist.%str(\u0026amp;)\u0026amp;geolist.; %put \u0026amp;=fullurl;  Your log should now show the full query URL:\nFULLURL=https://api.census.gov/data/2018/acs/acs1?get=NAME,B01003_001E,B19001_001E,B25105_001E\u0026amp;for=county:001,135\u0026amp;in=state:37  Making the API Request The API call is achieved with a simple PROC HTTP call using a temporary file to hold the response from the server.\nfilename response temp; proc http url=\u0026quot;\u0026amp;fullurl.\u0026quot; method=\u0026quot;GET\u0026quot; out=response; run;  Handling the JSON Response We read the JSON response by utilizing the LIBNAME JSON Engine in SAS.\nlibname manual JSON fileref=response;  Now run proc datasets lib=manual; quit;. You\u0026rsquo;ll see two data sets that were created: ALLDATA which contains the whole JSON file\u0026rsquo;s contents in a single data set, and ROOT which is a data set of all the root-level data. The latter one is the one we want. Here\u0026rsquo;s what the first few observations in each look like:\n  First few observations in the automatically created data sets.   Just like with Python, all columns are treated as character variables at first. Because of the way the Census API is structured, the first row consists of headers, which SAS didn\u0026rsquo;t use. This is something we\u0026rsquo;ll need to fix. At this point we have two main routes we can use to fix these issues - we can manually create a new data set from ROOT with PROC SQL and address the issues in that way, or we can take advantage of SAS' JSON map feature to define how we want to load the JSON when the LIBNAME statement is executed. There are good use cases for each, so I will show both methods.\nCleaning up via PROC SQL Using PROC SQL, you can rename all the character variables you want to keep. To change from character to numeric, you\u0026rsquo;ll use the input function. You can then assign formats and labels as desired. To get rid of the first row, you can just add a conditional having ordinal_root ne 1 to avoid loading that line.\nproc sql; create table census as select element1 as Name, input(element2, best12.) as B01003_001E format=COMMA12. label='Total Population', input(element3, best12.) as B19001_001E format=DOLLAR12. label='Household Income (12 Month)', input(element4, best12.) as B25105_001E format=DOLLAR12. label='Median Monthly Housing Cost', element5 as state, element6 as county from manual.root having ordinal_root ne 1; quit;    Result from the PROC SQL method.   A benefit of this method is that as you fix the input table, you can already begin to work with it thanks to the calculated keyword in PROC SQL. Say we weren\u0026rsquo;t actually interested in housing cost and household income, but instead would like to know what percent of their annual income a household spends on housing in a given county. We could just add a new variable to our PROC SQL call and build our table like this:\nproc sql; create table census as select element1 as Name, input(element2, best12.) as B01003_001E format=COMMA12. label='Total Population', input(element3, best12.) as B19001_001E format=DOLLAR12. label='Household Income (12 Month)', input(element4, best12.) as B25105_001E format=DOLLAR12. label='Median Monthly Housing Cost', /* Now calculate what we want from the new columns: */ 12*(calculated B25105_001E)/calculated B19001_001E as HousingCostPCT format=PERCENT10.2, element5 as state, element6 as county from manual.root having ordinal_root ne 1; quit;  Using a JSON MAP Alternatively, we could change the way SAS reads the JSON data by editing the JSON map it uses to decode the JSON file. The first step is to ask SAS to create a map for us to edit:\nfilename automap \u0026quot;sas.map\u0026quot;; libname autodata JSON fileref=response map=automap automap=create;  The map will look something like this:   Beginning of the automatically created JSON map.   Note that this is also a JSON file which you can edit in a text editor. With this map, you can change the names of the data sets and variables, assign labels and formats, and also re-format incoming data. Variables and data sets you don\u0026rsquo;t want to read can simply be deleted from the map. Here\u0026rsquo;s the beginning of my edited file:   Beginning of my edited JSON map.   Since the first row of observations in the JSON are actually a header and non-numeric, I add ? prior to the specified informat. This prevents errors in the log and simply replaces non-matching variables with missing values. We can now reload the JSON using our custom map by dropping the automap=create option from the LIBNAME statement:\nlibname autodata JSON fileref=response map=automap;  When I now print the resulting data set, the header row is still there, but replaced by missing values in numeric columns:   The data set as a result of the edited JSON map.   This means we\u0026rsquo;ll need to additionally drop this row in a separate step using a delete statement either in a PROC SQL or DATA step.\nWhichever method you choose, you now can access data via an API call from SAS. Happy exploring!\n","date":1649852855,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649852855,"objectID":"cbcbcdb9cb6071929f51ad2bc6b2a017","permalink":"https://dmsenter89.github.io/post/22-04-census-api-with-sas/","publishdate":"2022-04-13T08:27:35-04:00","relpermalink":"/post/22-04-census-api-with-sas/","section":"post","summary":"A post showing how PROC HTTP and LIBNAME JSON can be used to directly work with the Census API from SAS.","tags":["acs","api","census","json","sas"],"title":"Working with the Census API Directly from SAS","type":"post"},{"authors":["Sahana Raghunathan","D. Michael Senter, PhD","Elizabeth Senter, MPH","Susan Keen, MD, MCSR","Kaitlin Shartle, MA","Ross J. Simpson Jr., MD, PhD"],"categories":[],"content":"Background In the United States, former incarceration is a risk factor for chronic conditions and sudden death (SD) due to poor healthcare continuity after release and lack of community support. During the COVID-19 pandemic, all-cause mortality increased, and preexisting risk factors and social limitations of having an incarceration history were exacerbated. We hypothesized that sudden deaths among the formerly incarcerated increased during the pandemic.\nMethods North Carolina death certificates from pre-COVID-19 (2014) and during the COVID-19 pandemic (2020) were screened for presumed SD. Individuals were excluded based on age ($\u0026lt;18$ or $\u0026gt;65$), violent or expected deaths, and deaths in hospitals or care facilities. Deaths were matched to the North Carolina Department of Public Safety Criminal Offender Database for a history of incarceration. ICD-10 codes for hypertension, diabetes, chronic respiratory disease, obesity, mental health, and substance abuse were extracted from the top four causes of death on the death certificates.\nResults We found no significant difference in the prevalence of former incarceration in SD victims from 2014 to 2020. In 2020, the odds of substance abuse among SD victims with a history of incarceration were significantly greater compared to those without a history of incarceration (OR (95CI): 2.29 (1.91-2.73)). The odds of substance abuse among the formerly incarcerated were greater in 2020 SD victims compared to 2014 SD victims (2.29 (1.76-2.99)).\nConclusion Sudden death among the formerly incarcerated did not significantly increase during the COVID-19 pandemic. Increased public health funding may have limited the expected effects of COVID-19 on rates of sudden death in formerly incarcerated individuals. However, the formerly incarcerated appear to be vulnerable to dying of sudden death associated with substance abuse in 2020. Improving statewide transition programs targeting substance abuse counseling during healthcare crises should improve health outcomes and reduce the rate of sudden death among the formerly incarcerated population.\n","date":1648908000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648908000,"objectID":"df56695aab00e8376d4cc9014a2f688a","permalink":"https://dmsenter89.github.io/publication/raghunathan-2022-incarceration/","publishdate":"2022-08-15T11:27:55-04:00","relpermalink":"/publication/raghunathan-2022-incarceration/","section":"publication","summary":"Background In the United States, former incarceration is a risk factor for chronic conditions and sudden death (SD) due to poor healthcare continuity after release and lack of community support. During the COVID-19 pandemic, all-cause mortality increased, and preexisting risk factors and social limitations of having an incarceration history were exacerbated.","tags":[],"title":"Former Incarceration As A Risk Factor For COVID-19 Associated Sudden Death","type":"publication"},{"authors":["Hannah Vrooman, BS","Elizabeth Senter, MPH","Kaitlin Shartle, MA","Susan Keen, MD, MSCR","Catherine Sauter","D. Michael Senter, PhD","Ross J. Simpson Jr., MD PhD"],"categories":[],"content":"Background Housing insecurity is a powerful social determinant of health that is associated with increased all-cause mortality. The health consequences of and contributors to housing insecurity are poorly studied, which makes preventative care elusive for this population. In order to address these issues, we assessed the prevalence of housing insecurity among sudden death victims and examined its relationship to sudden death, mental illness, and clinical comorbidities.\nMethods From 1 March 2013 to 28 February 2015, out-of-hospital deaths in Wake County, North Carolina, were screened and adjudicated to identify 399 sudden deaths among residents between the ages of 18 and 64. A control sample of 1,101 living patients were generated by randomly sampling for age, gender, and Wake County residence from the Carolina Data Warehouse for Health. Housing status was abstracted from clinical records from sudden death victims and controls.\nResults Housing insecurity was documented in 28 (7.1%) of victims and 47 (4.3%) of controls (OR(95CI): 1.71(1.05-3.2)). This difference remained significant after adjusting for hypertension, age, and diabetes. However, when additionally adjusting for depression, anxiety, alcohol abuse, substance abuse, schizophrenia, and bipolar disorder, the increased prevalence of housing insecurity in the sudden death group became insignificant.\nConclusions Housing insecure individuals experience a higher burden of sudden death than housing secure individuals. Mental illness appears to confound the relationship between housing insecurity and sudden death. Treating underlying mental illness may be a path towards specializing clinical care to optimize health outcomes for housing insecure individuals.\nClinical Implications Optimize health outcomes for housing insecure individuals by acknowledging that housing insecurity appears to be a risk factor for sudden death - a relationship complicated by mental illness. Patients with housing insecurity should be screened for mental illness, and treatment and referral to a mental illness specialist should be considered.\n","date":1648908000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648908000,"objectID":"99111dbe47438dc1d72cd0ec752965f8","permalink":"https://dmsenter89.github.io/publication/vrooman-2022-housing-insecurity/","publishdate":"2022-08-15T11:27:55-04:00","relpermalink":"/publication/vrooman-2022-housing-insecurity/","section":"publication","summary":"Background Housing insecurity is a powerful social determinant of health that is associated with increased all-cause mortality. The health consequences of and contributors to housing insecurity are poorly studied, which makes preventative care elusive for this population.","tags":[],"title":"Housing Insecurity: Effects on Sudden Death and Interaction with Mental Illness","type":"publication"},{"authors":[],"categories":[],"content":"I\u0026rsquo;ve recently needed to append several lines of data to a SAS data step that I collected and built via a shell script. For search-and-replace in bash I typically use sed, but this time I ran into a problem - sed does not like multiline shell variables. Thanks to Stack, I found a way to accomplish this task using awk instead.\nSuppose you have a file called data.sas with the following contents:\ndata person; infile datalines delimiter=','; input name :$10. dept :$30.; datalines4; John,Sales Mary,Accounting Theresa,Management Stewart,HR ;;;; run;  Note that I am using a datalines4 statement so that I get an easy to identify target for the substitution. I want to insert a multiline shell variable before the ;;;; to add my data to this data step. Say I have the following variable:\nNEWDATA=$(cat \u0026lt;\u0026lt;-END Will,Compliance Sidney,Management END )  If I try to use sed (sed \u0026quot;s/\\;\\{4\\}/$DATA\\n;;;;/\u0026quot; data.sas) I will get an error about an unterminated s command. Instead of sed, I can use awk with a variable to achieve the same goal:\nawk -v r=\u0026quot;$NEWDATA\\n;;;;\u0026quot; '{gsub(/;{4}/, r)}1' data.sas  The one downside is that awk does not have an in-place option like sed, and if I try to redirect to the same file I\u0026rsquo;m reading from I get an empty file out. So you\u0026rsquo;ll have to rename the original file in your processing script to achieve a similar effect as with the inplace option in sed.\nFor additional approaches, see this StackOverflow Question.\n","date":1647436992,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647436992,"objectID":"6290406c362731acc4be266a77db0d72","permalink":"https://dmsenter89.github.io/post/22-03-multiline-replacement/","publishdate":"2022-03-16T09:23:12-04:00","relpermalink":"/post/22-03-multiline-replacement/","section":"post","summary":"I\u0026rsquo;ve recently needed to append several lines of data to a SAS data step that I collected and built via a shell script. For search-and-replace in bash I typically use sed, but this time I ran into a problem - sed does not like multiline shell variables.","tags":["awk","bash","data-step","sed"],"title":"Multline Bash Variable Replacement","type":"post"},{"authors":[],"categories":[],"content":"I love using SASPy, but the setup can take a minute. I used to do the setup via the CLI until I started thinking I might be able to just do it straight from a Jupyter notebook. Having just a couple of cells in Jupyter notebook makes for easy copy-and-paste and reduces setup time. The code below has been tested on both Windows and Linux. As a bonus, this also works on Google Colab.\nYou can easily install packages via pip from Jupyter either by using a shell cell (!) or by using the pip magic command: %pip install saspy. Once done, copy and paste the following into a code cell and run to create the sascfg_personal.py file:\nimport saspy, platform from pathlib import Path # get path for configuration file cfgpath = saspy.__file__.replace('__init__.py','sascfg_personal.py') # To pick the path for Java, we need to know whether we're on Windows or not if platform.system()=='Windows': print(\u0026quot;Windows detected.\u0026quot;) javapath = !where java authfile = Path(Path.home(),\u0026quot;_authinfo\u0026quot;) else: javapath = !which java authfile = Path(Path.home(),\u0026quot;.authinfo\u0026quot;) # the `!` command returns a string list, we want only the string javapath = javapath[0] print(f\u0026quot;Java is present at {javapath}\u0026quot;) # US home Region configuration string set up via string-replacement. # For other server addresses, see https://support.sas.com/ondemand/saspy.html cfgtext = f\u0026quot;\u0026quot;\u0026quot;SAS_config_names=['oda'] oda = {{'java' : '{repr(javapath).strip(\u0026quot;'\u0026quot;)}', #US Home Region 'iomhost' : ['odaws01-usw2.oda.sas.com','odaws02-usw2.oda.sas.com','odaws03-usw2.oda.sas.com','odaws04-usw2.oda.sas.com'], 'iomport' : 8591, 'authkey' : 'oda', 'encoding' : 'utf-8' }}\u0026quot;\u0026quot;\u0026quot; # write the configuration file with open(cfgpath, 'w') as file: file.write(cfgtext) print(f\u0026quot;Wrote configuration file to {cfgpath}\u0026quot;) print(f\u0026quot;Content of file: \\n```\\n{cfgtext}\\n```\u0026quot;)  Optionally, you can set up an authentication file with your username and password. Without this file, you\u0026rsquo;ll be prompted for your username and password each time you log in.\n# change variables to match your username and password omr_user_id = r\u0026quot;max.mustermann@sample.com\u0026quot; omr_user_password = r\u0026quot;K5d7#QBPw\u0026quot; with open(authfile, \u0026quot;w\u0026quot;) as file: file.write(f\u0026quot;oda user {omr_user_id} password {omr_user_password}\u0026quot;)  And that\u0026rsquo;s it! You\u0026rsquo;re now ready to connect to SASPy. In my experience you don\u0026rsquo;t even need to restart the kernel to begin work with SAS on ODA. You can try the following snippet in a new cell:\n# starts a new SAS session with the `oda` configuration we set up sas_session = saspy.SASsession(cfgname='oda') # load a SAS data set and make a scatter plot cars = sas_session.sasdata('cars', 'sashelp') cars.scatter(x='msrp', y='horsepower') # directly run SAS code to print a table sas_session.submitLST(\u0026quot;proc print data=sashelp.cars(obs=6); run;\u0026quot;) # quit SAS connection sas_session.endsas()  ","date":1647001829,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647433829,"objectID":"8c2613a506535c47e05b52d69adebcd5","permalink":"https://dmsenter89.github.io/post/22-03-saspy-setup/","publishdate":"2022-03-11T08:30:29-04:00","relpermalink":"/post/22-03-saspy-setup/","section":"post","summary":"I love using SASPy, but the setup can take a minute. I used to do the setup via the CLI until I started thinking I might be able to just do it straight from a Jupyter notebook.","tags":["jupyter","oda","saspy"],"title":"Easy SASPy Setup from Jupyter","type":"post"},{"authors":[],"categories":["data-cleaning"],"content":"Sometimes we have to deal with manually entered data, which means there is a good chance that the data needs to be cleaned for consistency due to the inevitable errors that creep in when typing in data, not to speak of any inconsistencies between individuals entering data.\nIn my particular case, I was recently dealing with a data set that included manually calculated ages that had been entered as a complete string of the number of years, months, and days of an individual. Such a string is not particularly useful for analysis and I wanted to have the age as a numeric variable instead. Regular expressions can help out a lot in this type of situation. In this post, we will look at a few representative examples of the type of entries I\u0026rsquo;ve encountered and how to read them using RegEx in SAS.\nLet\u0026rsquo;s Look at the Data   What we\u0026rsquo;re starting from.   If we look at our sample data, we notice a few things. The data is consistently ordered from largest to smallest, in the order of year, month, and day. For some lines, only the year variable is available. In all cases, the string starts with two digits.\nSeparation of the time units is inconsistent; occasionally they are separated by commas, sometimes by hyphens, and in some cases by spaces alone. The terms indicating the units are spelled and capitalized inconsistently as well. There are some abbreviations and occasionally the plural \u0026rsquo;s' in days is wrapped in parentheses.\nIf you want to follow along, you can create the sample data with the following code:\ndata raw; infile datalines delimiter = ',' MISSOVER DSD; attrib ID informat=best32. format=1. STR_AGE informat=$500. format=$500. label='Age String' VAR1 informat=best32. format=1.; input ID STR_AGE $ VAR1; datalines; 1,\u0026quot;62 Years, 5 Months, 8 Days\u0026quot;,1 2,43 Yrs. -2 Months -4 Day(s), 2 3,33 years * months 24 days, 1 4,58,1 5,\u0026quot;47 Yrs. -11 Months -27 Day(s)\u0026quot;,2 ; run;  The RegEx Patterns We will use a total of three regex patterns, one for each of the time units: year, month, day. SAS uses Pearl regex and the function prxparse to define the regex patterns that are supposed to be searched for.\nFor the year variable, we need to match the first two digits in our string. Therefore, the correct call is prxparse('/^(\\d{2}).*/'). Note that the ( and ) delimit the capture group.\nThe month and day regex patterns are very similar. For the months, we want to lazy-match the until we hit between one or two digits followed by an \u0026rsquo;m' and some number of other characters. We use the i flag since we cannot guarantee capitalization: prxparse('/.*?(\\d{1,2}).M.*/i'). The day pattern is nearly identical: prxparse('/.*?(\\d{1,2}).D\\D*$/i').\nWe can extract our matches using the prxposn function. We use the prxmatch function to check if we actually have a match:\n/* match into strings */ if prxmatch(year_rxid, STR_AGE) then year_dig_str = prxposn(year_rxid,1,STR_AGE); if prxmatch(month_rxid, STR_AGE) then month_dig_str = prxposn(month_rxid,1,STR_AGE); if prxmatch(day_rxid, STR_AGE) then day_dig_str = prxposn(day_rxid,1, STR_AGE);  The extracted strings can then be converted to numeric variables using the input function.\nThe last step is the calculation of the age from the three components. Since not all three time units are specified for every row, we cannot use the standard arithmetic of years + months + days, because the missing values would propagate. We need to use the sum function instead.\nPutting it all together, we get the correct output:\n  The Result   Complete Code data fixed; set raw; /* define the regex patterns */ year_rxid = prxparse('/^(\\d{2}).*/'); month_rxid = prxparse('/.*?(\\d{1,2}).M.*/i'); day_rxid = prxparse('/.*?(\\d{1,2}).D\\D*$/i'); /* match 2 digits followed by D and non-digit chars */ /* make sure we have enough space to store the extraction */ length year_dig_str month_dig_str day_dig_str $4; /* match into strings */ /* match into strings */ if prxmatch(year_rxid, STR_AGE) then year_dig_str = prxposn(year_rxid,1,STR_AGE); if prxmatch(month_rxid, STR_AGE) then month_dig_str = prxposn(month_rxid,1,STR_AGE); if prxmatch(day_rxid, STR_AGE) then day_dig_str = prxposn(day_rxid,1, STR_AGE); /* use input to convert str -\u0026gt; numeric */ years = input(year_dig_str, ? 12.); months = input(month_dig_str, ? 12.); days = input(day_dig_str, ? 12.); /* Use SUM function when calculating age to avoid missing values propagating */ age = sum(years,months/12,days/365.25); /* get rid of temporary variables */ drop month_rxid month_dig_str year_rxid year_dig_str day_rxid day_dig_str; run; proc print data=fixed; run;  ","date":1632937296,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632937296,"objectID":"b0eb41be5b3eb4768a2ee1465f7be9ae","permalink":"https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/","publishdate":"2021-09-29T13:41:36-04:00","relpermalink":"/post/21-09-sas-regex-date-cleanup/","section":"post","summary":"Sometimes we have to deal with manually entered data, which means there is a good chance that the data needs to be cleaned for consistency due to the inevitable errors that creep in when typing in data, not to speak of any inconsistencies between individuals entering data.","tags":["data-cleaning","data-step","regex","sas"],"title":"Cleaning up a Date String with RegEx in SAS","type":"post"},{"authors":[],"categories":[],"content":"I find myself needing to import CSV files with a relatively large number of columns. In many cases, proc import works surprisingly well in giving me what I want. But sometimes, I need to do some work while reading in the file and it would be nice to just use a data step to do so, but I don\u0026rsquo;t want to type it in by hand. That\u0026rsquo;s when a combination of proc import and some regex substitution can come in handy.\nFor the first step, run a proc import, like this sample code that is provided by SAS Studio when you double click on a CSV file:\nFILENAME REFFILE '/path/to/file/data.csv'; PROC IMPORT DATAFILE=REFFILE DBMS=CSV OUT=WORK.IMPORT; GETNAMES=YES; RUN;  If you run this code, you will see that SAS generates a complete data step for you. This is what the beginning of one looks like:\n  Sample log output.   There will be be two lines for each variable, one giving the informat and one giving the format that SAS decided on. This will be followed by an input statement. You can copy that from the log into a text editor such as VSCode, but unfortunately the line numbering of the LOG will carry over. One convenient way of fixing this is to use regex search-and-replace. Each line starts with a space followed by 1-3 digits, followed by a variable number of spaces until the next word. To capture this I use ^\\s\\d{1,3}\\s+ as my search term and replace with nothing. This will left align the whole data step, but this can be adjusted later.\nAt this point the data step can be saved as a SAS file or copied back over to the file you are working within SAS Studio, but I like to do one more adjustment. I really like using the attrib statement, see documentation, because it allows me to see the informat, format, and label of a variable all in one place. So I use regex to re-arrange my informat statement into the beginnings of an attribute statement. Use the search term informat\\s([^\\s]+)\\s([^\\s]+)\\s+; to capture each informat line and create two capture groups - the variable name as group 1 and the informat as group 2. If you use the replace code $1 informat=$2 format=$2, you will see the beginnings of an attribute statement. In this replacement scheme, each informat matches each format. This is fine for date and character variables, but you may want to adjust the display format for some of your numeric variables.\nTo clean this up, get rid of the format lines (you can search for ^format.+\\n and replace with an empty replace to delete them), add the attrib statement below the infile and make sure to end the block of attributes with a semicolon, and indent your code as desired.\n  Sample data step view.   And there you have it! The beginning of a nicely formatted data step that you can start to work with.\n","date":1627562770,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627562770,"objectID":"6e3ce61f3fcb0347e61ce9a68b07fa32","permalink":"https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/","publishdate":"2021-07-29T08:46:10-04:00","relpermalink":"/post/21-07-proc-import-to-data-step-with-regex/","section":"post","summary":"I find myself needing to import CSV files with a relatively large number of columns. In many cases, proc import works surprisingly well in giving me what I want. But sometimes, I need to do some work while reading in the file and it would be nice to just use a data step to do so, but I don\u0026rsquo;t want to type it in by hand.","tags":["data-step","import","regex","sas"],"title":"From Proc Import to a Data Step with Regex","type":"post"},{"authors":[],"categories":[],"content":"One of the editors I use regularly is VS Code. I work a lot with Python, but when installing Anaconda using default settings on a Windows machine already having VSC installed there\u0026rsquo;s a good chance you\u0026rsquo;ll run into an issue. When attempting to run Python code straight from VSC you may get an error. This should be fixed on some newer versions of Anaconda, but I\u0026rsquo;ve needed to do something about it often enough I feel it\u0026rsquo;s useful to save the solution janh posted on StackExchange.\nSpecifically, the issue can be fixed by manually changing VSC\u0026rsquo;s default shell from PowerShell to CMD. Just open the command palette (CTRL+SHIFT+P), search \u0026ldquo;Terminal: Select Default Profile\u0026rdquo; and switch to \u0026ldquo;Command Prompt\u0026rdquo;. Everything should work as expected from now on!\n","date":1626871792,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626871792,"objectID":"f664ecba8b426abe9eaf090413bde694","permalink":"https://dmsenter89.github.io/post/21-07-vsc-python-fix/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/21-07-vsc-python-fix/","section":"post","summary":"One of the editors I use regularly is VS Code. I work a lot with Python, but when installing Anaconda using default settings on a Windows machine already having VSC installed there\u0026rsquo;s a good chance you\u0026rsquo;ll run into an issue.","tags":["python","vscode","windows"],"title":"Making VS Code and Python Play Nice on Windows","type":"post"},{"authors":[],"categories":[],"content":"I am currently working with a database provided by the North Carolina Department of Public Safety that consists of several fixed-width files. Each of these has an associated codebook that gives the internal variable name, a label of the variable, its data type, as well as the start column and the length of the fields for each column. To import the data sets into SAS, I could copy and paste part of that data into my INPUT and LABEL statements, but that gets tedious pretty fast when dealing with dozens of lines. And since I have multiple data sets like that, I didn\u0026rsquo;t really want to do it that way. In this post I show how a simple command-line script can be written to deal with this problem.\nIntroducing AWK Here are the first few lines of one of these files:\nCMDORNUM OFFENDER NC DOC ID NUMBER CHAR 1 7 CMCLBRTH OFFENDER BIRTH DATE DATE 8 10 CMCLSEX OFFENDER GENDER CODE CHAR 18 30 CMCLRACE OFFENDER RACE CODE CHAR 48 30 CMCLHITE OFFENDER HEIGHT (IN INCHES) CHAR 78 2 CMWEIGHT OFFENDER WEIGHT (IN LBS) CHAR 80 3  We can see that the data is tabular and separated by multiple spaces. Linux programs often deal with column data and a tool is available for manipulating column-based data on the command-line: AWK, a program that can be used for complex text manipulation from the command-line. Some useful tutorials on AWK in general are available at grymoire.com and at tutorialspoint.\nFor our purposes, we want to know about the print and printf commands for AWK. To illustrate how this works, make a simple list of three lines with each term separated by a space:\ncat \u0026lt;\u0026lt; EOF \u0026gt; list.txt 1 one apple pie 2 two orange cake 3 three banana shake EOF  To print the whole file, you\u0026rsquo;d use the print statement: awk '{print}' list.txt. But I could do that with cat, so what\u0026rsquo;s the point? Well, what if I only want one of the columns? By default, $n refers to the nth column in AWK. So to print only the fruits I could write awk '{print $3}' list.txt.\nMultiple columns can be printed by listing multiple columns separated by a comma: awk '{print $2,$3}' list.txt. Note that if you omit the comma the two columns get concatenated into a single column.\nIf additional formatting is required, we can use the printf command. So to create a hyphenated fruit and food-item column, we could use awk '{printf \u0026quot;%s-%s\\n\u0026quot;, $3, $4}' list.txt. Note that we have to indicate the end-of line or else everything will be printed into a single line of text.\nNow we almost have all of the skills to create the label and input statements in SAS! Let\u0026rsquo;s create a comma-delimited list for practice:\ncat \u0026lt;\u0026lt; EOF \u0026gt; list.txt 1,one,apple pie 2,two,orange cake 3,three,banana shake EOF  The -F flag is used to tell AWK to use a different column separator. So to print the third column, we\u0026rsquo;d use awk -F ',' '{print $3}' list.txt.\nMaking the SAS statements Now we know everything we need to know about AWK to create code we want. First we note that our coding file uses multiple spaces as column separators as opposed to single spaces. If each item was a single word, this wouldn\u0026rsquo;t be a problem. Unfortunately, our second column reads \u0026ldquo;OFFENDER NC DOC ID NUMBER\u0026rdquo; which would be split into five columns by default. So we will need to use the column separator flag as -F '[[:space:]][[:space:]]+'.\nThe LABEL Statement A SAS label has the general form LABEL variable-1=label-1\u0026lt;...variable-n=label-n\u0026gt;;, so for example\nlabel score1=\u0026quot;Grade on April 1 Test\u0026quot; score2=\u0026quot;Grade on May 1 Test\u0026quot;;  is a valid label statement. In our file the variable names are given in column 1 and the appropriate labels in column 2. So an AWK script to print the appropriate labels can be written like this:\nawk -F '[[:space:]][[:space:]]+' '{printf \u0026quot;\\t%s=\\\u0026quot;%s\\\u0026quot;\\n\u0026quot;, $1, $2}' FILE.DAT  This is what everything looks like given our code:\nThe INPUT STATEMENT The INPUT statement can be made in a similar way, it just requires some minor tweaking as INPUT can be a bit more complex to handle a variety of data, see the documentation. In our case we are dealing with a fixed-width record. The fourth column gives the starting column of the data and the fifth gives us the width of that field. The third gives us the data type. The majority of ours are character, so it seems easiest to just have the AWK script print each line as though it were a character together with a SAS comment giving the name and \u0026ldquo;official\u0026rdquo; data type. Then the few lines that need adjustment can be manually adjusted. The corresponding code would look like this:\nawk -F '[[:space:]][[:space:]]+' '{printf \u0026quot;\\t@%s %s $%s. /*%s - %s*/\\n\u0026quot;,$4, $1, $5, $3, $2}' FILE.DAT  This is what is returned by our code (highlighted part has been manually edited):\nI hope you all find this useful and that it will save you some typing!\n","date":1625582307,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625582307,"objectID":"13eae08f3341c48169beb8080f35417a","permalink":"https://dmsenter89.github.io/post/21-07-awk-for-sas/","publishdate":"2021-07-06T10:38:27-04:00","relpermalink":"/post/21-07-awk-for-sas/","section":"post","summary":"I am currently working with a database provided by the North Carolina Department of Public Safety that consists of several fixed-width files. Each of these has an associated codebook that gives the internal variable name, a label of the variable, its data type, as well as the start column and the length of the fields for each column.","tags":["awk","codebook","ncdps","sas"],"title":"Making INPUT and LABEL Statements with AWK","type":"post"},{"authors":[],"categories":[],"content":"I have been using both SAS and Python extensively for a while now. With each having great features, it was very useful to combine my skills in both languages by seamlessly moving between SAS and Python in a single notebook. In the video below, fellow SAS intern Ariel Chien and I show how easy it is to connect the SAS and Python kernels using the open-source SASPy package together with SAS OnDemand for Academics. I hope you will also find that this adds to your workflow!\n  The Jupyter notebook from the video can be viewed on GitHub. For installation instructions, check out the SASPy GitHub page. Configuration for SASPy to connect to ODA can be found at this support page. For more information on SAS OnDemand for Academics, click here.\n","date":1624978625,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624978625,"objectID":"7f3c04f09bfebb28b854e08a6091095c","permalink":"https://dmsenter89.github.io/post/21-06-youtube-tutorial/","publishdate":"2021-06-29T10:57:05-04:00","relpermalink":"/post/21-06-youtube-tutorial/","section":"post","summary":"I have been using both SAS and Python extensively for a while now. With each having great features, it was very useful to combine my skills in both languages by seamlessly moving between SAS and Python in a single notebook.","tags":["python","oda","sas","saspy"],"title":"SASPy Video Tutorial","type":"post"},{"authors":[],"categories":[],"content":"The Census Bureau has updated its population estimates for 2020 with county level data. This means any projects that have had to rely on the 2019 estimates can now switch to the 2020 estimates.\nThis is particularly useful for those of us who have been trying to track the development of COVID-19. The average incidence rates are typically rescaled to new cases per 100,000 people. Previous graphs and maps I have created used the 2019 estimates. I have now updated my code for mapping North Carolina developments to use the 2020 estimates.\n  County level data for North Carolina using the NYT COVID data set. Date set to June 8, 2021.   Below this post is my code for loading the necessary data using SAS. Note that I\u0026rsquo;m using a macro called mystate that can be set to the statecode abbreviation of your choice. The conditional County ne 0 is in the code because the county level CSV includes both the county data as well as the totals for each state.\nfilename popdat url 'https://www2.census.gov/programs-surveys/popest/datasets/2010-2020/counties/totals/co-est2020-alldata.csv'; data censusdata; infile POPDAT delimiter=',' MISSOVER DSD lrecl=32767 firstobs=2; informat SUMLEV REGION DIVISION State County best32. STNAME $20. CTYNAME $35. CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020 best32.; format SUMLEV REGION DIVISION STATE best32. COUNTY 5. STNAME $20. CTYNAME $35. CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020 COMMA12. StateCode $2.; input SUMLEV REGION DIVISION STATE COUNTY STNAME $ CTYNAME $ CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020; if (State ne 0) and (State ne 72) then do; FIPS=put(State, Z2.); Statecode=fipstate(FIPS); if Statecode eq \u0026amp;mystate and County ne 0 then output; end; keep STNAME CTYNAME County FIPS Statecode Popestimate2020; run;  The media release can be viewed here. The county-level data set can be downloaded at this page.\n","date":1623268834,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623268834,"objectID":"5354baad22577889f3cfc0a85b71f1ab","permalink":"https://dmsenter89.github.io/post/21-06-covid-county-incidence/","publishdate":"2021-06-09T16:00:34-04:00","relpermalink":"/post/21-06-covid-county-incidence/","section":"post","summary":"The Census Bureau has updated its population estimates for 2020 with county level data. This means any projects that have had to rely on the 2019 estimates can now switch to the 2020 estimates.","tags":["census","covid","sas"],"title":"Census 2020 Population Estimates Updated","type":"post"},{"authors":["D. Michael Senter"],"categories":[],"content":"All organisms must deal with fluid transport and interaction, whether it be internal, such as lungs moving air for the extraction of oxygen, or external, such as the expansion and contraction of a jellyfish bell for locomotion. Most organisms are highly deformable and their elastic deformations can be used to move fluid, move through fluid, and resist fluid forces. A particularly effective numerical method for biological fluid-structure interaction simulations is the immersed boundary (IB) method. An important feature of this method is that the fluid is discretized separately from the boundary interface, meaning that the two meshes do not need to conform with each other. This thesis covers the development of a new software tool for the semi-automated creation of finite difference meshes of complex 2D geometries for use with immersed boundary solvers IB2d and IBAMR, alongside two examples of locomotion - the flight of tiny insects and the metachronal paddling of brine shrimp.\nAs mentioned, an advantage of the IB method is that complex geometries, e.g., internal or external morphology, can easily be handled without the need to generate matching grids for both the fluid and the structure. Consequently, the difficulty of modeling the structure lies often in discretizing the boundary of the complex geometry (morphology). Both commercial and open source mesh generators for finite element methods have long been established; however, the traditional immersed boundary method is based on a finite difference discretization of the structure. In chapter 2, I present a software library called MeshmerizeMe for obtaining finite difference discretizations of boundaries for direct use in the 2D immersed boundary method. This library provides tools for extracting such boundaries as discrete mesh points from digital images. Several examples of how the method can be applied are given to demonstrate the effectiveness of the software, including passing flow through the veins of insect wings, within lymphatic capillaries, and around starfish using open-source immersed boundary software.\nAs an example of insect flight, I present a 3D model of clap and fling. Of the smallest insects filmed in flight, most if not all clap their wings together at the end of the upstroke and fling them apart at the beginning of the downstroke. This motion increases the strength of the leading edge vortices generated during the downstroke and augments the lift. At the Reynolds numbers ($Re$) relevant to flight in these insects (roughly $4\u0026lt;Re\u0026lt;40$), the drag produced during the fling is substantial, although this can be reduced through the presence of wing bristles, chordwise wing flexibility, and more complex wingbeat kinematics. It is not clear how flexibility in the spanwise direction of the wings can alter the lift and drag generated. In chapter 3, a hybrid version of the immersed boundary method with finite elements is used to simulate a 3D idealized clap and fling motion across a range of wing flexibilities. I find that spanwise flexibility, in addition to three-dimensional spanwise flow, can reduce the drag forces produced during the fling while maintaining lift, especially at lower $Re$. While the drag required to fling 2D wings apart may be more than an order of magnitude higher than the force required to translate the wings, this effect is significantly reduced in 3D. Similar to previous studies, dimensionless drag increases dramatically for Re\u0026lt;20, and only moderate increases in lift are observed. Both lift and drag decrease with increasing wing flexibility, but below some threshold, lift decreases much faster. This study highlights the importance of flexibility in both the chordwise and spanwise directions for low Re insect flight. The results also suggest that there is a large aerodynamic cost if insect wings are too flexible.\nMy second application of locomotion pertains to a 2D model of swimming, specifically the method known as metachronal paddling. This method is used by a variety of organisms to propel themselves through a fluid. This mode of swimming is characterized by an array of appendages that beat out of phase, such as the swimmerets used by long-tailed crustaceans like crayfish and lobster. This form of locomotion is typically observed over a range of Reynolds numbers greater than 1 where the flow is dominated by inertia. The majority of experimental, modeling, and numerical work on metachronal paddling has been conducted on the higher Reynolds number regime (order 100). In this chapter, a simplified numerical model of one of the smaller metachronal swimmers, the brine shrimp, is constructed. Brine shrimp are particularly interesting since they swim at Reynolds numbers on the order of 10 and sprout additional paddling appendages as they grow. The immersed boundary method is used to numerically solve the fluid-structure interaction problem of multiple rigid paddles undergoing cycles of power and return strokes with a constant phase difference and spacing that are based on brine shrimp parameters. Using a phase difference of 8%, the volumetric flux and efficiency per paddle as a function of the Reynolds number and the spacing between legs is quantified. I find that the time to reach periodic steady state for adult brine shrimp is large (approx. 150 stroke cycles) and decreases with decreasing Reynolds number. Both efficiency and average flux increase with Reynolds number. In terms of leg spacing, the average flux decreases with increased spacing while the efficiency is maximized for intermediate leg spacing.\n","date":1621092475,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621092475,"objectID":"9018ea50e7ae87868c895467ba763a1c","permalink":"https://dmsenter89.github.io/publication/phd-dissertation/","publishdate":"2022-08-11T10:45:00-04:00","relpermalink":"/publication/phd-dissertation/","section":"publication","summary":"All organisms must deal with fluid transport and interaction, whether it be internal, such as lungs moving air for the extraction of oxygen, or external, such as the expansion and contraction of a jellyfish bell for locomotion.","tags":[],"title":"Immersed Boundary Simulations and Tools for Studying Insect Flight and Other Applications","type":"publication"},{"authors":[],"categories":null,"content":"","date":1617807600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617807600,"objectID":"57d6d3d7994b3e5729ae4e32fb33b3d7","permalink":"https://dmsenter89.github.io/talk/dissertation-defense/","publishdate":"2021-03-07T16:24:53-05:00","relpermalink":"/talk/dissertation-defense/","section":"talk","summary":"All organisms must deal with fluid transport and interaction, whether it be internal, such as lungs moving air for the extraction of oxygen, or external, such as the expansion and contraction of a jellyfish bell for locomotion. Most organisms are highly deformable and their elastic deformations can be used to move fluid, move through fluid, and resist fluid forces. A particularly effective numerical method for biological fluid-structure interaction simulations is is the immersed boundary (IB) method. An important feature of this method is that the fluid is discretized separately from the boundary interface, meaning that the two meshes do not need to conform with each other. This thesis covers a software tool for the semi-automated creation of finite difference meshes of complex 2D geometries for use with 2D immersed boundary solvers IB2d and IBAMR, alongside two examples of locomotion - flight and swimming.","tags":[],"title":"Dissertation Defense","type":"talk"},{"authors":[],"categories":[],"content":"Metachronal paddling can be described as the sequential oscillation of appendages whereby adjacent paddles maintain a nearly constant phase difference. This mechanism is widely used in nature, both in locomotion such as swimming in crustaceans and in fluid transport such as the clearance of mucus in the mammalian lung. Aside from the wide range of applications, metachronal paddling can be observed across a wide range of Reynolds number regimes.\nI work on simulating the hydrodynamics of metachronal paddling in brine shrimp (Artemia). Brine shrimp are small aquatic crustaceans who lay dormat eggs and are widely used in aquaculture. Their thoracopods are spaced closely together and beat with a small phase difference. We are interested in the hydrodynamics and efficiency of this swimming pattern, which has not previously been rigorously explored.\n","date":1616432400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616432400,"objectID":"27e854c7eb907e3d28b622f58694fc8e","permalink":"https://dmsenter89.github.io/project/metachronal-paddling/","publishdate":"2021-03-22T13:00:00-04:00","relpermalink":"/project/metachronal-paddling/","section":"project","summary":"Metachronal paddling is a widely used mechanism for fluid transport and locomotion. We study the hydrodynamics of metachronal paddling in brine shrimp (*Artemia*)","tags":[],"title":"Metachronal Paddling","type":"project"},{"authors":[],"categories":[],"content":"Git is a widely used version control system that allows users to track their software development in both public and private repositories. It is also increasingly used to store data in text formats, see for example the New York Times COVID-19 data set. This post will briefly demonstrate how to clone and pull updates from a GitHub repository using the git functions that are built into SAS Studio.\nGit functionality has been built into SAS Studio for a little while, so there are actually two slightly different iterations of the git functions. The examples in this post will use the versions compatible with SAS Studio 3.8, which is the current version available at SAS OnDemand for Academics. All git functions use the same prefix. In older versions such as SAS Studio 3.8 the prefix is gitfn_, which is followed by a git command such as \u0026ldquo;clone\u0026rdquo; or \u0026ldquo;pull\u0026rdquo;. In SAS Studio 5, the prefix has been simplified to just git_. Most git functions have the same name between the\ntwo versions, so that the only difference is the prefix. A complete table of the old and new versions of the git functions is available in the documentation.\nWe use the git functions by calling them in an otherwise empty DATA step. In other words, we use the format\ndata _null_; /* use your git functions here */ run;  Cloning a Repo To clone a repo from github we use gitfn_clone. It takes two arguments - the URL of the repository of interest and the path to an empty folder. You can have SAS create the folder for you by using OPTIONS DLCREATEDIR. The basic syntax for the clone is as follows:\ndata _null_; rc = gitfn_clone ( \u0026quot;\u0026amp;repoURL.\u0026quot;, /* URL to repo */ \u0026quot;\u0026amp;targetDIR.\u0026quot;); /* folder to put repo in */ put rc=; /* equals 0 if successful */ run;  It doesn\u0026rsquo;t matter if the URL you use ends in \u0026ldquo;.git\u0026rdquo; or not. In other words, the following two macros would both work the same:\n%LET repoURL=https://github.com/nytimes/covid-19-data; /* works the same as */ %LET repoURL=https://github.com/nytimes/covid-19-data.git;  You can also use password based authentication to pull in private repositories:\ndata _null_; rc = gitfn_clone ( \u0026quot;\u0026amp;repoURL.\u0026quot;, \u0026quot;\u0026amp;targetDIR.\u0026quot;, \u0026quot;\u0026amp;githubUSER.\u0026quot;, /* your GitHub username */ \u0026quot;\u0026amp;githubPASSW.\u0026quot;); /* your GitHub password */ put rc=; /* equals 0 if successful */ run;  NOTE: GitHub is deprecating password-based authentication; you will need to switch to OAuth authentication or SSH keys if you are not already using them. To access a repository using an SSH key, use the following:\ndata _null_; rc = gitfn_clone( \u0026quot;\u0026amp;repoURL.\u0026quot;, \u0026quot;\u0026amp;targetDIR.\u0026quot;, \u0026quot;\u0026amp;sshUSER.\u0026quot;, \u0026quot;\u0026amp;sshPASSW.\u0026quot;, \u0026quot;\u0026amp;sshPUBkey.\u0026quot;, \u0026quot;\u0026amp;sshPRIVkey.\u0026quot;); put rc=; run;  Pull-ing in Updates It is just as easy to pull in updates to a local repository by using gitfn_pull(\u0026quot;\u0026amp;repoDIR.\u0026quot;). This also works with SSH keys for private repositories:\ndata _null_; rc = gitfn_pull( \u0026quot;\u0026amp;repoDIR.\u0026quot;, \u0026quot;\u0026amp;sshUSER.\u0026quot;, \u0026quot;\u0026amp;sshPASSW.\u0026quot;, \u0026quot;\u0026amp;sshPUBkey.\u0026quot;, \u0026quot;\u0026amp;sshPRIVkey.\u0026quot;); run;  Other Functions SAS also offers other built-in functions, such as _diff, _status, _push, _commit, and others. For a complete list, see the SAS documentation here.\n","date":1610394170,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610394170,"objectID":"0901eabe06af43b69e3b768ba24c96ab","permalink":"https://dmsenter89.github.io/post/21-01-git-with-sas-studio/","publishdate":"2021-01-11T14:42:50-05:00","relpermalink":"/post/21-01-git-with-sas-studio/","section":"post","summary":"Git is a widely used version control system that allows users to track their software development in both public and private repositories. It is also increasingly used to store data in text formats, see for example the New York Times COVID-19 data set.","tags":["git","sas"],"title":"Using Git with SAS Studio","type":"post"},{"authors":[],"categories":[],"content":"A popular beginners machine learning problem is the prediction of housing prices. A frequently used data set for this purpose uses housing prices in California along some additional gathered through the 1990 Census. One such data set is available here at Kaggle. Unfortunately, that data set is rather old. And I live in North Carolina, not California! So I figured I might as well create a new housing data set, but this time with more up-to-date information and using North Carolina as the state to be analyzed. One thing that may be interesting about North Carlina as compared to California is the position of major populations centers. In California, major population centers are near the beach, while major population centers in North Carolina are in the interior of the state. Both large citites and proximity to the beach tend to correlate with higher housing prices. In California, unlike in North Carolina, both of these go together.\nThis post will describe the Kaggle data set with California housing prices and then walk you through how the relevant data can be acquired from the Census Bureau. I\u0026rsquo;ll also show how to clean the data. For those who just want to explore the complete data set, I have made it available for download here.\nTable of Contents  The Source Data Set Acquiring the Census Data Set  Census Variables Geography Considerations   Acquiring Location Data Data Merge with GEOID Matching Data Cleaning   The Source Data Set The geographic unit of the Kaggle data set is the Census block group, which means we will have several thousand data points for our analysis. For a good big-picture overview of Census geography divisions, see this post from the University of Pittsburgh library. The data set\u0026rsquo;s ten columns contain geographic, housing, and Census information that can be broken down as follows:\n geographic information  longitude latitude ocean proximity   housing information  median age of homes median value of homes total number of rooms in area total number of bedrooms in the area   Census information  population number of households median income    Most of these exist directly in the Census API data that we have covered previously. The ocean proximity variable is a categorical giving approximate distance from the beach. My data set will not include this last categorical variable.\nAcquiring the Census Data Set Census Variables The first, and most time consuming aspect, is to figure out where the data we want is located. We know that the US has a decennial census, so accurate information is available every ten years at every level of geography that the Census Bureau tracks. Since it is currently a census year 2020 and the newest information hasn\u0026rsquo;t been tabulated yet, that means the last census count that is available is from 2010. While this is 20 years more current than the California set from 1990, it still seems a bit outdated. Luckily, since the introduction of the American Community Survey (ACS) we have annually updated information available - but not for every level of geography. Only the 5-year ACS average gives us census block-level information for the whole state, making it comparable to the Kaggle data set. The most recent of these is the 2018.\nI start by creating a data dictionary from the groups and variables pages of the \u0026ldquo;American Community Survey: 1-Year Estimates: Detailed Tables 5-Year\u0026rdquo; data set. Note that median home age is not directly available. Instead, we will use the median year structures were built to calculate the median home age. Our data dictionary also does not include any data for the longitude and latitude of each row. We will get that data separately.\ndata_dictionary = { 'B01001_001E' : \u0026quot;population\u0026quot;, 'B11001_001E' : \u0026quot;households\u0026quot;, 'B19013_001E' : \u0026quot;median_income\u0026quot;, 'B25077_001E' : \u0026quot;median_house_value\u0026quot;, 'B25035_001E' : \u0026quot;median_year_structure_built\u0026quot;, 'B25041_001E' : \u0026quot;total_bedrooms\u0026quot;, 'B25017_001E' : \u0026quot;total_rooms\u0026quot;, }  Geography Considerations The next step is figuring out exactly what level of geography we want. Our data set goes down to the Census block level at its most granular. Unfortunately, the Census API won\u0026rsquo;t let us pull the data for all the Census blocks in a state at once. Census tracts on the other hand can be acquired in one go. If we were to shortcut and use only tract data, this would be a pretty quick API call build:\nprimary_geo = \u0026quot;tract:*\u0026quot; secondary_geo = \u0026quot;state:37\u0026quot; query = base_URL + \u0026quot;?get=\u0026quot; + \u0026quot;,\u0026quot;.join(data_dictionary.keys()) + f\u0026quot;\u0026amp;for={primary_geo}\u0026amp;in={secondary_geo}\u0026quot;  But let\u0026rsquo;s try and do it for the Census blocks instead. This will require us to build a sequence of API calls that loops over a larger geographic area, say the different counties in the state, and pull in the respective census block data for that geographic unit. While the FIPS codes for the state counties are sorted alphabetically, they are not contiguous. A full listing of North Carolina county FIPS codes is availalbe from NCSU here. It appears to be that the county FIPS codes are three digits long, starting at 001 and go up to 199 in increments of 2, meaning only odd numbers are in the county set. So it looks like we will be using range(1,200,2) with zero-padding to create the list of county FIPS codes. So we could use a loop similar to this:\nvars_requested = \u0026quot;,\u0026quot;.join(data_dictionary.keys()) for i in range(1,200,2): geo_request = f\u0026quot;for=block%20group:*\u0026amp;in=state:37%20county:{i:03}\u0026quot; query = base_URL + f\u0026quot;?get={vars_requested}\u0026amp;{geo_request}\u0026quot;  While practicing to write the appropriate API call, you may find it useful to give it frequent, quick tests using curl. If you are using Jupyter or IPython, you can use !curl \u0026quot;{query}\u0026quot; to test your API query. Don\u0026rsquo;t forget the quotation marks, since the ampersand has special meaning in the shell. It may be helpful to test the output of your call at the county or city level with that reported on the Census Quickfacts page, if your variable is listed there. This can help make sure you are pulling the data you actually want.\nNow that we have figured out the loop necessary for creation of the API calls, we can put everything together and create a list of Pandas DataFrames which we then concatenate to create our master list.\nimport pandas as pd import requests # create the base-URL host_name = \u0026quot;https://api.census.gov/data\u0026quot; year = \u0026quot;2018\u0026quot; dataset_name = \u0026quot;acs/acs5\u0026quot; base_URL = f\u0026quot;{host_name}/{year}/{dataset_name}\u0026quot; # build the api calls as a list query_vars = base_URL + \u0026quot;?get=\u0026quot; + \u0026quot;,\u0026quot;.join(list(data_dictionary.keys()) + [\u0026quot;NAME\u0026quot;,\u0026quot;GEO_ID\u0026quot;]) api_calls = [query_vars + f\u0026quot;\u0026amp;for=block%20group:*\u0026amp;in=state:37%20county:{i:03}\u0026quot; for i in range(1,200,2) ] # running the API calls will take a moment rjson_list = [requests.get(call).json() for call in api_calls] # create the data frame by concatenation df_list = [pd.DataFrame(data[1:], columns=data[0]) for data in rjson_list] df = pd.concat(df_list, ignore_index=True) # save the raw output to disk df.to_csv(\u0026quot;raw_census.csv\u0026quot;, index=False)  And now we have the data set! We do still have to address the issue of our values all being imported as strings as mentioned in my Census API post.\nAcquiring Location Data As mentioned above, we are still missing information regarding the latitude and longitude of the different block groups. The Census Bureau makes a lot of geographically coded data available on its TIGERweb page. You can interact with it both using a REST API and its web-interface. A page with map information exists here.\nDealing with shapefiles and the TIGERweb API can get a little complicated. Luckily, I know someone with expertise in GIS and shapefiles so we will be using a CSV file of the geographic data we need courtesy of Summer Faircloth, a GIS intern at the North Carolina Department of Transportation. She downloaded the TIGER/Line Shapefiles for the 20189 ACS Block Groups and Census Tracts and joined the data sets in ArcMap, from where she exported our CSV file, which is now available here.\nWe don\u0026rsquo;t need all of the columns in the CSV file, so we will limit the import to the parts we need with the usecols keyword.\ndf = pd.read_csv(\u0026quot;raw_census.csv\u0026quot;, dtype={})  shapedata = pd.read_csv(\u0026quot;BlockGroup_Tract2018.csv\u0026quot;, dtype={\u0026quot;GEOID\u0026quot;: str}, usecols=['GEOID','NAMELSAD','INTPTLAT','INTPTLON','NAMELSAD_1'] ) shapedata = shapedata.rename(columns={'INTPTLAT' : 'latitude', 'INTPTLON' : 'longitude' })  Data Merge with GEOID Matching At this stage we have two data frames - the first consists of all the Census information sans the geographic coordinates of the block groups, and a second data set containing the block groups' location. Both data sets contain a GEOID column that can be used for merging. The GEOID returned by the Census API includes additional information to the regular FIPS code based GEOID used in the TIGERweb system. For example, \u0026ldquo;1500000US370010204005\u0026rdquo; in the census data set is actually GEOID \u0026ldquo;370010204005\u0026rdquo; for purposes of the TIGERweb data set. We\u0026rsquo;ll use a string split to make our GEO_ID variable from the Census API compatible with the FIPS code based GEOID from the TIGERweb service.\ndf[\u0026quot;GEO_ID\u0026quot;] = df[\u0026quot;GEO_ID\u0026quot;].str.split('US').str[1] df = df.merge(shapedata, left_on='GEO_ID', right_on=\u0026quot;GEOID\u0026quot;)  Data Cleaning Now that our data set has been assembled, we can work on cleaning up the merged data set. We have the following tasks left:\n convert column data types to numeric drop unnecessary columns rename columns handle missing values calculate median age of homes  for col in data_dictionary.keys(): if col not in [\u0026quot;NAME\u0026quot;, \u0026quot;GEO_ID\u0026quot;]: df[col] = pd.to_numeric(df[col])  To indicate missing values, the Census API returns a value of \u0026ldquo;-666666666\u0026rdquo; in numeric columns. As all of our variables - except for longitude - ought to be positive, we can use the mask function to convert all negative values to missing. We\u0026rsquo;ll start by filtering out the string columns that are no longer necessary.\n# filter down to our numerical columns keeps = list(data_dictionary.keys()) +[\u0026quot;latitude\u0026quot;, \u0026quot;longitude\u0026quot;] df = df.filter(items=keeps) # replace vals \u0026lt; 0 with missing k = df.loc[:, df.columns != 'longitude'] k = k.mask(k \u0026lt; 0) df.loc[:, df.columns != 'longitude'] = k  Now that the missing values have been handled, we can go ahead and calculate our median home age.\ndf.rename(columns=data_dictionary, inplace=True) df[\u0026quot;housing_median_age\u0026quot;] = 2018 - df[\u0026quot;median_year_structure_built\u0026quot;] df.drop(columns=\u0026quot;median_year_structure_built\u0026quot;, inplace=True)  And now we\u0026rsquo;re done! We will save our output data set to disk for future analysis in a different post.\ndf.to_csv(\u0026quot;NC_Housing_Prices_2018.csv\u0026quot;, index=False)  ","date":1604675401,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606749001,"objectID":"f7e0af2bd9aee1557c21c03cb9e7ee1f","permalink":"https://dmsenter89.github.io/post/20-11-north-carolina-housing/","publishdate":"2020-11-06T10:10:01-05:00","relpermalink":"/post/20-11-north-carolina-housing/","section":"post","summary":"A popular beginners machine learning problem is the prediction of housing prices. A frequently used data set for this purpose uses housing prices in California along some additional gathered through the 1990 Census.","tags":["acs","api","census","data-cleaning","finding-data","fips","geoid","kaggle","merging","python","pandas"],"title":"North Carolina Housing Data","type":"post"},{"authors":[],"categories":[],"content":"After reading a news article about teacher pay in the US, I was curious and wanted to look into the source data myself. Unfortunately, the source that was mentioned was a publication by the National Education Association (NEA) which had the data as tables embedded inside a PDF report. As those who know me can attest, I don\u0026rsquo;t like hand-copying data. It is slow and error-prone. Instead, I decided to use the tabula package to extract the information from the PDFs directly into a Pandas dataframe. In this post, I will show you how to extract the data and how to clean it up for analysis.\nTable of Contents  The Data Source Loading the Data  Cleaning the Data Numeric Conversion   Table B-6   The Data Source Several years worth of data are available in PDF form on the NEA website. Reading through the technical notes, they highlight that they did not collect all of their own salary information. Some states' information is calculated from the American Community Survey (ACS) done by the Census Bureau - a great resource whose API I have covered in a different post. Each report includes accurate data for the previous school year, as well as estimates for the current school year. As of this post, the newest report is the 2020 report which includes data for the the 2018-2019 school year, as well as estimates of the 2019-2020 school year.\nThe 2020 report has the desired teacher salary information in two separate locations. One is in table B-6 on page 26 of the PDF, which shows a ranking of the different states' average salary in addition to the average salary:\nA second location is in table E-7 on page 46, which gives salary data for the completed school year as well as different states' estimates for the 2019-2020 school year:\nNote that table E-7 lacks the star-annotation marking NEA estimated values. This, and the lack of the ranking column, makes Table E-7 easier to parse. In the main example below, this will be the source of the five years of data. I will however also show how to parse table B-6 at the end of this post for completion.\nLoading the Data As of October 2020, the NEA site has five years worth of reports online. Unfortunately, these are not labeled consistently for all five years. Similarly the page numbers differ for each report. Prior to the 2018 report, inconsistent formats were used for the tables which require previous years to be parsed separately from the newer tables. For this reason, I\u0026rsquo;ll make a dictionary for the 2018-2020 reports only, which will simplify the example below.\nreport = { '2020' : { 'url' : \u0026quot;https://www.nea.org/sites/default/files/2020-10/2020%20Rankings%20and%20Estimates%20Report.pdf\u0026quot;, 'page' : 46, }, '2019' : { 'url' : \u0026quot;https://www.nea.org/sites/default/files/2020-06/2019%20Rankings%20and%20Estimates%20Report.pdf\u0026quot;, 'page' : 49, }, '2018' : { 'url' : \u0026quot;https://www.nea.org/sites/default/files/2020-07/180413-Rankings_And_Estimates_Report_2018.pdf\u0026quot;, 'page' : 51, }, }  We can now use dictionary comprehension to fill in a dictionary with all the source tables of interest. We will be using the tabula package to extract data from the PDFs. If you don\u0026rsquo;t have it installed, you can use pip install tabula-py to get a copy. The method that reads in a PDF is aptly called read_pdf. Its first argument is a file path to the PDF. Since we want to use a URL, we will use the keyword argument stream=True and then name the specific page in each PDF that contains the information we are after. By default, read_pdf returns a list of dataframes, so we just save the first element from the list, which is the report we are interested in.\nNote: if you are using WSL, depending on your settings, you may get the error Exception in thread \u0026quot;main\u0026quot; java.awt.AWTError: Can't connect to X11 window server using 'XXX.XXX.XXX.XXX:0' as the value of the DISPLAY variable. error when running read_pdf. This is fixed by having an X11 server running.\nimport tabula import pandas as pd source_df = {year : tabula.read_pdf(report[year]['url'], stream=True, pages=report[year]['page'])[0] for year in report.keys()}  And that\u0026rsquo;s it in principle. How cool is that! Of course, we still need to clean our data a little bit.\nCleaning the Data Let\u0026rsquo;s take a look at the first and last few entries of the 2020 report:\npd.concat([source_df['2020'].head(), source_df['2020'].tail()])   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Unnamed: 0 2018-19 2019-20 From 2018-19 to 2019-20 From 2010-11 to 2019-20 (%)     0 State Salary($) Salary($) Change(%) Current Dollar Constant Dollar   1 Alabama 52,009 54,095 4.01 13.16 -2.58   2 Alaska 70,277 70,877 0.85 15.36 -0.69   3 Arizona 50,353 50,381 0.06 8.03 -7.00   4 Arkansas 49,438 49,822 0.78 8.31 -6.75   48 Washington 73,049 72,965 -0.11 37.86 18.69   49 West Virginia 47,681 50,238 5.36 13.51 -2.28   50 Wisconsin 58,277 59,176 1.54 9.17 -6.02   51 Wyoming 58,861 59,014 0.26 5.19 -9.44   52 United States 62,304 63,645 2.15 14.14 -1.73     We see that each column is treated as a string object (which you can confirm by running source_df['2020'].dtypes) and that the first row of data is actually at index 1 due to the fact that the PDF report used a two-row header. This means we can safely drop the first row of every dataframe. We can also drop the last row of every dataframe since that just contains summary data of the US as a whole, which we can easily regenerate as necessary. So row indices 0 and 52 can go for all of our data sets.\nfor df in source_df.values(): df.drop([0, 52], inplace=True)  Next up I\u0026rsquo;d like to fix the column names. The fist column is clearly the name of the state (except in the case of Washington D.C.), while the next two columns give the years for which the salary information is given. Let\u0026rsquo;s rename the second and third columns according to the pattern Salary %YYYY-YY using Python\u0026rsquo;s f-string syntax.\nfor df in source_df.values(): df.rename(columns={ df.columns[0] : \u0026quot;State\u0026quot;, df.columns[1] : f\u0026quot;Salary {str(df.columns[1])}\u0026quot;, df.columns[2] : f\u0026quot;Salary {str(df.columns[2])}\u0026quot;, }, inplace=True) source_df[\u0026quot;2020\u0026quot;].head() # show the result of our edits so far   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  State Salary 2018-19 Salary 2019-20 From 2018-19 to 2019-20 From 2010-11 to 2019-20 (%)     1 Alabama 52,009 54,095 4.01 13.16 -2.58   2 Alaska 70,277 70,877 0.85 15.36 -0.69   3 Arizona 50,353 50,381 0.06 8.03 -7.00   4 Arkansas 49,438 49,822 0.78 8.31 -6.75   5 California 83,059 84,659 1.93 24.74 7.39     Looks like we\u0026rsquo;re almost done! Let\u0026rsquo;s drop the unnecessary columns and check our remaining column names:\nfor year, df in source_df.items(): df.drop(df.columns[3:], axis=1, inplace=True) print(f\u0026quot;{year}:\\t{df.columns}\u0026quot;)  2020:\tIndex(['State', 'Salary 2018-19', 'Salary 2019-20'], dtype='object') 2019:\tIndex(['State', 'Salary 2017-18', 'Salary 2018-19'], dtype='object') 2018:\tIndex(['State', 'Salary 2017', 'Salary 2018'], dtype='object')  We can see that the column naming scheme in 2018 was different than in the previous reports. To make them all compatible for our merge, we\u0026rsquo;re going to have to do some more editing. Based on the other reports, it appears as though the 2018 report used the calendar year of the end of the school year, while the others utilized a range. This can easily be solved using regex substitution. We\u0026rsquo;ll do that now.\nimport re for year, df in source_df.items(): if year != \u0026quot;2018\u0026quot;: df.rename(columns={ df.columns[1] : re.sub(r\u0026quot;\\d{2}-\u0026quot;, '', df.columns[1]), df.columns[2] : re.sub(r\u0026quot;\\d{2}-\u0026quot;, '', df.columns[2]), }, inplace=True) # print the output for verification print(f\u0026quot;{year}:\\t{df.columns}\u0026quot;)  2020:\tIndex(['State', 'Salary 2019', 'Salary 2020'], dtype='object') 2019:\tIndex(['State', 'Salary 2018', 'Salary 2019'], dtype='object') 2018:\tIndex(['State', 'Salary 2017', 'Salary 2018'], dtype='object')  Now that everything works, we can do our merge to create a single dataframe with the information for all of the school years we have downloaded.\nmerge_df = source_df[\u0026quot;2018\u0026quot;].drop([\u0026quot;Salary 2018\u0026quot;], axis=1).merge( source_df[\u0026quot;2019\u0026quot;].drop([\u0026quot;Salary 2019\u0026quot;], axis=1)).merge( source_df[\u0026quot;2020\u0026quot;]) merge_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  State Salary 2017 Salary 2018 Salary 2019 Salary 2020     0 Alabama 50,391 50,568 52,009 54,095   1 Alaska 6 8,138 69,682 70,277 70,877   2 Arizona 4 7,403 48,723 50,353 50,381   3 Arkansas 4 8,304 50,544 49,438 49,822   4 California 7 9,128 80,680 83,059 84,659     Numeric Conversion We\u0026rsquo;re almost done! Notice that we still have not dealt with the fact that every column is still treated as a string. Before we can use the to_numeric function, we still need to take care of two issues:\n The commas in the numbers. While they are nice for our human eyes, Pandas doesn\u0026rsquo;t like them. In the 2017 salary column, there appears to be extraneous white space after the first digit for some entries.  Luckily, both of these problems can be remedied with a simple string replacement operation.\nmerge_df.iloc[:,1:] = merge_df.iloc[:,1:].replace(r\u0026quot;[,| ]\u0026quot;, '', regex=True) for col in merge_df.columns[1:]: merge_df[col] = pd.to_numeric(merge_df[col])  Now we\u0026rsquo;re done! We have created an overview of annual teacher salaries from the 2016-17 school year until 2019-20 extracted from a series of PDFs published by the NEA. We have cleaned up the data and converted everything to numerical values. We can now get summary statistics and do any analysis of interest with this data.\nmerge_df.describe() # summary stats of our numeric columns   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Salary 2017 Salary 2018 Salary 2019 Salary 2020     count 51.000000 51.000000 51.000000 51.000000   mean 56536.196078 57313.039216 58983.254902 60170.647059   std 9569.444674 9795.914601 10286.843230 10410.259274   min 42925.000000 44926.000000 45105.000000 45192.000000   25% 49985.000000 50451.500000 51100.500000 52441.000000   50% 54308.000000 53815.000000 54935.000000 57091.000000   75% 61038.000000 61853.000000 64393.500000 66366.000000   max 81902.000000 84227.000000 85889.000000 87543.000000     Table B-6 As mentioned above, table B-6 in the 2020 Report presents slightly greater challenges. A lot of the cleaning is similar or identical, so I will not reproduce it in full. Instead, I have loaded a subsetted part of table B-6 and will show how this can be cleaned up as well. But first, let\u0026rsquo;s look at the first several entries:\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Unnamed: 0 2017-18 (Revised) 2018-19     0 State Salary($) Rank Salary($)   1 Alabama 50,568 36 52,009   2 Alaska 69,682 7 70,277   3 Arizona 48,315 45 50,353   4 Arkansas 49,096 44 49,438   5 California 80,680 2 83,059 *   6 Colorado 52,695 32 54,935   7 Connecticut 74,517 * 5 76,465 *   8 Delaware 62,422 13 63,662     We can see that there is an additional hurdle compared to the previous tables: the second column now contains data from two columns, both the Salary information as well as a ranking of the salary as it compares to the different states. For a few states, there is additionally a \u0026lsquo;*\u0026rsquo; to denote values that were estimated as opposed to received. We can again use a simple regex replace together with a capture group to parse out only those values that we are interested in, while dropping the extraneous information using the code below.\nb6.iloc[:,1:] = b6.iloc[:,1:].replace(r\u0026quot;([\\d,]+).*\u0026quot;, r\u0026quot;\\1\u0026quot;, regex=True)  And now we\u0026rsquo;re back to where we were above before we did the string conversion. This is what it looks like after also dropping the first row and renaming the columns:\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  State Salary 2018 Salary 2019     1 Alabama 50,568 52,009   2 Alaska 69,682 70,277   3 Arizona 48,315 50,353   4 Arkansas 49,096 49,438   5 California 80,680 83,059   6 Colorado 52,695 54,935   7 Connecticut 74,517 76,465   8 Delaware 62,422 63,662     From here on out, we can proceed as in the previous example.\n","date":1604025540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604419200,"objectID":"c5eca41aa0719561a38b4163c276f4ab","permalink":"https://dmsenter89.github.io/post/20-10-tabula/","publishdate":"2020-10-29T22:39:00-04:00","relpermalink":"/post/20-10-tabula/","section":"post","summary":"What do you do when your data table is in PDF format? Let's use tabula-py to extract teacher salary information from PDFs directly into Pandas dataframes. We'll also use some regex to clean up the results.","tags":["finding-data","formatting","pandas","pdf","python","tabula","teachers"],"title":"Teacher Salaries","type":"post"},{"authors":[],"categories":[],"content":"The Census Bureau makes an incredible amount of data available online. In this post, I will summarize how to get access to this data via Python by using the Census Bureau\u0026rsquo;s API. The Census Bureau makes a pretty useful guide available here - I recommend checking it out.\nTable of Contents  API Basics Building an the Base URL Building the Query  The \u0026lsquo;Get\u0026rsquo; Variables Location Variables The Complete Call   Making the API Request Reading the JSON into Pandas   API Basics We can think of an API query of consisting of two main parts: a base URL (also called a root URL) and a query string. These two strings are joined together with the query character \u0026ldquo;?\u0026rdquo; to create an API call. The resulting API call can in theory be copy-and-pasted into the URL bar of your browser, and I recommend this when first playing around with a new API. Seeing the raw text returned in the browser can help you understand the structure of what is being returned. In the case of the Census Bureau\u0026rsquo;s API, it returns a string that essentially looks like a list of lists from a Python perspective. This can easily be turned into a Pandas dataset. Be aware that all values are returned as strings. You\u0026rsquo;ll have to convert number columns to numeric by yourself.\nTo get an overview of all available data sets, you can go to the data page which contains a long list of data sets. This data page is incredibly useful because it gives access to all of the information needed to build a correct API call, including the base URLs of all data sets and the variables available in each.\n  A snapshot of two datasets available as part of the 2018 American Community Survey (ACS).   Building an the Base URL Let\u0026rsquo;s build a sample API call for the 2018 American Community Survey 1-Year Detailed Table. While we could just copy the base URL from the data page, I like to assemble mine manually from its component parts. This makes it easier to write a wrapper for the API calls if you plan on scraping the same data from multiple years.\nhost_name = \u0026quot;https://api.census.gov/data\u0026quot; year = \u0026quot;2018\u0026quot; dataset_name = \u0026quot;acs/acs1\u0026quot; base_URL = f\u0026quot;{host_name}/{year}/{dataset_name}\u0026quot;  Building the Query Now that we have the base URL, we can work on building the query. For purposes of the Census Bureau, you will need two components: the variables of interest, which are listed after the get= keyword, and the geography for which you would like the data listed after the for= keyword. For certain subdivisions, like counties, you can specify two levels of geography by adding an in= keyword at at the end.\nThe \u0026lsquo;Get\u0026rsquo; Variables Since many of the data sets have a large amount of variables in them, it often makes sense to take a look at the \u0026ldquo;groups\u0026rdquo; page first. This page lists variables as groups, giving you a better overview of what data is available. This page is available at {base_URL}/groups.html. A complete list of all variables in the data set is available at {base_URL}/variables.html.\nLet\u0026rsquo;s find some variables. The most basic variable we\u0026rsquo;d expect to find here is total population. We can find this variable in group \u0026ldquo;B01003\u0026rdquo;. The total estimate is in sub-variable \u0026ldquo;001E\u0026rdquo;, meaning that the variable for total population is \u0026ldquo;B01003_001E\u0026rdquo;. Let\u0026rsquo;s also get household income (group \u0026ldquo;B19001\u0026rdquo;) not broken down by race: \u0026ldquo;B19001_001E\u0026rdquo;. There is also median monthly housing cost (group B25105) with variable \u0026ldquo;B25105_001E\u0026rdquo;. Since the variable names can be a little difficult to parse, I recommend making a data dictionary as you prepare the list of variables to fetch.\ndata_dictionary = { \u0026quot;B01003_001E\u0026quot; : \u0026quot;Total Population\u0026quot;, \u0026quot;B19001_001E\u0026quot; : \u0026quot;Household Income (12 Month)\u0026quot;, \u0026quot;B25105_001E\u0026quot; : \u0026quot;Median Monthly Housing Cost\u0026quot;, }  This way, the list of variables can easily be created from the data dictionary:\nget_vars = ','.join(data_dictionary.keys())  Location Variables Which geographic variables are available for a particular data set can be found {base_URL}/geography.html. The Census Bureau uses FIPS codes to reference the different geographies. To find the relevant codes, see here. Delaware for example has FIPS code 10 while North Carolina is 37. So to get information for these two states, we\u0026rsquo;d use for=state:10,37. You can also use \u0026lsquo;*\u0026rsquo; as a wildcard. So to get all the states' info you\u0026rsquo;d write for=state:*.\nSubdivisions for similarly. To get information for Orange County (FIPS 135) in North Carolina (FIPS 37), you could write for=county:135 with the keyword in=state:37. Let\u0026rsquo;s get the information for Orange and Alamance counties in North Carolina.\ncounty_dict = { \u0026quot;001\u0026quot; : \u0026quot;Alamance County\u0026quot;, \u0026quot;135\u0026quot; : \u0026quot;Orange County\u0026quot;, } county_fips = ','.join(county_dict.keys()) state_dict = {\u0026quot;37\u0026quot; : \u0026quot;North Carolina\u0026quot;} state_fips = ','.join(state_dict.keys()) query_str = f\u0026quot;get={get_vars}\u0026amp;for=county:{county_fips}\u0026amp;in=state:{state_fips}\u0026quot;  The Complete Call The complete API call can now be easily assembled from the previous two pieces:\napi_call = base_URL + \u0026quot;?\u0026quot; + query_str  If we copy-and-paste this output into our browser, we can see the result looks as follows:\n  The result of our sample API query.   Making the API Request We can make the API request with Python\u0026rsquo;s requests package:\nimport requests r = requests.get(api_call)  And that\u0026rsquo;s it! We now have the response we wanted. To interpret the response as JSON, we would call the json method of the response object: r.json(). The result can then be fed into Pandas to generate our data set.\nReading the JSON into Pandas We can use Pandas' DataFrame method directly on our data, making sure to specify that the first row consists of column headers.\nimport pandas as pd data = r.json() df = pd.DataFrame(data[1:], columns=data[0])  We can then do any renaming based on the dictionaries we have created previously.\ndf.rename(columns=data_dictionary, inplace=True) df['county'] = df['county'].replace(county_dict) df['state'] = df['state'].replace(state_dict)  The last step is to make sure our numeric columns are interpreted as such. Since all of the requested variables are in fact numeric, we can use the dictionary of variables to convert what we need to numeric variables.\nfor col in data_dictionary.values(): df[col] = pd.to_numeric(df[col])  And that\u0026rsquo;s it! We\u0026rsquo;re now ready to work with our data.\n","date":1598100835,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598100835,"objectID":"b15cce3c2e5806b0971c4615b9a1a94f","permalink":"https://dmsenter89.github.io/post/20-08-census-api/","publishdate":"2020-08-22T08:53:55-04:00","relpermalink":"/post/20-08-census-api/","section":"post","summary":"The Census Bureau makes an incredible amount of data available online. In this post, I will summarize how to get access to this data via Python by using the Census Bureau\u0026rsquo;s API.","tags":["acs","api","census","finding-data","json","pandas","python","requests"],"title":"Accessing Census Data via API","type":"post"},{"authors":["D. Michael Senter","Dylan Ray Douglas","W. Christopher Strickland","Steven G. Thomas","Anne M Talkington","Laura Miller"],"categories":[],"content":"","date":1596468475,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596468475,"objectID":"f53c5f8137d2c17ac968dc30b8fb6a58","permalink":"https://dmsenter89.github.io/publication/senter-2020-meshmerizeme/","publishdate":"2020-08-05T11:27:55-04:00","relpermalink":"/publication/senter-2020-meshmerizeme/","section":"publication","summary":"Numerous fluid-structure interaction problems in biology have been investigated using the immersed boundary method. The advantage of this method is that complex geometries, e.g., internal or external morphology, can easily be handled without the need to generate matching grids for both the fluid and the structure. Consequently, the difficulty of modeling the structure lies often in discretizing the boundary of the complex geometry (morphology). Both commercial and open source mesh generators for finite element methods have long been established; however, the traditional immersed boundary method is based on a finite difference discretization of the structure. Here we present a software library for obtaining finite difference discretizations of boundaries for direct use in the 2D immersed boundary method. This library provides tools for extracting such boundaries as discrete mesh points from digital images. We give several examples of how the method can be applied that include passing flow through the veins of insect wings, within lymphatic capillaries, and around starfish using open-source immersed boundary software.","tags":[],"title":"A semi-automated finite difference mesh creation method for use with immersed boundary software IB2d and IBAMR","type":"publication"},{"authors":[],"categories":null,"content":"This workshop covers data acquisition and basic data preparation with a focus on using Python with Jupyter Notebooks. To avoid having to install Python locally during the workshop, we will be utilizing an Azure notebook project. The example files are located here.\nPlease note that the free Azure notebooks will only be available until early October. To continue using Python and Jupyter notebooks, you may want to consider using a local installation. For Windows and Mac users, I recommend using Anaconda. For continued cloud usage, you may consider Cocalc. Please note that you will need a subscription for your Cocalc notebooks to be able to download data from external sources.\nAdditional Links:\n  Engauge Digitizer (software to extract data points from graphs).  Markdown Cheatsheet.  ","date":1596128400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596128400,"objectID":"80d634f592a1f22b7ec5837df0dace9d","permalink":"https://dmsenter89.github.io/talk/webscraping-tutorial/","publishdate":"2020-07-27T14:10:17-04:00","relpermalink":"/talk/webscraping-tutorial/","section":"talk","summary":"Data acquisition is a key step in research. In this workshop, we will consider how to effectively access publicly available data sets. We will discuss how to find and load data published in CSV/Excel formats.  We will learn how to use Pandas to parse HTML tables. We will discuss some best practises for data acquisition and storage.","tags":[],"title":"Basics of Web Scraping with Python","type":"talk"},{"authors":[],"categories":[],"content":"Basics of Web Scraping with Python Michael Senter\n Goals for Today  Understand what tools and methods are available.  Be able to create a new project using Python and Jupyter.  Be able to edit existing code snippets to gather data.    Python  easy to learn, reads like \u0026ldquo;pseudocode\u0026rdquo; widely used in a variety of fields many books, websites, etc. to help you learn  print(\u0026quot;Hello, world!\u0026quot;)   Data Sources  CSV/Excel Downloads  COVID Related Data  Johns Hopkins Dashboard The Johns Hopkins data is published on GitHub and is updated regularly.\n Using SAS filename outfile \u0026quot;~/import-data-nyt.sas\u0026quot;; /* download official SAS script to above filename */ proc http url=\u0026quot;https://raw.githubusercontent.com/sassoftware/covid-19-sas/master/Data/import-data-nyt.sas\u0026quot; method=\u0026quot;get\u0026quot; out=outfile; run; /* run the downloaded script */ %include \u0026quot;~/import-data-nyt.sas\u0026quot;; /* state and county level data are now in memory */  ","date":1596126600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596126600,"objectID":"4e6dec377db9b7f6c1777b9311af259c","permalink":"https://dmsenter89.github.io/slides/webscraping-tutorial/","publishdate":"2020-07-30T12:30:00-04:00","relpermalink":"/slides/webscraping-tutorial/","section":"slides","summary":"Basics of Web Scraping with Python Michael Senter\n Goals for Today  Understand what tools and methods are available.  Be able to create a new project using Python and Jupyter.","tags":[],"title":"Webscraping Tutorial","type":"slides"},{"authors":[],"categories":[],"content":"My website is back up and running! Some incompatabilities between my old site and updates to both Hugo and the Academic Theme have led to some downtime on this page as I didn\u0026rsquo;t have time to look through how to rebuild my site without loosing previous content. I\u0026rsquo;m currently in the process of updating everything and will try to bring back some material as well. Stay tuned!\nThis page is currently using the Academic theme from Hugo. Docs and other templates are available at wowchemy.\n","date":1595863867,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625580900,"objectID":"92d904527521646e80dcda8e55bf841f","permalink":"https://dmsenter89.github.io/post/porting-forward/","publishdate":"2020-07-27T11:31:07-04:00","relpermalink":"/post/porting-forward/","section":"post","summary":"My website is back up and running! Some incompatabilities between my old site and updates to both Hugo and the Academic Theme have led to some downtime on this page as I didn\u0026rsquo;t have time to look through how to rebuild my site without loosing previous content.","tags":[],"title":"Porting Forward","type":"post"},{"authors":[],"categories":null,"content":"Please join me as I present the work I have done so far in my graduate career and discuss avenues for future study.\n","date":1587128400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587128400,"objectID":"d4925dbe1b1b4dd4dad7517dcbe7fbcf","permalink":"https://dmsenter89.github.io/talk/thesis-proposal/","publishdate":"2020-04-15T08:00:00-04:00","relpermalink":"/talk/thesis-proposal/","section":"talk","summary":"Please join me as I present the work I have done so far in my graduate career and discuss avenues for future study.","tags":[],"title":"Thesis Proposal","type":"talk"},{"authors":[],"categories":[],"content":"Insects are ubiquitious throughout the world. Most of us are familiar with winged insects such as butterflies and bees. Insect flight is an interesting topic from a biomechanics perspective. Unlike birds, most insects (with some eceptions, such as dragonflies and others) do not have flight muscles attached to their wings. Instead, their flight muscles oscillate their thorax, which in turn makes the wings move. Furthermore, they beat their wings at a very high speed. The aerodynamics of insect flight are also very interesting. Larger insects are able to fly by creating a leading edge vortex. This method does not work in the smallest insect fliers. Such insects include the thrips and chalcid wasps, some of which have wingspans as small as 1 mm. These insects have unusual wing structures, as can be seen in this image:\nThe solid part of the wing is rather small and narrow, with many large bristles projecting from the solid part of the wing. Insects such as thrips do not create a leading edge vortex; instead, they fly using the \u0026ldquo;clap and fling\u0026rdquo; method. This method is common amongst insects who fly in the intermediate Reynolds number regime, $1\\leq \\mathrm{Re} \\leq 100$.\n","date":1527866603,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527866603,"objectID":"047eb4aa355fddd02ce6989411f5488b","permalink":"https://dmsenter89.github.io/project/clap-and-fling/","publishdate":"2018-06-01T11:23:23-04:00","relpermalink":"/project/clap-and-fling/","section":"project","summary":"Simulating the aerodynamics of the smallest insect fliers.","tags":[],"title":"Clap and Fling","type":"project"},{"authors":[],"categories":[],"content":" IB2d and IBAMR are two software packages implementing the immersed boundary method (see below). These packages model fluid-structure interaction problems based on user given parameters and geometry. The manual creation of the initial geometry mesh can be difficult and time consuming, especially for the complex shapes encountered in biological applications. Oftentimes we have images of the geometry we wish to explore. I am developing software to help automate the creation of such CFD meshes for 2D simulations with a file-format suitable for use with IB2d and IBAMR from images. An initial prototype version is available on Github. A paper exploring the use of MeshmerizeMe in conjuction with IB2d for simulations is in preparation.\nUsage MeshmerizeMe needs two input files per experimental geometry: an SVG image file with the geometry of interest and an input2d file with the experiment parameters. When selecting an SVG for use with MeshmerizeMe it will automatically look for the input2d file in the same folder. It will then parse the paths, transform them into the correct coordinate system and appropriately sample the paths based on the size of the Cartesian grid set in the input2d file. The geometry will be exported as a vertex file. This file is readable by both IB2d and IBAMR.\nSVGs were chosen as the image source as the are an open, text-based format making them very accesible to work with. They are standardized for web use and many tools exist for creating and manipulating SVG images. They can be created from source images such as photographs or scans by means of edge detection tools and by manually tracing the outline of a shape of interest Consider optimizing the SVG prior to processing to save time.\nAs the current version of MeshmerizeMe only handles a subset of SVG, tools that optimize the SVG files created by your editor are very useful. Examples of such software include SVGO, which also offers a webapp called SVGOMG. Another software is svgcleaner.\nIBM Background One aspect of computational fluid dynamics is the investigation of fluid-structure interactions. One method developed for the study of such interactions is the immersed boundary method (IBM) developed by Peskin1. It is well known that fluids can be studied from both a Eulerian and a Lagrangian view. The IBM combines these - the domain of the problem is resolved as a Cartesian grid on which Eulerian equations are solved for fluid velocity and pressure. In the case of Newtonian fluids the incompressible Navier-Stokes equations comprising of\n$$ \\rho \\left( \\frac{\\partial \\mathbf{u}}{\\partial t} + \\mathbf{u} \\cdot \\nabla \\mathbf{u} \\right) = - \\nabla \\mathbf{p} + \\mu \\nabla^2 \\mathbf{u} + \\mathbf{f}$$\nand\n$$\\nabla \\cdot \\mathbf{u} = 0$$\nneed to be solved.\nThe immersed structures are modeled as fibers in the form of parametric curves $X(s,t)$, where $s$ is a parameter and $t$ is time. The fiber experiences force distributions $F(s,t)$, and we can derive the force the fiber exerts on the fluid from the momentum equation. For the fibers we then solve\n$$\\mathbf{f} = \\int_\\Gamma \\mathbf{F}(s,t),\\delta\\left(\\mathbf{x}-\\mathbf{X}(s,t)\\right),ds$$\nand\n$$\\frac{\\partial \\mathbf{X}}{\\partial t} = \\int_\\Omega \\mathbf{u}(\\mathbf{x},t), \\delta \\left( \\mathbf{x}-\\mathbf{X}(s,t)\\right),d\\mathbf{x}.$$\nHere, $\\Gamma$ is the immersed structure and $\\Omega$ is the fluid domain.\nThe immersed structures are discretized not on a Cartesian grid but on a separate Lagrangian grid on the fiber itself. Of import to CFD software users is that the initial discretization of the immersed structure has to be supplied by the user. While this is not too difficult for simple geometries, the often complex structures encountered in mathematical biology can present a significant time investment. This is the part where MeshmerizeMe comes in handy.\n  Charles S Peskin. 2002. \u0026ldquo;The immersed boundary method.\u0026rdquo; Acta numerica 11:479-517. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1505921706,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505921706,"objectID":"75d971656dea94655081afc1899e8ab1","permalink":"https://dmsenter89.github.io/project/meshmerizeme/","publishdate":"2017-09-20T11:35:06-04:00","relpermalink":"/project/meshmerizeme/","section":"project","summary":"Automatic mesh generation from 2D images for use with immersed boundary solvers.","tags":[],"title":"MeshmerizeMe","type":"project"},{"authors":["C Hohenegger","R Durr","DM Senter"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"424a48dfbcfb4647e011e675044fd04a","permalink":"https://dmsenter89.github.io/publication/hohenegger-2017-mean/","publishdate":"2020-07-27T15:40:59.097696Z","relpermalink":"/publication/hohenegger-2017-mean/","section":"publication","summary":"The motion of a passive spherical particle in a fluid has been widely described via a balance of force equations known as a Generalized Langevin Equation (GLE) where the covariance of the thermal force is related to the time memory function of the fluid. For viscous fluids, this relationship is simply a delta function in time, while for a viscoelastic fluid it depends on the constitutive equation of the fluid memory function. In this paper, we consider a general setting for linear viscoelasticity which includes both solvent and polymeric contributions, and a family of memory functions known as the generalized Rouse kernel. We present a statistically exact algorithm to generate paths which allows for arbitrary large time steps and which relies on the numerical evaluation of the covariance of the velocity process. As a consequence of the viscoelastic properties of the fluid, the particle exhibits subdiffusive behavior, which we verify as a function of the free parameters in the generalized Rouse kernel. We then numerically compute the mean first passage time of a passive particle through layers of different widths and establish that, for the generalized Rouse kernel, the mean first passage time grows quadratically with the layerâ€™s width independently of the free parameters. Along the way, we also find the linear scaling of the mean first passage time for a layer of fixed width as a function of the particleâ€™s radius.","tags":null,"title":"Mean first passage time in a thermally fluctuating viscoelastic fluid","type":"publication"}]