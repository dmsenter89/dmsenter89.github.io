<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Michael&#39;s Site</title>
    <link>https://dmsenter89.github.io/</link>
      <atom:link href="https://dmsenter89.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Michael&#39;s Site</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 14 Nov 2022 22:34:25 -0500</lastBuildDate>
    <image>
      <url>https://dmsenter89.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Michael&#39;s Site</title>
      <link>https://dmsenter89.github.io/</link>
    </image>
    
    <item>
      <title>Setup an Arbitrary WSL2 Distro</title>
      <link>https://dmsenter89.github.io/post/22-11-setup-an-arbitrary-wsl2-distro/</link>
      <pubDate>Mon, 14 Nov 2022 22:34:25 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/22-11-setup-an-arbitrary-wsl2-distro/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Windows Subsystem for Linux&lt;/a&gt; (WSL) is an important part of my daily work flow. Unfortunately, the main distro supplied by Windows
is Ubuntu, which - for a variety of reasons - is not exactly my favorite distro. Luckily, WSL2 allows you to import an arbitrary Linux distro
to use instead. I got the idea from 
&lt;a href=&#34;https://dev.to/bowmanjd/install-fedora-on-windows-subsystem-for-linux-wsl-4b26&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an article (Dev.to)&lt;/a&gt;
by Jonathan Bowman explaining how to get Fedora up and running in WSL2. This article summarizes the
key points of Bowman&amp;rsquo;s post and includes information for my long time daily driver, 
&lt;a href=&#34;https://archlinux.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arch Linux&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The short of it is that you can import a root filesystem tarball into WSL2 from Windows terminal
using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;wsl --import &amp;lt;distro-name&amp;gt; &amp;lt;distro-target-location&amp;gt; &amp;lt;path-to-tarball&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once imported, you can launch into your distro using &lt;code&gt;wsl -d &amp;lt;distro-name&amp;gt;&lt;/code&gt;. The only question is
how to get the root filesystem for this import step.&lt;/p&gt;
&lt;p&gt;There are two options we can go with: using a pre-fabricated root filesystem (rootfs), or creating our own
using Docker.&lt;/p&gt;
&lt;h2 id=&#34;using-an-existing-root-filesystem&#34;&gt;Using an Existing Root Filesystem&lt;/h2&gt;
&lt;p&gt;Some distros publish these. For Arch Linux, you can find them on 
&lt;a href=&#34;https://gitlab.archlinux.org/archlinux/archlinux-docker/-/releases&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitLab&lt;/a&gt;.
Two main images are available: base and base-devel. The latter has the base-devel 
&lt;a href=&#34;https://archlinux.org/groups/x86_64/base-devel/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;package group&lt;/a&gt; pre-installed.&lt;/p&gt;
&lt;p&gt;For Fedora, you can head over to 
&lt;a href=&#34;https://github.com/fedora-cloud/docker-brew-fedora/tree/37/x86_64&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;
to get a copy of the rootfs. Note that for Fedora, the rootfs is merely part of the repo and not a separate release
page. You&amp;rsquo;ll be able to pick your base version of Fedora by switching branches in the repository.&lt;/p&gt;
&lt;p&gt;These rootfs images are usually compressed. Before you can use them with WSL2, the tarball needs to be extracted.
The Arch Linux rootfs can be extracted with zstd and the Fedora rootfs can be extracted using 7z.&lt;/p&gt;
&lt;h2 id=&#34;making-your-own-root-filesystem&#34;&gt;Making your Own Root Filesystem&lt;/h2&gt;
&lt;p&gt;Docker allows you to export a container to a root filesystem tarball:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker export -o &amp;lt;rootfs-name&amp;gt;.tar &amp;lt;container-or-image-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The neat thing here is that you can use either an image or a container name.&lt;/p&gt;
&lt;p&gt;Arch Linux images are available from 
&lt;a href=&#34;https://hub.docker.com/_/archlinux&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DockerHub&lt;/a&gt;. Available
tags include the above mentioned base and base-devel. Fedora is also available on 
&lt;a href=&#34;https://hub.docker.com/_/fedora&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DockerHub&lt;/a&gt;
and its tags include version numbers (e.g., 37 or 36).&lt;/p&gt;
&lt;h2 id=&#34;additional-setup&#34;&gt;Additional Setup&lt;/h2&gt;
&lt;p&gt;Once you have imported the distro you only have a barebones system available. Likely only the root user is
available, which is not ideal. You&amp;rsquo;ll want to install the packages you want to use and set up your own
user in addition to root. If you are building your own rootfs using Docker, you can build everything
interactively in your container by running &lt;code&gt;docker run -it &amp;lt;image-name&amp;gt;:&amp;lt;tag&amp;gt;&lt;/code&gt; to drop into a
shell and do all your setup there. Alternatively, you can create a Dockerfile with the basic setup
and build an image from that.&lt;/p&gt;
&lt;h3 id=&#34;arch-linux&#34;&gt;Arch Linux&lt;/h3&gt;
&lt;p&gt;Pacman won&amp;rsquo;t work out-of-the-box because it doesn&amp;rsquo;t ship with keys. You&amp;rsquo;ll need to run &lt;code&gt;pacman-keys --init&lt;/code&gt;
first. Install your favorite software using pacman, e.g.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pacman -Syu exa htop vim
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;User management and other common setup tasks are covered in the Arch Wiki&amp;rsquo;s 
&lt;a href=&#34;https://wiki.archlinux.org/title/General_recommendations#System_administration&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;General Recommendations&lt;/a&gt;.
Key tasks include 
&lt;a href=&#34;https://wiki.archlinux.org/title/Users_and_groups#User_management&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;adding a new user&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;useradd -m -G wheel $username
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;fedora&#34;&gt;Fedora&lt;/h3&gt;
&lt;p&gt;Make sure to run &lt;code&gt;dnf upgrade&lt;/code&gt; to get the latest version of your packages.
You may need to install either the &lt;code&gt;util-linux&lt;/code&gt; or &lt;code&gt;util-linux-core&lt;/code&gt; packages in order to get
the mount command working (used by WSL to mount the Windows filesystem). To be
able to add a non-root user with a password you&amp;rsquo;ll need to make sure that &lt;code&gt;passwd&lt;/code&gt; is installed.&lt;/p&gt;
&lt;p&gt;To add a non-root user in Fedora, use&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;useradd -G wheel $username 
passwd $username  # in interactive mode, you&#39;ll type in your password here
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;general-case&#34;&gt;General Case&lt;/h3&gt;
&lt;p&gt;In order to actually start the WSL instance as your non-root user, you&amp;rsquo;ll need
to edit &lt;code&gt;/etc/wsl.conf&lt;/code&gt; inside of your distro. If the user section doesn&amp;rsquo;t exist
yet, you can just run&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo -e &amp;quot;\n[user]\ndefault = ${username}\n&amp;quot; &amp;gt;&amp;gt; /etc/wsl.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those are the basics to get you up and running. Not everything will necessarily
work smoothly out-of-the box as you may be missing some packages that you&amp;rsquo;re not
aware of until you need them, but overall I&amp;rsquo;ve had a positive experience with this
setup.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SAS Markdown for Reproducibility</title>
      <link>https://dmsenter89.github.io/post/22-11-sas-markdown-for-reproducibility/</link>
      <pubDate>Fri, 11 Nov 2022 15:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/22-11-sas-markdown-for-reproducibility/</guid>
      <description>&lt;p&gt;One of the coolest packages for R is 
&lt;a href=&#34;https://yihui.org/knitr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;knitr&lt;/a&gt;. Essentially, it allows you to combine explanatory writing, such as a paper or blog post, directly with your analysis code in a Markdown document. When the target document
is compiled (&amp;lsquo;knitted&amp;rsquo;), the R code in the document is run and the results inserted into the final document. The target document could
be an HTML or a PDF file, for example. This is great for many reasons. You have a regular report you want to run, but the data updates?
Just re-knit and your entire report is updated. No more separate running of the code followed by copying the results into whatever
software you use to build the report itself. This makes it not just less cumbersome, but less error prone. It also improves reproducibility.
Somebody wants to see your work, perhaps because they are unsure of your results or they want to extend your work? You can share the
markdown file and the other party can see exactly what code was used to generate what part of your report or paper.&lt;/p&gt;
&lt;p&gt;While knitr is certainly not the first package that allows for this workflow, and also not the only one, I have found it to be the most consistent and easy to use.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Luckily, knitr supports 
&lt;a href=&#34;https://yihui.org/knitr/demo/engines/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a variety&lt;/a&gt; of 
&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/language-engines.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;languages&lt;/a&gt;, including 
&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/eng-sas.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAS&lt;/a&gt;. And you can even mix and match multiple languages in 
&lt;a href=&#34;https://github.com/yihui/knitr-examples/blob/master/106-polyglot.Rmd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;one document&lt;/a&gt;.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;You might think that this sounds similar to Jupyter notebooks. While that is true, and there is a 
&lt;a href=&#34;https://github.com/sassoftware/sas_kernel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter kernel for SAS&lt;/a&gt; as well, knitr has some advantages over Jupyter for report-generation. Without additional tools, you have the option to execute but not display the code that generates your results, making a cleaner report. You can also elect to only show part of the code, with manual setup code running behind the scenes without being printed to the report itself. Additionally, the entire document is executed linearly. That means that if you update a code chunk towards the beginning of your document, it affects the code chunks following it, while in Jupyter you easily get in the habit of executing the chunks independently which can lead to inconsistencies if you don&amp;rsquo;t pay attention to the cell numbers.&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;In this post, I&amp;rsquo;ll demonstrate the basics of setting up a reproducible report using the SAS engine in knitr.&lt;/p&gt;
&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;
&lt;p&gt;Perhaps the easiest way to get started for beginners is to use RStudio and Anaconda. With that you can create a sample R Markdown document (&lt;code&gt;File -&amp;gt; New File -&amp;gt; R Markdown&lt;/code&gt;). Press the &lt;code&gt;knit&lt;/code&gt; button. If any packages required by knitr are missing, RStudio will install them for you. This way you can be sure that all the R parts are set up correctly. Additionally, I recommend installing the 
&lt;a href=&#34;https://github.com/Hemken/SASmarkdown&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SASmarkdown&lt;/a&gt; package with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# from CRAN:
install.packages(&amp;quot;SASmarkdown&amp;quot;)
# from GitHub: 
devtools::install_github(&amp;quot;Hemken/SASmarkdown&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once install is complete, load the package (&lt;code&gt;library(SASmarkdown)&lt;/code&gt;) and check the output. If you see a message that SAS was found, you are good to go. If not, you will either need to add SAS to your PATH or simply provide the path to SAS as an option in your document (see below).&lt;/p&gt;
&lt;h2 id=&#34;a-basic-markdown-file&#34;&gt;A Basic Markdown File&lt;/h2&gt;
&lt;p&gt;The important thing is to load the SASMarkdown package in your document. I recommend making a setup chunk at the very top of your document and setting include to FALSE.
That way the setup chunk is executed, but not printed to your final document.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r setup, include=FALSE}
library(SASMarkdown)
# if SAS is not in your path, define it manually:
saspath &amp;lt;- &amp;quot;C:/Program Files/SASHome/SASFoundation/9.4/sas.exe&amp;quot;
knitr::opts_chunk$set(engine=&amp;quot;sashtml&amp;quot;, engine.path=saspath)
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With that, we&amp;rsquo;re ready to run a basic SAS chunk using just the SAS option. This produces the typewriter-style output that is familiar from Enterprise Guide for example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;```{sas example1}
proc print data=sashelp.class; run;
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to take advantage of the modern HTML output that is standard in SAS Studio, we use the &lt;code&gt;sashtml&lt;/code&gt; engine instead:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;```{sashtml example2}
/* if you want, you can set an ODS style for HTML output: */
ods html style=journal;
proc print data=sashelp.class; run;
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want graphical output, for example from SGPLOT, you&amp;rsquo;ll need to use the &lt;code&gt;sashtml&lt;/code&gt; engine. To get the default blue look from SAS Studio, use the HTMLBLUE style:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;```{sashtml example3}
ods html style=HTMLBLUE;
proc sgplot data=sashelp.cars;
  scatter x=EngineSize y=MPG_CITY;
run;
```
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;some-additional-comments&#34;&gt;Some Additional Comments&lt;/h2&gt;
&lt;p&gt;The first thing that is important to note is that each chunk is processed &lt;em&gt;separately&lt;/em&gt;. That means each chunk should be written so as to be capable of being executed independent of the others. It is possible to get around this using the &lt;code&gt;collectcode=TRUE&lt;/code&gt; chunk option. This chunk will then subsequently be executed prior to the code from a following chunk. So for example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;```{sashtml save1, collectcode=TRUE}
data sample;
  set sashelp.class;
run;
```
  And now use it again:
```{sashtml save2}
proc means data=sample; run;
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is particularly useful for libnames and setting the preferred ODS style, so you don&amp;rsquo;t have to keep doing it again in each cell.&lt;/p&gt;
&lt;p&gt;The other thing to note is that knitr for SAS works best with HTML output. It can use SAS styles and produce output looking like what
you would expect running in SAS Studio. If you want PDF output, you can get nicer output using 
&lt;a href=&#34;https://support.sas.com/rnd/base/ods/odsmarkup/latex.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX Tagsets for ODS&lt;/a&gt; and the 
&lt;a href=&#34;https://support.sas.com/rnd/app/papers/statrep.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StatRep System&lt;/a&gt;.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;knitr itself was based on Sweave, but uses Markdown instead of LaTeX code. Other languages have similar packages, for
example 
&lt;a href=&#34;https://mpastell.com/pweave/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pweave&lt;/a&gt; for Python or 
&lt;a href=&#34;https://docs.juliahub.com/Weave/9EzOc/0.9.4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weave&lt;/a&gt; for Julia. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The chunks from different languages do not have access to each other&amp;rsquo;s data. To move data between the different engines,
more setup work is needed. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;If you code in Julia, there is an interesting new reactive notebook called 
&lt;a href=&#34;https://github.com/fonsp/Pluto.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pluto&lt;/a&gt; that
promises to always keep your cells in sync, while being geared towards a Jupyter-style workflow. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Does it ever make sense to play the Lottery?</title>
      <link>https://dmsenter89.github.io/post/22-09-lottery/</link>
      <pubDate>Fri, 30 Sep 2022 15:40:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-09-lottery/</guid>
      <description>&lt;p&gt;In a first semester probability course, students encounter combinatorics and point estimates such as the mean and median of a data set. A common example is the low odds of winning the lottery. When discussing the topic of point estimates, students are exposed to the idea of a &amp;ldquo;fair bet&amp;rdquo; or &amp;ldquo;fair game&amp;rdquo; - one in which the expected value of the random variable associated with the game is equal to the cost of participation or zero, depending on if a fixed cost is included in the game or tracked separately. This year, the Mega Millions had a jackpot in excess of one billion dollars. This had me thinking - mathematically, this is likely a fair game. But I still would expect to loose out playing it. In this article, I want to explore this idea further using the Mega Millions lottery as a particular example.&lt;/p&gt;
&lt;p&gt;Mega Millions is played by a choosing five numbers from 1 to 70 (the white balls) and one number from 1 to 25 (the golden &amp;ldquo;Mega Ball&amp;rdquo;). Five white balls (W) and one golden ball (G) are drawn without replacement twice per week. Prizes are earned by matching the drawn numbers. Payouts generally follow a fixed schedule for everything but the jackpot, at least outside of California where the payouts for all prizes are pari-mutual instead. Below is a table of all possible events as given on the Mega Millions 
&lt;a href=&#34;https://www.megamillions.com/How-to-Play.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;, sorted by increasing odds.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Event&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Variable&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Value&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Odds&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;5 W + G&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_1$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Jackpot&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/302,575,350&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5 W&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_2$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$1,000,000&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/12,607,306&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4 W + G&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_3$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$10,000&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/931,001&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4 W&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_4$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$500&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/38,792&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3 W + G&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_5$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$200&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/14,547&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2 W + G&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_6$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$10&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/693&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3 W&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_7$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$10&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/606&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1 W + G&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_8$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$4&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/89&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;G&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_9$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1/37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No Match&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$x_{10}$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;24/1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;the-fair-bet-analysis&#34;&gt;The Fair Bet Analysis&lt;/h2&gt;
&lt;p&gt;A Fair Bet or Fair game is one in which the expected value of the random variable doesn&amp;rsquo;t favor either the player or the house. Given a cost of 2 USD per game,
we can say that Mega Millions is fair when $E[X]=2$, or more specifically when&lt;/p&gt;
&lt;p&gt;$$E[X] = \sum_{i=1}^{10} x_i P(X=x_i) = \frac{n}{302,575,351}  + \sum_{i=2}^{10} x_i P(X=x_i) = 2$$&lt;/p&gt;
&lt;p&gt;I use Maxima to solve for the jackpot representing a fair game and to print a few representative values of the expected value for some jackpot options.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-maxima&#34;&gt;/* Define Expectation as dependent on n */
E(n) := n/302575351 + 1000000/12607307 + 10000/931002 + 
		500/38793 + 200/14548 +  10/694 + 10/607 + 4/90 + 2/38;
/* solve for fair game */
float(solve(E(n)=2,n));

/* give expected return for different jackpot values */
jackpots : [5e7, 1e8, 2.5e8, 5e8, 7.5e8, 1e9, 2e9];
for i in jackpots do printf(true, &amp;quot;E(~:D) = $~$ ~%&amp;quot;, i, E(i))$
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this we learn that to have a fair jackpot, we require $n = 531,123,698.80$. Even with a fair pet, the expected value is very modest. For example, a 2 billion USD jackpot has $E[X]~=6.85$ - less than 5 USD above the ticket price.&lt;/p&gt;
&lt;h2 id=&#34;how-long-until-we-profit&#34;&gt;How long until we Profit?&lt;/h2&gt;
&lt;p&gt;Most people don&amp;rsquo;t play the lottery to win small amounts like 5 USD. They want to become millionaires. Given that our expected values are so low, let&amp;rsquo;s take a look at how long it will take us to become rich if we take the lottery game route.&lt;/p&gt;
&lt;h3 id=&#34;the-geometric-distribution-and-our-lottery&#34;&gt;The Geometric Distribution and Our Lottery&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s start by considering only the events that would result in a win of a million dollars or more. In other words, events $x_1$ and $x_2$. We have&lt;/p&gt;
&lt;p&gt;$$P(x_1 \vee x_2) = \frac{315,182,658}{3,814,660,340,689,757} \approx 8.262\times 10^{-8}. $$&lt;/p&gt;
&lt;p&gt;If we are only interested in this outcome, we can treat our outcome as a Bernoulli variable with $p=P(x_1 \vee x_2)$. Then the expected number of games we need to play to win a million dollars or more is distributed like a geometric with $E[G] = 1/p$. For our specific case:&lt;/p&gt;
&lt;p&gt;$$ E[G] = \frac 1 p = \frac{3,814,660,340,689,757}{315,182,658} \approx 12,103,014.$$&lt;/p&gt;
&lt;p&gt;Recall that two games are played per week. Converting this expected number of games to years, it would take approximately $115,977$ years for us to win. Even if one drawing were held each day, we would expect to take more than $33,000$ years to win.&lt;/p&gt;
&lt;p&gt;Since the CDF of the geometric distribution is well defined, we can use it to estimate the number of games required for a certain likelihood of having a win of at least a million dollars. To have roughly 50% odds of winning, we need to play about $8,400,000$ games of Mega Millions. Note that in this case you would still likely be in the hole since the $1,000,000$ USD jackpot is nearly 24 times more likely than the main jackpot and each game costs 2 USD to play.&lt;/p&gt;
&lt;h3 id=&#34;simulating-a-lifetime-of-playing&#34;&gt;Simulating a Lifetime of Playing&lt;/h3&gt;
&lt;p&gt;At this point you might agree that the lottery is not a good get-rich-quick scheme. That alone doesn&amp;rsquo;t mean that you are all but guaranteed to loose money over a lifetime of playing. So let&amp;rsquo;s run some simulations and see what the distribution of our net worth is after taking everything into account. To make things as fair as possible, we will assume a constant jackpot of 750 million USD.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say you spend 50 years playing the Mega Millions at 2 USD for one ticket at each of the two weekly drawings. That comes out to just about $2,609$ weeks or $5,218$ games for a total price of $10,436$ USD. I simulated $50,000$ individuals each playing $5,218$ games for a constant jackpot of $750,000,000$ USD - much higher than the 
&lt;a href=&#34;https://www.megamillions.com/jackpot-history&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;typical jackpot&lt;/a&gt; and advantageous to the players. This will cost them each $10,436$ USD in ticket costs over the 50 years they play. Yet, despite the simulated lottery being rigged in the players&#39; favor, 99% of my players win less than 600 USD total over this 50 year time period.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Statistic&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Value ($)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9,169.43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SD&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;20,985.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Min&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9,974.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9,912.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;50%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9,898.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;75%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9,890.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;99%&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-9,884.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Max&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1,990,204.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;From these results it is clear that for all but the luckiest few, even just saving the money under a mattress outperforms playing the lottery. You can explore a distribution plot of my simulation with Plotly 
&lt;a href=&#34;results.html&#34;&gt;here&lt;/a&gt;. Note that this page may take a moment to load due to the many data points. You will need to zoom in on the left-hand side to be able really make anything out.&lt;/p&gt;
&lt;h4 id=&#34;implementation-note&#34;&gt;Implementation Note&lt;/h4&gt;
&lt;p&gt;The number of simulations grows quickly given the $5,218$ games we are using. Doing $50,000$ simulations of that many games requires over 260 million random draws. Prototyping in Python often makes sense because of the many features available for analysis and plotting, but this seems like an example where a compiled language might outperform by a considerable amount. I decided to 
&lt;a href=&#34;https://github.com/dmsenter89/lottery-sims&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;try this out (GitHub)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All of the programs were written with an emphasis on simplicity over performance so as to avoid biasing the results. Since the different individuals play their games independently, I wrote both a single-threaded C++ version as well as one utilizing OpenMP&amp;rsquo;s parallel for loop. As alternative compiled languages I added implementations in Go and Rust.&lt;/p&gt;
&lt;p&gt;For scripting languages I included Python and Julia. In Julia the main loop can trivially be set to run concurrently by prepending &lt;code&gt;Threads.@threads&lt;/code&gt; to the for loop, so inlcuded that as an option as well. This instructs the Julia interpreter to run this loop with the available threads. By default this is one, but can be set higher using an environment variable or by starting Julia with the &lt;code&gt;-t&lt;/code&gt; flag and specifying the desired number of threads.&lt;/p&gt;
&lt;p&gt;I used 
&lt;a href=&#34;https://github.com/sharkdp/hyperfine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hyperfine (GitHub)&lt;/a&gt; to benchmark the performance of my programs in WSL; see output below for details.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Command&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Mean [s]&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Min [s]&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Max [s]&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Relative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;C++ (Single Thread)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.602 ± 0.087&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.514&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;4.828&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.88 ± 0.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;C++ (OpenMP)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.653 ± 0.170&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.506&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.092&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.08 ± 0.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Go&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9.362 ± 0.058&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9.258&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;9.417&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;3.83 ± 0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Julia&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;28.606 ± 0.372&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;28.198&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;29.520&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;11.69 ± 0.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Julia (4 Threads)&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;19.016 ± 0.274&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;18.673&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;19.511&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7.77 ± 0.23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Python&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;57.727 ± 0.530&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;59.783 ± 3.811&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;57.833&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;70.242&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rust&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.447 ± 0.062&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.391&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2.579&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I was surprised by Rust&amp;rsquo;s performance. I only looked up enough Rust to be able to implement this simple example, so I find it surprising that it can keep up with a multi-threaded C++ implementation.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Updates: This blogpost has been updated with new benchmark values. The original post did not include results in Rust.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Life Expectancy Data</title>
      <link>https://dmsenter89.github.io/post/22-09-life-expectancy/</link>
      <pubDate>Fri, 02 Sep 2022 13:11:30 +0000</pubDate>
      <guid>https://dmsenter89.github.io/post/22-09-life-expectancy/</guid>
      <description>&lt;p&gt;Most people can guess the current life expectancy for Americans at birth as being in the high 70s or around 80. In fact, given the 
&lt;a href=&#34;https://www.ssa.gov/oact/STATS/table4c6.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;current mortality table&lt;/a&gt; published by the Social Security Administration (SSA), males have a life expectancy of about 76 compared to a female life expectancy of about 81. Of course that is only an expected value. Guessing the &lt;em&gt;distribution&lt;/em&gt; of a person&amp;rsquo;s life expectancy is somewhat more difficult. In this post, we&amp;rsquo;ll take a look at some simulated lives to get a feel for the distribution of life expectancy and its implications for retirement planning.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s begin by looking at our mortality table. The rows indicate an individual&amp;rsquo;s current age. For both a male and a female, three values are then given: the probability of death in a given year, the &amp;ldquo;Number of Lives&amp;rdquo;, and the life expectancy for this individual. The probability of death in a given year is somewhat self-explanatory. The &amp;ldquo;Number of Lives&amp;rdquo; variable starts with 100,000 individuals and gives the number of survivors at a given age. So for example, of the 100,000 males &amp;ldquo;born&amp;rdquo; at age 0, we expect 99,392 to be alive at age 1. The life expectancy is the expected number of years of life remaining for an individual. We can start by plotting this to get a feel for the data.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-survival-curve-for-100000-males-and-females-given-the-2019-ssa-mortality-tables-the-dashed-line-indicates-the-typical-retirement-age-of-67&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-09-life-expectancy/survivalPlot_hu8976ee6293875d841810953bda6bf5f5_123683_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Survival curve for 100,000 males and females given the 2019 SSA mortality tables. The dashed line indicates the typical retirement age of 67.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-09-life-expectancy/survivalPlot_hu8976ee6293875d841810953bda6bf5f5_123683_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2606&#34; height=&#34;1684&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Survival curve for 100,000 males and females given the 2019 SSA mortality tables. The dashed line indicates the typical retirement age of 67.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;To get a feeling for the distribution of age at death, I ran 10,000 simulations each for males and females starting at ages 0, 25, 40, 60, and 80. These ages were chosen to represent the full range of possibilities at birth, followed by early, mid- and late career individuals. Age 80 was included for comparison as an older retiree value. Since the probability of death by age 50 is so low, we expect very little difference for the first three ages, with differences becoming more pronounced as age progresses, but it is still useful to visualize.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-overview-of-the-distribution-of-age-at-death-by-sex-for-different-ages-at-the-beginning-of-the-simulation-outliers-are-are-not-represented-note-how-the-results-for-ages-0-to-40-are-nearly-indistinguishable&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-09-life-expectancy/Sample_Overview_hub6662c611b080d5ff9aca3d611ac8c57_52510_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Overview of the distribution of age at death by sex for different ages at the beginning of the simulation. Outliers are are not represented. Note how the results for ages 0 to 40 are nearly indistinguishable.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-09-life-expectancy/Sample_Overview_hub6662c611b080d5ff9aca3d611ac8c57_52510_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2606&#34; height=&#34;1684&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Overview of the distribution of age at death by sex for different ages at the beginning of the simulation. Outliers are are not represented. Note how the results for ages 0 to 40 are nearly indistinguishable.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;As expected, we see relatively little variation between birth and age 40, with some recognizable changes beginning at age 60. Given that, I will visualize the distribution for an individual starting at age 40. A 40 year old is about 25-30 years away from retirement and has probably at least started thinking about saving and how much they&amp;rsquo;ll need to put away to last through retirement.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-distribution-of-age-at-death-for-males-and-females-given-a-starting-age-of-40-half-of-the-starting-population-is-expected-to-make-it-to-at-least-8185-malefemale-and-a-quarter-will-make-it-at-least-to-8891&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-09-life-expectancy/distributionPlot_hue89b8b06e05cd8b82745d76fbdceaf60_146013_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Distribution of age at death for males and females given a starting age of 40. Half of the starting population is expected to make it to at least 81/85 (Male/Female), and a quarter will make it at least to 88/91.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-09-life-expectancy/distributionPlot_hue89b8b06e05cd8b82745d76fbdceaf60_146013_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2606&#34; height=&#34;1684&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Distribution of age at death for males and females given a starting age of 40. Half of the starting population is expected to make it to at least 81/85 (Male/Female), and a quarter will make it at least to 88/91.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;So now that have seen the distribution, let&amp;rsquo;s consider how long we&amp;rsquo;ll live past the typical retirement age of 67. The table below lists the ages by sex for the top percentiles given a starting age of 40.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Top Percentiles - Age at Death&lt;/th&gt;
&lt;th&gt;Males&lt;/th&gt;
&lt;th&gt;Females&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;5%&lt;/td&gt;
&lt;td&gt;95&lt;/td&gt;
&lt;td&gt;98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10%&lt;/td&gt;
&lt;td&gt;93&lt;/td&gt;
&lt;td&gt;95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;20%&lt;/td&gt;
&lt;td&gt;89&lt;/td&gt;
&lt;td&gt;92&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;30%&lt;/td&gt;
&lt;td&gt;86&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;40%&lt;/td&gt;
&lt;td&gt;84&lt;/td&gt;
&lt;td&gt;87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;50%&lt;/td&gt;
&lt;td&gt;81&lt;/td&gt;
&lt;td&gt;85&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that 40% of females and 30% of males are expected to live at least 20 years past retirement age. A little more than 5% of females will make it thirty years past retirement, but only 2.5% of males will. While only a small minority of retirees will need to fund their retirement for thirty or more years, it is not unreasonable to target retirement funds to last until we reach age 90.&lt;/p&gt;
&lt;p&gt;Unfortunately, a large share of Americans have insufficient 401k balances to cover their expected longevity (see 
&lt;a href=&#34;https://www.forbes.com/advisor/retirement/average-401k-balance-by-age/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, 
&lt;a href=&#34;https://www.investopedia.com/articles/personal-finance/010616/whats-average-401k-balance-age.asp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, and 
&lt;a href=&#34;https://mint.intuit.com/blog/retirement/average-401k-balance-by-age/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for some estimates of savings by age group). Many are likely relying on social security benefits to cover some of the difference. This system may not last that long, or at least not with current benefit levels. Social Security outlays have exceeded allocated revenues since 2010 and are currently expected to continue to do so well into the 2090s (see 
&lt;a href=&#34;https://www.cbo.gov/publication/57342&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Table A-1&lt;/a&gt;). Social security trust fund balances for old-age and survivor benefits are rapidly declining. Between 2020 and 2030, the CBO expects a drop of 80% in this fund. Curiously, over the same time period the trust fund for military personnel is expected to grow by more than 70%, while the fund for civilian government employees is expected to grow by more than 20% (see 
&lt;a href=&#34;https://www.cbo.gov/publication/56541&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CBO report&lt;/a&gt;). As such, younger Americans not working for the government will need to consider how to fund a multi-decade retirement in the face of potentially large reductions in social security benefits.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is it better to buy or rent housing?</title>
      <link>https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/</link>
      <pubDate>Sat, 20 Aug 2022 16:21:38 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/</guid>
      <description>&lt;p&gt;This post is a follow-up to my post on how to load 
&lt;a href=&#34;https://dmsenter89.github.io/post/22-08-zillow-data/&#34;&gt;data from Zillow&lt;/a&gt;. Housing prices have soared through the COVID-19 pandemic, leading to a lot of discussion about housing affordability. The quickly growing home values coupled with the subsequent raising of interest rates on mortgages are seeing more and more people priced out of  the ability to purchase a home. While rent prices have increased as well, they haven&amp;rsquo;t increased as sharply as home prices.&lt;/p&gt;
&lt;p&gt;In this post, I will look at the Zillow data set and consider a popular question for millennials - is it better to buy or rent in the current market?
For this analysis, I will use the zip code level data of the Zillow Home Value Index (ZHVI) and the Zillow Observed Rent Index (ZORI) for North Carolina metropolitan areas in the years 2019 through June 2022. To evaluate the monthly costs of owning a home, I will utilize the 30 year fixed rate mortgage average from the 
&lt;a href=&#34;https://fred.stlouisfed.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FRED database&lt;/a&gt;, a comprehensive database aggregating various economic time series maintained by the St. Louis Federal Reserve.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: This post is not financial advice. We are only exploring some aggregate data sets. Past performance is not indicative of future performance.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#initial-thoughts&#34;&gt;Initial Thoughts&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#crafting-a-mortgage-to-rent-index&#34;&gt;Crafting a Mortgage-to-Rent Index&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#a-naive-implementation&#34;&gt;A Naive Implementation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#a-slightly-better-implementation&#34;&gt;A Slightly Better Implementation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#further-thoughts&#34;&gt;Further Thoughts&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#risk-and-other-costs&#34;&gt;Risk and Other Costs&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#housing-as-an-investment&#34;&gt;Housing as an Investment&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#last-thoughts&#34;&gt;Last Thoughts&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;

&lt;h2 id=&#34;initial-thoughts&#34;&gt;Initial Thoughts&lt;/h2&gt;
&lt;p&gt;Both Zillow data sets are available with monthly data published at the
zip code level. As such, they are equally well-spaced in time. One issue
we notice at initial inspection is that the ZHVI observations are dated to
the last day of every month, while the ZORI observations are dated to the first of the month. For merging and comparison, we will set the ZHVI to the first of the month.&lt;/p&gt;
&lt;p&gt;Both data sets are available for download at the zip code level. The ZORI data set is much smaller, however. It covers 2,453 distinct zip codes compared to the ZHVI&amp;rsquo;s 27,366 distinct zip codes.&lt;/p&gt;
&lt;p&gt;An initial graph of average ZHVI and ZORI for North Carolina shows the dramatic growth in home values and the degree to which rent is lagging behind. Both y-axes have been scaled proportionally.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-average-zillow-home-value-index-zhvi-and-zillow-observed-rent-index-zori-values-for-north-carolina-the-left-and-right-axes-have-been-scaled-proportionally&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/average_ZHVI_ZORI_huc9fc43bcc57fade6a9155dc83f0fbcac_25161_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Average Zillow Home Value Index (ZHVI) and Zillow Observed Rent Index (ZORI) values for North Carolina. The left and right axes have been scaled proportionally.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/average_ZHVI_ZORI_huc9fc43bcc57fade6a9155dc83f0fbcac_25161_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Average Zillow Home Value Index (ZHVI) and Zillow Observed Rent Index (ZORI) values for North Carolina. The left and right axes have been scaled proportionally.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The FRED data set is published weekly on Thursdays
and provides a seasonally unadjusted look at the average, actual mortgage rates that
homebuyers have received throughout the US. So while this data set has the largest number of data points in time, it is the least granular on a geographic level. I don&amp;rsquo;t have detailed information about the geographic variability in mortgage rates and will ignore this for now. To allow merging with the Zillow data, the FRED data set will need to be averaged in some form. For this post, I will use simple averages by month. It is important to note that the FRED data is not in decimal notation. That means that 3.5% is written as 3.5 as opposed to 0.035.&lt;/p&gt;
&lt;p&gt;One thing that is interesting about the FRED data is that it goes back as far as 1971. Looking at the overall historic values, we see that the past decade&amp;rsquo;s very low interest rates (less than 5%) are an anomaly.&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/fredgraph_hucd93d0035b9764d5c892e209e7d9935a_74003_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/fredgraph_hucd93d0035b9764d5c892e209e7d9935a_74003_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1168&#34; height=&#34;450&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;crafting-a-mortgage-to-rent-index&#34;&gt;Crafting a Mortgage-to-Rent Index&lt;/h2&gt;
&lt;p&gt;I will create two indices, one being a very naive implementation focusing just on the actual monthly payment on the mortgage loan and the monthly rent payments. This is followed by an improved index that takes into account additional monthly expenses. After both indices have been created, I will discuss some of their shortcomings.&lt;/p&gt;
&lt;h3 id=&#34;a-naive-implementation&#34;&gt;A Naive Implementation&lt;/h3&gt;
&lt;p&gt;For monthly cost comparisons, we don&amp;rsquo;t actually care about the value of the home directly. What matters is the monthly mortgage cost. There is a lot of variability here and I will make the following assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The mortgage originates in the month given by &lt;code&gt;Date&lt;/code&gt; at the rate given by the monthly average of the Mortgage30US time series.&lt;/li&gt;
&lt;li&gt;A 20% down payment was made, so that the mortgage amount is for 80% of the ZHVI. A 20% down payment is often recommended because it avoids the need for mortgage insurance, which would create an additional monthly expense.&lt;/li&gt;
&lt;li&gt;Closing costs are handled separately and not rolled into the mortgage.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Monthly mortgage payments can be calculated with the 
&lt;a href=&#34;https://go.documentation.sas.com/doc/en/vdmmlcdc/8.1/ds2ref/n0a5e0wwqhslcvn1y173c5c4nbg2.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PMT function&lt;/a&gt; in SAS. The general syntax is &lt;code&gt;PMT( rate, number-of-periods, principal-amount [, future-amount] [, type])&lt;/code&gt;. The &lt;code&gt;rate&lt;/code&gt; is the APR divided by the number of periods in a year, in our case 12. The number of periods is 12 payments over 30 years, i.e. 360. The principal amount is 80% of ZHVI, while the future amount is zero (default value). SAS allows us to pick of we want to use end of period (&lt;code&gt;type=0&lt;/code&gt;) or beginning of period (&lt;code&gt;type=1&lt;/code&gt;) payments. For the monthly amount the difference is small, so I will use the default end of period scheme. With that, our formula for the monthly payment is &lt;code&gt;PMT(M30Rate/12, 360, 0.8*ZHVI)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now construct a simple unit-less ratio of monthly mortgage costs over monthly rent. If this index is greater than 1, it is cheaper to rent while if it is less than 1 it is cheaper to purchase a home. Below is a figure demonstrating the distribution of this index in North Carolina for 2019 through June 2022.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-mean-index-value-for-north-carolina-the-shaded-region-represents-the-25th-through-75th-percentile-of-index-values-note-that-the-index-is-1-until-2022-despite-the-sharp-increase-in-zhvi-across-the-state-seen-above&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/index_mean_q1_q3_huadcd954adcdbd90973c926bbc7d2269a_19435_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Mean index value for North Carolina. The shaded region represents the 25th through 75th percentile of index values. Note that the index is &amp;amp;lt;1 until 2022, despite the sharp increase in ZHVI across the state seen above.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/index_mean_q1_q3_huadcd954adcdbd90973c926bbc7d2269a_19435_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Mean index value for North Carolina. The shaded region represents the 25th through 75th percentile of index values. Note that the index is &amp;lt;1 until 2022, despite the sharp increase in ZHVI across the state seen above.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The index indicates that the situation was favorable to home buyers until 2022, when it flips to being generally favorable to renters. FRED data indicate that the average mortgage rate didn&amp;rsquo;t substantially begin growing until winter 2021. This lends credence to the view that historically low interest rates have helped buffer would-be homeowners from increases in home value.&lt;/p&gt;
&lt;p&gt;Individual zip codes&#39; index values behave similar in pattern to the the figure above, making it a good approximation to observed behavior.&lt;/p&gt;
&lt;h3 id=&#34;a-slightly-better-implementation&#34;&gt;A Slightly Better Implementation&lt;/h3&gt;
&lt;p&gt;The previous implementation underestimates the true monthly cost of home ownership. For one, homeowners are required to pay property taxes each year. While an individual doesn&amp;rsquo;t directly pay the local government on a monthly basis, funds for this purpose are typically collected in an escrow account so it is an ongoing cost. Property tax rates are set at the county and municipal levels and mapping them directly to the ZIP codes requires some work. The rates themselves are fixed fees for every $100 of assessed home value. For the purpose of taxation, the home value used is &lt;em&gt;not&lt;/em&gt; the ZHVI, but the value assigned to the home by the county. In North Carolina these assessments must take place at least once every 8 years, but individual counties may choose to do more frequent assessments. As such, there can be a large discrepancy between the ZHVI and the assessed value. I have seen cases where the ZHVI is roughly double that of the county&amp;rsquo;s assessed value. Finding the average assessed value is easy for individual homes (Zillow displays it on its website), but finding an aggregate value that can be used for analysis is tricky. For ease of use I&amp;rsquo;ll assume that assessed value is 70% of ZHVI. I expect this measure to underestimate the assessed values of homes in 2019 but to be close to target in 2022.&lt;/p&gt;
&lt;p&gt;Based on the 
&lt;a href=&#34;https://www.ncdor.gov/taxes-forms/property-tax/property-tax-rates&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;most recent&lt;/a&gt; effective tax rates published by the North Carolina Department of Revenue, the average tax rate is approximately 1.02%, with a minimum of 0.33% and a maximum of 1.7%.&lt;/p&gt;
&lt;p&gt;In addition to taxes, escrow accounts will typically collect monthly insurance fees as well. The two main types of insurance included are homeowners insurance and mortgage insurance. Mortgage insurance is mandatory when purchasing a home with less than a 20% down payment and protects the lender from the borrower defaulting on the loan repayment. In the above example we assumed a 20% down payment, so mortgage insurance would be optional.&lt;/p&gt;
&lt;p&gt;The cost of homeowners insurance varies wildly as can be seen in 
&lt;a href=&#34;https://www.nerdwallet.com/article/insurance/average-homeowners-insurance-cost&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this article&lt;/a&gt; from NerdWallet. Their calculated average monthly cost for North Carolina is $142. Renters may be required to purchase renters insurance, but this is not a requirement for all properties and the cost is substantially lower than homeowners insurance. NerdWallet&amp;rsquo;s 
&lt;a href=&#34;https://www.nerdwallet.com/article/insurance/how-much-is-renters-insurance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;analysis&lt;/a&gt; gives a monthly average cost of $12 for North Carolina.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-mean-adjusted-index-value-for-north-carolina-the-shaded-region-represents-the-25th-through-75th-percentile-of-index-values&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/index_mean_q1_q3_adj_huce0c902e7276254854e7cbbd20be94dc_20731_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Mean adjusted index value for North Carolina. The shaded region represents the 25th through 75th percentile of index values.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/index_mean_q1_q3_adj_huce0c902e7276254854e7cbbd20be94dc_20731_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Mean adjusted index value for North Carolina. The shaded region represents the 25th through 75th percentile of index values.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;As expected, the adjusted index shows the same overall behavior but is shifted up. This index gives a more complicated picture, in which the zip code starts mattering somewhat more in whether renting or buying is cheaper on a monthly basis.&lt;/p&gt;
&lt;h2 id=&#34;further-thoughts&#34;&gt;Further Thoughts&lt;/h2&gt;
&lt;p&gt;The above analysis was relatively simple, but a good excuse to take a look at the data made available by Zillow. This is not the end of the story, however. I have made several simplifications here, although on average I would argue they were in favor of home purchases. There additional matters to consider,  which I will take up below. We also only discussed aggregate data. Particular housing may offer other benefits, especially if you have knowledge of some likely future events. For example, a large tech company planning to move to an area tends to increase both rent and housing prices, so purchasing now may be beneficial by locking in today&amp;rsquo;s prices. We also haven&amp;rsquo;t considered whether there are differences in housing stock and location that may outweigh purely financial considerations.&lt;/p&gt;
&lt;h3 id=&#34;risk-and-other-costs&#34;&gt;Risk and Other Costs&lt;/h3&gt;
&lt;p&gt;Another item to consider is the question of risk in either scenario. A homeowner incurs somewhat larger risks of on-going expenses that we have conveniently ignored. Roofs, HVAC systems, etc. need maintenance and eventually will need to be repaired. Neither last forever either, so they will each need to be replaced at least once, if not more often, during the 30 years the house is being paid down. A 
&lt;a href=&#34;https://ipropertymanagement.com/research/hoa-statistics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;majority&lt;/a&gt; of new construction and a large number of existing homes are part of a Home Owners Association (HOA). Being part of an HOA not only increases monthly cost due to HOA fees, but also exposes a homeowner to the risk of special assessments. These are one-time financial obligations on homeowners to cover some expense for the HOA.&lt;/p&gt;
&lt;p&gt;While a renter may be immune to costs such as these, they are not immune to changing rent prices. By having a fixed-rate mortgage on the other hand, monthly housing expenses stay locked at current rates. Furthermore, while rent is not tax deductible, mortgage interest is. Under current guidelines, up to 750,000 USD in mortgage interest on your primary home can be deducted from your income tax. This is especially valuable during the first 15 years of the loan, when interest accounts for the majority of the mortgage payment.&lt;/p&gt;
&lt;h3 id=&#34;housing-as-an-investment&#34;&gt;Housing as an Investment&lt;/h3&gt;
&lt;p&gt;One issue that we have ignored so far is the idea of home purchases as investments. Home prices have historically increased. Each year lived and mortgage paid on a property increases your share in the home&amp;rsquo;s value, otherwise known as equity. Some would argue that even if the monthly cost of homeownership is in excess of the cost of renting an equivalent home, it&amp;rsquo;s worth it in the long run as an investment in your financial future. Proponents may point to homeownership being a key component in long term wealth accrual.&lt;/p&gt;
&lt;p&gt;Assessing the value of equity growth in wealth building needs to be viewed in opposition to investing the potential price differential. We are only talking about the difference in housing cost being utilized here, not in using additional funds to pay down a mortgage early. This has not been an effective strategy for nearly 30 years.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; The S&amp;amp;P 500&amp;rsquo;s average annual return during the period of 1975-2021 has been 10.2%. Home values on the other hand increased annually by an average of only 4.5% between 1975 and today.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; This would indicate investment in the S&amp;amp;P 500 would be expected to outperform real estate investment for the average person over the long run.&lt;/p&gt;
&lt;p&gt;The variability of the two is rather different, however. For comparison, we can show a distribution of annual returns from the S&amp;amp;P 500 compared to FRED&amp;rsquo;s All-Transactions House Price Index.&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-histograms-showing-the-distribution-of-annual-returns-of-the-all-transactions-house-price-index-for-north-carolina-and-the-sp-500-for-1976-2021&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/investment_housing_sp_comp_hu4e2264ca40b96fcef6fef1eaaeebef26_27041_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Histograms showing the distribution of annual returns of the All-Transactions House Price Index for North Carolina and the S&amp;amp;amp;P 500 for 1976-2021.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-08-should-you-buy-or-rent/investment_housing_sp_comp_hu4e2264ca40b96fcef6fef1eaaeebef26_27041_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;960&#34; height=&#34;720&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Histograms showing the distribution of annual returns of the All-Transactions House Price Index for North Carolina and the S&amp;amp;P 500 for 1976-2021.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;While this shows that we can expect greater payoff from stock market investments compared to real estate, this is not helpful if we have to pay for housing and don&amp;rsquo;t have excess funds to throw at the stock market. If the actual monthly cost of homeownership and rent are about equal and max out our available funds, then purchasing a home builds at least &lt;em&gt;some&lt;/em&gt; equity, no matter how small, compared to no investment being made. Only when rent is lower than mortgage costs and fees does it make sense to start comparing rates of return.&lt;/p&gt;
&lt;p&gt;We should never underestimate the power of compounding, however. Consider the following example: you have the option of purchasing a 400,000 USD home with a 80,000 USD (20%) down payment at 5% interest as a 30 year fixed rate mortgage. Assume no other costs. In total, this housing purchase will cost you approximately 698,000 USD over the course of 30 years.  Assuming 4.53% annual increase in housing value, this leaves you with about 1,553,000 USD in equity at the end. Given your total cost you have made a profit of approximately 855,000 USD. Now compare this to stock market returns, assuming the 10.22% annual growth we found above. Investing the down payment only, with no further payments made, would leave you with approximately 1,695,000 USD after 30 years. To match the amount of equity in our example home after 30 years would require only a 73,500 USD in initial investment. If we only wanted to match the profit of about 855,000 USD with a one-time investment, we would require an initial investment of approximately 42,500 USD.&lt;/p&gt;
&lt;p&gt;Despite these superior returns from investment, we still need to live somewhere. I have already mentioned that unlike a mortgage payment, rent is not fixed over the 30 year timespan. How much should I expect renting to cost me? Let&amp;rsquo;s assume we invest the entire down payment and rent a home that costs exactly the same as the mortgage would have cost. Assuming once annual 
&lt;a href=&#34;https://ipropertymanagement.com/research/average-rent-by-year&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;increases in rent of 4.17%&lt;/a&gt;, we spend a total of nearly 1,190,000 USD on rent over the course of 30 years. Given these expenses, the profit from investing the 80,000 USD shrinks to a mere 500,000 USD.&lt;/p&gt;
&lt;p&gt;This suggests a strategy of minimizing the down payment and investing the difference, if possible. One downside of this is that having a smaller down payment may lead to less favorable mortgage terms. In this second scenario, let&amp;rsquo;s assume that we make a 40,000 USD down payment on a 400,000 USD home with an increased, but still fixed, mortgage rate of 5.25% over 30 years coupled with a 40,000 USD initial investment in the S&amp;amp;P 500 at 10.22% increase annually with no additional investments made. Here we invest a total of approximately 766,000 USD into our home, reducing our profit on the home purchase to approximately 797,000 USD. But our initial 40,000 USD investment has accumulated to about 847,000 USD. This brings our total profit to approximately 1,644,000 USD in this scenario - nearly double the 855,000 USD profit in the 80,000 USD down payment scenario.&lt;/p&gt;
&lt;h3 id=&#34;last-thoughts&#34;&gt;Last Thoughts&lt;/h3&gt;
&lt;p&gt;Overall, we see that the more precise we want to be, the more difficult it becomes to estimate an ideal strategy as there are a lot of moving pieces, not all of which can best be approximated by a geographic average value. Given the very different variability in housing value increase compared to stock market returns, it may make sense to generate a large number of random walks to get a better feel for the distribution of outcomes after 30 years. Finally, this entire post has assumed that we remain living in the same home for the entire 30 year life of the mortgage. Many will want to move at some point during their next thirty years, perhaps to make room for a larger  family, to down-size, or to follow a job opportunity. This creates additional costs and considerations for deciding between renting and owning a home.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Average mortgage rates have been below average annual return of the S&amp;amp;P 500 since the 1990s. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Annual returns of S&amp;amp;P 500 were calculated from data provided by 
&lt;a href=&#34;https://www.investopedia.com/ask/answers/042415/what-average-annual-return-sp-500.asp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Investopedia&lt;/a&gt;. Annual returns of housing prices were calculated from the All-Transactions House Price Index for North Carolina (FRED time series NCSTHPI). &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;I have chosen this measure for historic house price increases as this index is available on a quarterly basis starting in 1975, unlike Zillow data which only goes back a little more than 20 years. Both US wide data (USSTHPI) and state specific data (e.g., for North Carolina: NCSTHPI) are available from FRED. I will use the North Carolina data set for my comparison. An alternative measure to consider is the median sales price of houses sold in the South census region, available on a quarterly basis starting in 1965 from FRED (MSPS). &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Cross Compiling With Go</title>
      <link>https://dmsenter89.github.io/post/22-08-cross-compiling-with-go/</link>
      <pubDate>Thu, 11 Aug 2022 09:15:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-08-cross-compiling-with-go/</guid>
      <description>&lt;p&gt;A big difference between using compiled languages like C/C++ compared to scripting languages
like Javascript or Python is that prior to execution, compiled languages require an explicit
compilation step where the human readable code is translated to machine code for execution,
the so-called &amp;ldquo;binary&amp;rdquo; of the code. Typically, a separate binary needs to be compiled for
each target operating system and architecture. Compiling for your own machine is not a problem.
The difficulty lies in creating binaries for machines that you don&amp;rsquo;t normally use, so you might
not have an extra Mac lying around just to compile your program on for other Mac users.
Compiling programs for an operating system or architecture other than the one you are working
with is called cross-compiling. This would allow a Linux developer to create binaries for
Windows and Mac computers, for example.&lt;/p&gt;
&lt;p&gt;For most languages, this requires installing additional development tools and increases the
complexity of the compilation workflow. I have found Go to be a pleasant exception to this,
because cross-compilation is built into the standard Go tools. There is no need to learn
any additional build-tools. All you need to learn about are some system variables that you
need to set when compiling.&lt;/p&gt;
&lt;p&gt;Go build tools know which system you are building for by checking the GOOS and GOARCH environment
variables. If they are unset, the tools fall back to GOHOSTOS and GOHOSTARCH. In other words,
to change the target OS/architecture for your build, all you have to do is set the GOOS and GOARCH
variables during the build. So say you want to build a simple program hello.go for a Windows
computer with the same architecture as your development machine. All you have to do is write&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;GOOS=windows go build hello.go
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;instead of just &lt;code&gt;go build hello.go&lt;/code&gt; and you&amp;rsquo;re good to go. This would produce a &lt;code&gt;hello.exe&lt;/code&gt;
binary you could copy to a Windows machine to run.&lt;/p&gt;
&lt;p&gt;To check what combinations of GOOS and GOARCH are valid, run &lt;code&gt;go tool dist list&lt;/code&gt;. To see
which environment variables Go is currently seeing, run &lt;code&gt;go env&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Loading Zillow Housing Data in SAS</title>
      <link>https://dmsenter89.github.io/post/22-08-zillow-data/</link>
      <pubDate>Mon, 01 Aug 2022 17:21:38 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-08-zillow-data/</guid>
      <description>&lt;p&gt;Zillow is a well-known website widely used by those searching for a home or curious to find out
the value of their current home. What you may not know is that Zillow has a dedicated research page.
To make their website work optimally, they churn through tons of data on the American housing market.
They share insights they gleaned via 
&lt;a href=&#34;https://www.zillow.com/research/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;zillow.com/research&lt;/a&gt;. If you
visit their research  website you&amp;rsquo;ll notice they have a data page where you can download some really
cool data sets for your own research. They even have an API with which you can load data directly, but
you&amp;rsquo;ll have to register for access. In this post, we&amp;rsquo;ll look at how to load the CSV files that are
available for direct download into SAS for analysis.&lt;/p&gt;
&lt;p&gt;The CSV files can be downloaded 
&lt;a href=&#34;https://www.zillow.com/research/data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. In the example below,
I&amp;rsquo;m working with the Zillow Home Value Index file for all homes, seasonally adjusted at the ZIP code level.
Tha file is fairly large. It has data going from January 2000 through June 2022 in more than 27,000 rows of data
and about 280 columns. Below is an image of the beginning of this file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;zhvi_data_preview.png&#34; alt=&#34;&#34; title=&#34;The beginning of the of the ZHVI &#39;flagship&#39; data file.&#34;&gt;&lt;/p&gt;
&lt;p&gt;When working with large CSV files, I find it useful to get a feel for it in the CLI with

&lt;a href=&#34;https://csvkit.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;csvkit&lt;/a&gt;. This is especially important when importing
with a SAS data step, because we need to know the number of columns and their order, amongst other things,
for our code. To get an overview of the total number of columns and their contents, run&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;csvcut -n Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is fairly long, so you may prefer piping to a pager. I don&amp;rsquo;t need all the different identifiers
in the file, so I&amp;rsquo;m going to exclude those I won&amp;rsquo;t need and put them into a separate, smaller CSV.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# ignore these four columns which I won&#39;t need
csvcut -C RegionID,SizeRank,RegionType,StateName Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv &amp;gt; Zip_zhvi_small.csv
# alternatively, also cut down on date columns to only 2022 for debugging 
csvcut -C RegionID,SizeRank,RegionType,StateName,10-273 Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv &amp;gt; Zip_zhvi_small.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also reduce the file size by using &lt;code&gt;csvgrep&lt;/code&gt; to filter any of the columns. For example, if we only wanted
the data for North Carolina we could run &lt;code&gt;csvgrep -c State -m NC&lt;/code&gt; in the pipe.&lt;/p&gt;
&lt;p&gt;For SAS, we need to know the maximum length of string columns so we can allocate the appropriate length to the
corresponding SAS variables. This is easily done with the csvstat tool:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;csvcut -c Metro,City,CountyName Zip_zhvi_small.csv | csvstat --len
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also specify the list of columns in csvstat directly, but in my experience that tends to be slower.&lt;/p&gt;
&lt;p&gt;Alright, now we have everything we need to start on our DATA step! We start with the attribute statement.
One problem with importing this file is that everyhing is in wide format, with the dates used as headers.
We will get around this shortly. I have seen people use transpose etc for similar problems online, but this
is unnecessary if we feel comfortable with the DATA step. We&amp;rsquo;ll start by naming the identifying columns
just as in the CSV file. For the date columns, we will use a numeric range prefixed by date (&lt;code&gt;date1-date270&lt;/code&gt;).
You can use csvcut to find the exact number of date columns you have. We will also allocate the same number of
columns for the ZHVI values, so we&amp;rsquo;ll need to add a &lt;code&gt;val1-val270&lt;/code&gt;. This and the date variable are temporary
and will be dropped later, in favor of the &lt;code&gt;Date&lt;/code&gt; and &lt;code&gt;ZHVI&lt;/code&gt; variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;attrib 
    ZIP           informat=best12.    format=z5.
    State         informat=$2.
    City          informat=$30.
    Metro         informat=$42.
    CountyName    informat=$29.
    date1-date270 informat=YYMMDD10.  format=DATE9.
    val1-val270   informat=best16.
    Date                              format=Date9.
    ZHVI                              format=Dollar16.
  ;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we will allocate an array to hold &lt;em&gt;all&lt;/em&gt; of the date and ZHVI values during the processing of each row.
Since the date column won&amp;rsquo;t change, we&amp;rsquo;ll tell SAS to retain its values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;   retain date1-date270;
   array d(270) date1-date270;
   array v(270) val1-val270;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is where the magic happens now. You may not know it, but you are not limited to a single INPUT statement
in a DATA step. We use this and start by reading in only the first row. Because we use an OUTPUT
statement later, this reading of row 1 will be processed, but not saved into the output data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;if _n_ = 1 then do;
  input ZIP $ State $ City $ Metro $ CountyName $ date1-date270;
  PUT _ALL_; /* if you want to see what that looks like */
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this if clause, the date1 through date270 variables will be populated, and because we used a retain
statement earlier, these values remain available to us during the processing of every other row. You can
probably guess where this is going now: we will process each row, and then OUTPUT one line per date which
we have access to now thanks to our array and the retain statement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;input ZIP $ State $ City $ Metro $ CountyName $ val1-val270;
do i=1 to 270;
  Date  = d(i); /* look up date for column i */
  ZHVI =  v(i); /* use the corresponding i-th value for ZHVI */
  OUTPUT;       /* This output creates one line per date column */
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the end of your data step, don&amp;rsquo;t forget to&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;drop i date1-date270 val1-val270;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;so those variables don&amp;rsquo;t clutter your data set. And that&amp;rsquo;s it! You now
have the data set loaded and available in SAS.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;SAS_data_set.png&#34; alt=&#34;&#34; title=&#34;The beginning of the resulting SAS data set.&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Family History And Chronic Medical Conditions Associated With Sudden Death Among Working Age Adults</title>
      <link>https://dmsenter89.github.io/publication/doshi-2022-family-history/</link>
      <pubDate>Wed, 15 Jun 2022 10:00:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/publication/doshi-2022-family-history/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Sudden death accounts for $10%$ of deaths in the United States. Prior research has
focused on sudden death in older victims, leaving much unknown about risk factors for younger,
working age adults. Compared to older adults, younger adults may be more vulnerable to genetic factors.
Understanding age related differences in sudden death risk factors may guide future prevention efforts,
as the factors contributing to sudden death in younger patients may warrant different types of prevention
than those affecting older adults.&lt;/p&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods&lt;/h2&gt;
&lt;p&gt;From 2013-2015, out-of-hospital deaths among adults aged 18-64 in Wake County, NC were screened and adjudicated
to identify 306 sudden death victims. A comparison group of 1,113 patients matched for age, gender, and residence
was formed by randomly sampling individuals from the Carolina Data Warehouse. For sudden death victims and controls,
the prevalence of sudden death risk factors, comprising comorbidities, mental illnesses, and family history variables,
were assessed in three age groups (18-41, 42-54, and 55-64 years old). Hypothesis testing was conducted for each
variable across all pairwise combinations of age groups using an unpaired, two-sample proportion Z-test with $p&amp;lt;0.05$
as statistically significant.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;None of the variables analyzed were more prevalent in younger sudden death victims compared to older victims.
However, family history variables were largely missing ($&amp;gt;80%$) compared to comorbidities and mental illnesses ($&amp;lt;10%$),
with younger adults being disproportionately affected.&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Underreporting of family history in medical records leaves younger adults as a poorly understood subset of sudden death
victims who are currently unable to be appropriately screened for lifesaving preventive measures.&lt;/p&gt;
&lt;h2 id=&#34;public-health-implications&#34;&gt;Public Health Implications&lt;/h2&gt;
&lt;p&gt;Better family history documentation may identify younger adults at higher risk of sudden death, allowing more
comprehensive population level screening and implementation of prevention measures appropriate for addressing genetic
factors instead of the environmental factors more common in older adults.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conditional RegEx Matching with Python</title>
      <link>https://dmsenter89.github.io/post/22-05-conditional-regex-python/</link>
      <pubDate>Thu, 19 May 2022 21:45:46 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-05-conditional-regex-python/</guid>
      <description>&lt;p&gt;Recently I&amp;rsquo;ve needed to capture the entries of a datalines statement in SAS for editing. Generally,
this is a straight forward problem if I only need to do it with one file or all of the files that
I am using are formatted identically. But then I started thinking about the more general case. SAS
doesn&amp;rsquo;t care about the case of my keywords, so I need a case insensitive match. I need to account for
possible extra whitespace. So far so good. But what if I have two different keywords that can start
my data section, and the end of the data section is indicated with different characters depending on
the chosen keyword? Could I still use a single regular expression?&lt;/p&gt;
&lt;p&gt;SAS does in fact allow a number of different keywords to enter data in a data step.
In my experience, the most common are the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/v_027/lestmtsref/p0114gachtut3nn1and4ap8ke9nf.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;datalines&lt;/a&gt;
and 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/v_027/lestmtsref/p1mm9b070wj962n16q0v1d9uku5q.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;datalines4&lt;/a&gt;
statements. The main difference between them is how the end of the data is indicated. For datalines,
a single semicolon is used, while datalines4 uses a sequence of four semicolons, thereby allowing
the use of semicolons in the data itself. There are some aliases for these commands that can be used:
cards/lines and cards4/lines4 with matching behavior. A simple data step with these statements
could look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data person;
  input name $ sex $ age;
  datalines; /* or `cards` or `lines` */
Alfred M 14
Alice F 13
;

data person4;
  input name $ sex $ age;
  datalines4; /* or `cards4` or `lines4` */
Alfred M 14
Alice F 13
;;;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could write two separate RegEx expressions, one for the datalines/cards/lines statement and
a second one for the datalines4/cards4/lines4 statement. But, if the RegEx engine we are using allows
conditionals, e.g. the Python RegEx engine, then we can write a single statement that can capture
both types of statements. The basic format of the conditonal capture is &lt;code&gt;(?(D)A|B)&lt;/code&gt;, which can be read as &amp;ldquo;if capture
group D is set, then match A, otherwise match B.&amp;rdquo; For more details, see 
&lt;a href=&#34;https://www.regular-expressions.info/conditional.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Using this technique, we can capture both types of statements in one go.
The short form of the solution I found is this
regular expression: &lt;code&gt;r&amp;quot;(?:(?:(?:data)?lines)|cards)(4)?\s*;(.*?)(?(1);{4}|;)&amp;quot;&lt;/code&gt;
with two flags set: case insensitive and dot-all. If we utilize Python&amp;rsquo;s verbose flag,
we can format this a bit nicer as well:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;re.compile(
  r&amp;quot;&amp;quot;&amp;quot;(?:(?:    # mark groups as non-capture groups
      (?:data)? # maybe match `data`, but don&#39;t capture
       lines)   # matches `lines`
      |cards)   # alternatively, matches `cards`
      (4)?      # a `4` may be present
      \s*;      # there might be whitespace before the ;
      (.*?)     # lazy-match data content
      (?(1)     # check if capture group 1 is set, if so
      ;{4}      # match `;;;;`
      |;)       # otherwise, match a single ;
  &amp;quot;&amp;quot;&amp;quot;, flags=re.DOTALL | re.X | re.I)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A great website to help you build up a regular expression is 
&lt;a href=&#34;https://regex101.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;regex101.com&lt;/a&gt;.
It allows you to copy a sample text and regular expression. It then explains your expression and lists
the capture groups by number, which can be convenient. It also allows you to try out different RegEx engines.
Try setting it to Python with the flags we mentioned, and see how it works!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wordle in Golang</title>
      <link>https://dmsenter89.github.io/post/22-05-go-wordle/</link>
      <pubDate>Thu, 05 May 2022 16:30:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-05-go-wordle/</guid>
      <description>&lt;p&gt;Lately I&amp;rsquo;ve been playing around with 
&lt;a href=&#34;https://go.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Go&lt;/a&gt;. I&amp;rsquo;ve read about Go for a few years
and have been using some software written in Go (this website is built with 
&lt;a href=&#34;https://gohugo.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo&lt;/a&gt;),
but never tried it before. So what better way to give Go a shake
than to write some code. Since Wordle has been popular, I thought I&amp;rsquo;d write a very simple
Wordle implementation in Go; you can check it out on 
&lt;a href=&#34;https://github.com/dmsenter89/go-wordle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.
It&amp;rsquo;s been a good way for me to get familiar with some of the
basisc of Go, such as variables and their types, functions, etc. So far I&amp;rsquo;ve been enjoying it.&lt;/p&gt;
&lt;p&gt;The Go website has a very nicely written 
&lt;a href=&#34;https://go.dev/doc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; and

&lt;a href=&#34;https://pkg.go.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;package page&lt;/a&gt;. The 
&lt;a href=&#34;https://go.dev/play/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Go Playground&lt;/a&gt;
let&amp;rsquo;s you test out Go in your browser without needing to install anything. I&amp;rsquo;ve also found
Bodner&amp;rsquo;s &amp;ldquo;
&lt;a href=&#34;https://www.oreilly.com/library/view/learning-go/9781492077206/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning Go&lt;/a&gt;&amp;rdquo; to be helpful.&lt;/p&gt;
&lt;p&gt;Go is a compiled language with a pretty picky compiler. It won&amp;rsquo;t let you compile code
with unnecessary imports and variable declarations, which help keeps your code clean.
Cross-compilation is 
&lt;a href=&#34;https://freshman.tech/snippets/go/cross-compile-go-programs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;built-in&lt;/a&gt;.
While Go is not a common language in scientific computing, the 
&lt;a href=&#34;https://www.gonum.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gonum&lt;/a&gt; package
has implemented a number of important functions and seems to be well developed. I look forward
to learning more about Go in the future.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The INDSNAME Option in SAS</title>
      <link>https://dmsenter89.github.io/post/22-04-sas-indsname-option/</link>
      <pubDate>Wed, 20 Apr 2022 11:42:02 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-04-sas-indsname-option/</guid>
      <description>&lt;p&gt;I frequently find myself needing to concatenate data sets but also wanting to be able to distinguish
which row came from which data set originally. Introductory SAS courses tend to teach the &lt;code&gt;in&lt;/code&gt; keyword,
for a workflow similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data Concat1;
set data1(in = ds0)  
    data2(in = ds1);
if ds0 then source = &amp;quot;data1&amp;quot;;
else if ds1 then source = &amp;quot;data2&amp;quot;;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With more than two input data sets, this can get unwieldy and repetitive. In an old 
&lt;a href=&#34;https://blogs.sas.com/content/iml/2015/08/03/indsname-option.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;
on Rick Wicklin&amp;rsquo;s DO LOOP, a better method is introduced - the &lt;code&gt;indsname&lt;/code&gt; option. Using this method, the above code looks much nicer:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data Concat2;
set data1-data2 indsname = source;  /* the INDSNAME= option is on the SET statement */
libref = scan(source,1,&#39;.&#39;);        /* extract the libref */
dsname = scan(source,2,&#39;.&#39;);        /* extract the data set name */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As long as your input data sets are reasonably named, you&amp;rsquo;ll now have access to all the information needed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Working with the Census API Directly from SAS</title>
      <link>https://dmsenter89.github.io/post/22-04-census-api-with-sas/</link>
      <pubDate>Wed, 13 Apr 2022 08:27:35 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-04-census-api-with-sas/</guid>
      <description>&lt;p&gt;In a previous 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;post&lt;/a&gt;, I have shown how to connect to the Census API and load data
with Python. In this post, I will do the same using SAS instead. Before we get started, two important links
from last time: a guide to the API can be found 
&lt;a href=&#34;https://www.census.gov/data/developers/guidance/api-user-guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and a list of the
available data sets can be accessed 
&lt;a href=&#34;https://api.census.gov/data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;picking-the-data&#34;&gt;Picking the Data&lt;/h2&gt;
&lt;p&gt;For this post, I&amp;rsquo;ll use the same data as last time. There we used the 2018 American Community Survey 1-Year Detailed Table
and asked for three variables - total population, household income, and median monthly cost for Alamance and Orange
counties in North Carolina (FIPS codes 37001 and 37135). The variable names are not very intuitive, so I highly recommend starting
your code with a comment section that includes a markdown-style table of the variables that you want to use. Here is
an example table for our data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;B01003_001E&lt;/td&gt;
&lt;td&gt;Total Population&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B19001_001E&lt;/td&gt;
&lt;td&gt;Household Income (12 Month)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B25105_001E&lt;/td&gt;
&lt;td&gt;Median Monthly Housing Cost&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;building-the-query&#34;&gt;Building the Query&lt;/h2&gt;
&lt;p&gt;The next step is to build the query. Like last time, the API consists of a base
URL that points us to the data set we are looking for, a list of the variables
we want to request, and a description of the geography for which we want to
request those variables. Just like last time, I&amp;rsquo;ll build the query using several
macros for flexibility purposes. Note that since &lt;code&gt;&amp;amp;&lt;/code&gt; has a special meaning in SAS,
we need to use &lt;code&gt;%str(&amp;amp;)&lt;/code&gt; when referring to it to avoid having the log clobbered with
warnings about unresolved macros.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;%let baseurl=https://api.census.gov/data/2018/acs/acs1;
%let varlist=NAME,B01003_001E,B19001_001E,B25105_001E;
%let geolist=for=county:001,135%str(&amp;amp;)in=state:37;
%let fullurl=&amp;amp;baseurl.?get=&amp;amp;varlist.%str(&amp;amp;)&amp;amp;geolist.;
%put &amp;amp;=fullurl;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Your log should now show the full query URL:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FULLURL=https://api.census.gov/data/2018/acs/acs1?get=NAME,B01003_001E,B19001_001E,B25105_001E&amp;amp;for=county:001,135&amp;amp;in=state:37
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;making-the-api-request&#34;&gt;Making the API Request&lt;/h2&gt;
&lt;p&gt;The API call is achieved with a simple PROC HTTP call using a temporary file to hold the response from the server.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;filename response temp;

proc http url=&amp;quot;&amp;amp;fullurl.&amp;quot; method=&amp;quot;GET&amp;quot; out=response;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;handling-the-json-response&#34;&gt;Handling the JSON Response&lt;/h2&gt;
&lt;p&gt;We read the JSON response by utilizing the

&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lestmtsglobal/n1jfdetszx99ban1rl4zll6tej7j.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LIBNAME JSON Engine&lt;/a&gt;
in SAS.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;libname manual JSON fileref=response;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now run &lt;code&gt;proc datasets lib=manual; quit;&lt;/code&gt;. You&amp;rsquo;ll see two data sets that were created: ALLDATA which contains the whole JSON file&amp;rsquo;s contents
in a single data set, and ROOT which is a data set of all the root-level data. The latter one is the one we want. Here&amp;rsquo;s what the
first few observations in each look like:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-first-few-observations-in-the-automatically-created-data-sets&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/auto_datasets_hu58f00ccbf67d7d2f36e2c3ae0591a33b_44045_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;First few observations in the automatically created data sets.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/auto_datasets_hu58f00ccbf67d7d2f36e2c3ae0591a33b_44045_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1073&#34; height=&#34;490&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    First few observations in the automatically created data sets.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Just like with Python, all columns are treated as character variables at first. Because of the way the Census API is structured,
the first row consists of headers, which SAS didn&amp;rsquo;t use. This is something we&amp;rsquo;ll need to fix. At this point we have two main routes we can use to fix
these issues - we can manually create a new data set from ROOT with PROC SQL and address the issues in that way, or we can take
advantage of SAS&#39; JSON map feature to define how we want to load the JSON when the LIBNAME statement is executed. There are good use cases for each,
so I will show both methods.&lt;/p&gt;
&lt;h3 id=&#34;cleaning-up-via-proc-sql&#34;&gt;Cleaning up via PROC SQL&lt;/h3&gt;
&lt;p&gt;Using PROC SQL, you can rename all the character variables you want to keep. To change from character to numeric,
you&amp;rsquo;ll use the &lt;code&gt;input&lt;/code&gt; function. You can then assign formats and labels as desired. To get rid of the first row,
you can just add a conditional &lt;code&gt;having ordinal_root ne 1&lt;/code&gt; to avoid loading that line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;proc sql;
	create table census as
	select
		element1 as Name,
		input(element2, best12.) as B01003_001E format=COMMA12.  label=&#39;Total Population&#39;,
		input(element3, best12.) as B19001_001E format=DOLLAR12. label=&#39;Household Income (12 Month)&#39;,
		input(element4, best12.) as B25105_001E format=DOLLAR12. label=&#39;Median Monthly Housing Cost&#39;,
		element5 as state,
		element6 as county
	from manual.root
	having ordinal_root ne 1;
quit;
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-result-from-the-proc-sql-method&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/CensusData_SQL_hu13384c4c5502c575dbc7b9e51c49ebcb_20401_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Result from the PROC SQL method.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/CensusData_SQL_hu13384c4c5502c575dbc7b9e51c49ebcb_20401_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1022&#34; height=&#34;124&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Result from the PROC SQL method.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;A benefit of this method is that as you fix the input table, you can already begin to work with
it thanks to the &lt;code&gt;calculated&lt;/code&gt; keyword in PROC SQL. Say we weren&amp;rsquo;t actually interested in housing cost and
household income, but instead would like to know what percent of their annual income a household spends on
housing in a given county. We could just add a new variable to our PROC SQL call and build our table like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;proc sql;
	create table census as
	select
		element1 as Name,
		input(element2, best12.) as B01003_001E format=COMMA12. label=&#39;Total Population&#39;,
		input(element3, best12.) as B19001_001E format=DOLLAR12. label=&#39;Household Income (12 Month)&#39;,
		input(element4, best12.) as B25105_001E format=DOLLAR12. label=&#39;Median Monthly Housing Cost&#39;,
		/* Now calculate what we want from the new columns: */
		12*(calculated B25105_001E)/calculated B19001_001E as HousingCostPCT format=PERCENT10.2,
		element5 as state,
		element6 as county
	from manual.root
	having ordinal_root ne 1;
quit;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;using-a-json-map&#34;&gt;Using a JSON MAP&lt;/h3&gt;
&lt;p&gt;Alternatively, we could change the way SAS reads the JSON data by editing the JSON map it uses to decode
the JSON file. The first step is to ask SAS to create a map for us to edit:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;filename automap &amp;quot;sas.map&amp;quot;;
libname autodata JSON fileref=response map=automap automap=create;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The map will look something like this:





  
  











&lt;figure id=&#34;figure-beginning-of-the-automatically-created-json-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_hu793d98e625aa154a8d9815a25890d65f_22860_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Beginning of the automatically created JSON map.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_hu793d98e625aa154a8d9815a25890d65f_22860_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;460&#34; height=&#34;426&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Beginning of the automatically created JSON map.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Note that this is also a JSON file which you can edit in a text editor. With this map, you can change the names
of the data sets and variables, assign labels and formats, and also re-format incoming data. Variables and data sets
you don&amp;rsquo;t want to read can simply be deleted from the map. Here&amp;rsquo;s the beginning of my edited file:





  
  











&lt;figure id=&#34;figure-beginning-of-my-edited-json-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_EDITED_hua840053fe8872e6ad1fe911a8f4abee6_59487_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Beginning of my edited JSON map.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_EDITED_hua840053fe8872e6ad1fe911a8f4abee6_59487_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;475&#34; height=&#34;575&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Beginning of my edited JSON map.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Since the first row of observations in the JSON are actually a header and non-numeric, I add &lt;code&gt;?&lt;/code&gt; prior to the
specified informat. This prevents errors in the log and simply replaces non-matching variables with missing values.
We can now reload the JSON using our custom map by dropping the &lt;code&gt;automap=create&lt;/code&gt; option from the LIBNAME statement:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;libname autodata JSON fileref=response map=automap;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I now print the resulting data set, the header row is still there, but replaced by missing values in numeric
columns:





  
  











&lt;figure id=&#34;figure-the-data-set-as-a-result-of-the-edited-json-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/MAP_RESULT_hua2828dd6a29f70dd2548d0bd22c856b1_25610_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;The data set as a result of the edited JSON map.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/MAP_RESULT_hua2828dd6a29f70dd2548d0bd22c856b1_25610_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1026&#34; height=&#34;165&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The data set as a result of the edited JSON map.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This means we&amp;rsquo;ll need to additionally drop this row in a separate step using a delete statement either in
a PROC SQL or DATA step.&lt;/p&gt;
&lt;p&gt;Whichever method you choose, you now can access data via an API call from SAS. Happy exploring!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Former Incarceration As A Risk Factor For COVID-19 Associated Sudden Death</title>
      <link>https://dmsenter89.github.io/publication/raghunathan-2022-incarceration/</link>
      <pubDate>Sat, 02 Apr 2022 10:00:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/publication/raghunathan-2022-incarceration/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;In the United States, former incarceration is a risk factor for chronic conditions and sudden death (SD)
due to poor healthcare continuity after release and lack of community support. During the COVID-19 pandemic,
all-cause mortality increased, and preexisting risk factors and social limitations of having an incarceration
history were exacerbated. We hypothesized that sudden deaths among the formerly incarcerated increased during
the pandemic.&lt;/p&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods&lt;/h2&gt;
&lt;p&gt;North Carolina death certificates from pre-COVID-19 (2014) and during the COVID-19 pandemic (2020) were screened
for presumed SD. Individuals were excluded based on age ($&amp;lt;18$ or $&amp;gt;65$), violent or expected deaths, and deaths
in hospitals or care facilities. Deaths were matched to the North Carolina Department of Public Safety Criminal
Offender Database for a history of incarceration. ICD-10 codes for hypertension, diabetes, chronic respiratory
disease, obesity, mental health, and substance abuse were extracted from the top four causes of death on the death
certificates.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;We found no significant difference in the prevalence of former incarceration in SD victims from 2014 to 2020.
In 2020, the odds of substance abuse among SD victims with a history of incarceration were significantly greater
compared to those without a history of incarceration (OR (95CI): 2.29 (1.91-2.73)). The odds of substance abuse among
the formerly incarcerated were greater in 2020 SD victims compared to 2014 SD victims (2.29 (1.76-2.99)).&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Sudden death among the formerly incarcerated did not significantly increase during the COVID-19 pandemic.
Increased public health funding may have limited the expected effects of COVID-19 on rates of sudden death in formerly
incarcerated individuals. However, the formerly incarcerated appear to be vulnerable to dying of sudden death associated
with substance abuse in 2020. Improving statewide transition programs targeting substance abuse counseling during
healthcare crises should improve health outcomes and reduce the rate of sudden death among the formerly incarcerated
population.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Housing Insecurity: Effects on Sudden Death and Interaction with Mental Illness</title>
      <link>https://dmsenter89.github.io/publication/vrooman-2022-housing-insecurity/</link>
      <pubDate>Sat, 02 Apr 2022 10:00:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/publication/vrooman-2022-housing-insecurity/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Housing insecurity is a powerful social determinant of health that is associated with increased all-cause mortality.
The health consequences of and contributors to housing insecurity are poorly studied, which makes preventative care
elusive for this population. In order to address these issues, we assessed the prevalence of housing insecurity
among sudden death victims and examined its relationship to sudden death, mental illness, and clinical comorbidities.&lt;/p&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods&lt;/h2&gt;
&lt;p&gt;From 1 March 2013 to 28 February 2015, out-of-hospital deaths in Wake County, North Carolina, were screened and adjudicated
to identify 399 sudden deaths among residents between the ages of 18 and 64. A control sample of 1,101 living patients were
generated by randomly sampling for age, gender, and Wake County residence from the Carolina Data Warehouse for Health.
Housing status was abstracted from clinical records from sudden death victims and controls.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Housing insecurity was documented in 28 (7.1%) of victims and 47 (4.3%) of controls (OR(95CI): 1.71(1.05-3.2)). This difference
remained significant after adjusting for hypertension, age, and diabetes. However, when additionally adjusting for depression,
anxiety, alcohol abuse, substance abuse, schizophrenia, and bipolar disorder, the increased prevalence of housing insecurity
in the sudden death group became insignificant.&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Housing insecure individuals experience a higher burden of sudden death than housing secure individuals. Mental illness
appears to confound the relationship between housing insecurity and sudden death. Treating underlying mental illness may
be a path towards specializing clinical care to optimize health outcomes for housing insecure individuals.&lt;/p&gt;
&lt;h2 id=&#34;clinical-implications&#34;&gt;Clinical Implications&lt;/h2&gt;
&lt;p&gt;Optimize health outcomes for housing insecure individuals by acknowledging that housing insecurity appears to be a risk
factor for sudden death - a relationship complicated by mental illness.  Patients with housing insecurity should be screened
for mental illness, and treatment and referral to a mental illness specialist should be considered.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multline Bash Variable Replacement</title>
      <link>https://dmsenter89.github.io/post/22-03-multiline-replacement/</link>
      <pubDate>Wed, 16 Mar 2022 09:23:12 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-03-multiline-replacement/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve recently needed to append several lines of data to a SAS data step that I collected and built
via a shell script. For search-and-replace in bash I typically use sed, but this time I ran into a problem -
sed does not like multiline shell variables. Thanks to Stack, I found a way to accomplish this task using awk instead.&lt;/p&gt;
&lt;p&gt;Suppose you have a file called data.sas with the following contents:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data person;
   infile datalines delimiter=&#39;,&#39;; 
   input name :$10. dept :$30.;
   datalines4;                      
John,Sales
Mary,Accounting
Theresa,Management
Stewart,HR
;;;;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I am using a datalines4 statement so that I get an easy to identify target for the substitution.
I want to insert a multiline shell variable before the &lt;code&gt;;;;;&lt;/code&gt; to add my data to this data step. Say I have the
following variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;NEWDATA=$(cat &amp;lt;&amp;lt;-END
Will,Compliance
Sidney,Management
END
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I try to use sed (&lt;code&gt;sed &amp;quot;s/\;\{4\}/$DATA\n;;;;/&amp;quot; data.sas&lt;/code&gt;) I will get an error about an unterminated s command.
Instead of sed, I can use awk with a variable to achieve the same goal:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;awk -v r=&amp;quot;$NEWDATA\n;;;;&amp;quot; &#39;{gsub(/;{4}/, r)}1&#39; data.sas
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The one downside is that awk does not have an in-place option like sed, and if I try to redirect to the same file
I&amp;rsquo;m reading from I get an empty file out. So you&amp;rsquo;ll have to rename the original file in your processing script to
achieve a similar effect as with the inplace option in sed.&lt;/p&gt;
&lt;p&gt;For additional approaches, see this 
&lt;a href=&#34;https://stackoverflow.com/q/10107459&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StackOverflow Question&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Easy SASPy Setup from Jupyter</title>
      <link>https://dmsenter89.github.io/post/22-03-saspy-setup/</link>
      <pubDate>Fri, 11 Mar 2022 08:30:29 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-03-saspy-setup/</guid>
      <description>&lt;p&gt;I love using SASPy, but the setup can take a minute. I used to do the setup via the CLI until I
started thinking I might be able to just do it straight from a Jupyter notebook. Having just a
couple of cells in Jupyter notebook makes for easy copy-and-paste and reduces setup time. The code
below has been tested on both Windows and Linux. As a bonus,
this also works on  
&lt;a href=&#34;https://colab.research.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can easily install packages via pip from Jupyter either by using a shell cell (&lt;code&gt;!&lt;/code&gt;) or by
using the pip magic command: &lt;code&gt;%pip install saspy&lt;/code&gt;. Once done, copy and paste the following into
a code cell and run to create the sascfg_personal.py file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import saspy, platform
from pathlib import Path

# get path for configuration file
cfgpath = saspy.__file__.replace(&#39;__init__.py&#39;,&#39;sascfg_personal.py&#39;)

# To pick the path for Java, we need to know whether we&#39;re on Windows or not
if platform.system()==&#39;Windows&#39;:
    print(&amp;quot;Windows detected.&amp;quot;)
    javapath = !where java
    authfile = Path(Path.home(),&amp;quot;_authinfo&amp;quot;)
else:
    javapath = !which java
    authfile = Path(Path.home(),&amp;quot;.authinfo&amp;quot;)
    
# the `!` command returns a string list, we want only the string
javapath = javapath[0]
print(f&amp;quot;Java is present at {javapath}&amp;quot;)

# US home Region configuration string set up via string-replacement.
# For other server addresses, see https://support.sas.com/ondemand/saspy.html
cfgtext = f&amp;quot;&amp;quot;&amp;quot;SAS_config_names=[&#39;oda&#39;]
oda = {{&#39;java&#39; : &#39;{repr(javapath).strip(&amp;quot;&#39;&amp;quot;)}&#39;,
#US Home Region
&#39;iomhost&#39; : [&#39;odaws01-usw2.oda.sas.com&#39;,&#39;odaws02-usw2.oda.sas.com&#39;,&#39;odaws03-usw2.oda.sas.com&#39;,&#39;odaws04-usw2.oda.sas.com&#39;],
&#39;iomport&#39; : 8591,
&#39;authkey&#39; : &#39;oda&#39;,
&#39;encoding&#39; : &#39;utf-8&#39;
}}&amp;quot;&amp;quot;&amp;quot;

# write the configuration file
with open(cfgpath, &#39;w&#39;) as file:
    file.write(cfgtext)
    print(f&amp;quot;Wrote configuration file to {cfgpath}&amp;quot;)
    print(f&amp;quot;Content of file: \n```\n{cfgtext}\n```&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Optionally, you can set up an authentication file with your username and password. Without this file,
you&amp;rsquo;ll be prompted for your username and password each time you log in.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# change variables to match your username and password
omr_user_id = r&amp;quot;max.mustermann@sample.com&amp;quot;
omr_user_password = r&amp;quot;K5d7#QBPw&amp;quot;
with open(authfile, &amp;quot;w&amp;quot;) as file:
    file.write(f&amp;quot;oda user {omr_user_id} password {omr_user_password}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it! You&amp;rsquo;re now ready to connect to SASPy. In my experience you don&amp;rsquo;t even need to restart
the kernel to begin work with SAS on ODA. You can try the following snippet in a new cell:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# starts a new SAS session with the `oda` configuration we set up
sas_session = saspy.SASsession(cfgname=&#39;oda&#39;)

# load a SAS data set and make a scatter plot
cars = sas_session.sasdata(&#39;cars&#39;, &#39;sashelp&#39;)
cars.scatter(x=&#39;msrp&#39;, y=&#39;horsepower&#39;)

# directly run SAS code to print a table
sas_session.submitLST(&amp;quot;proc print data=sashelp.cars(obs=6); run;&amp;quot;)

# quit SAS connection
sas_session.endsas()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Cleaning up a Date String with RegEx in SAS</title>
      <link>https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/</link>
      <pubDate>Wed, 29 Sep 2021 13:41:36 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/</guid>
      <description>&lt;p&gt;Sometimes we have to deal with manually entered data, which means there is a good chance that the data needs to be cleaned for consistency due to the
inevitable errors that creep in when typing in data, not to speak of any inconsistencies between individuals entering data.&lt;/p&gt;
&lt;p&gt;In my particular case, I was recently dealing with a data set that included
manually calculated ages that had been entered as a complete string
of the number of years, months, and days of an individual. Such a string
is not particularly useful for analysis and I wanted to have the age as
a numeric variable instead. Regular expressions can help out a lot in this
type of situation. In this post, we will look at a few representative examples
of the type of entries I&amp;rsquo;ve encountered and how to read them using RegEx in SAS.&lt;/p&gt;
&lt;h2 id=&#34;lets-look-at-the-data&#34;&gt;Let&amp;rsquo;s Look at the Data&lt;/h2&gt;





  
  











&lt;figure id=&#34;figure-what-were-starting-from&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/RAW_DS_hu83735545411e40d8d73c711ad73aa038_21697_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;What we&amp;amp;rsquo;re starting from.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/RAW_DS_hu83735545411e40d8d73c711ad73aa038_21697_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;508&#34; height=&#34;250&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    What we&amp;rsquo;re starting from.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;If we look at our sample data, we notice a few things. The data is consistently
ordered from largest to smallest, in the order of year, month, and day.
For some lines, only the year variable is available. In all cases, the string
starts with two digits.&lt;/p&gt;
&lt;p&gt;Separation of the time units is inconsistent; occasionally they are separated
by commas, sometimes by hyphens, and in some cases by spaces alone. The terms
indicating the units are spelled and capitalized inconsistently as well. There
are some abbreviations and occasionally the plural &amp;rsquo;s&#39; in days is wrapped in
parentheses.&lt;/p&gt;
&lt;p&gt;If you want to follow along, you can create the sample data with the
following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data raw;
    infile datalines delimiter = &#39;,&#39; MISSOVER DSD;
    attrib
        ID     informat=best32. format=1.
        STR_AGE informat=$500.   format=$500. label=&#39;Age String&#39;
        VAR1   informat=best32. format=1.;
    input ID STR_AGE $ VAR1;

    datalines;
    1,&amp;quot;62 Years, 5 Months, 8 Days&amp;quot;,1
    2,43 Yrs. -2 Months -4 Day(s), 2
    3,33 years * months 24 days, 1
    4,58,1
    5,&amp;quot;47 Yrs. -11 Months -27 Day(s)&amp;quot;,2
    ;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-regex-patterns&#34;&gt;The RegEx Patterns&lt;/h2&gt;
&lt;p&gt;We will use a total of three regex patterns, one for each of the time units:
year, month, day.  SAS uses Pearl regex and the function &lt;code&gt;prxparse&lt;/code&gt; to define
the regex patterns that are supposed to be searched for.&lt;/p&gt;
&lt;p&gt;For the year variable, we need to match the first two digits in our string.
Therefore, the correct call is &lt;code&gt;prxparse(&#39;/^(\d{2}).*/&#39;)&lt;/code&gt;. Note that the
&lt;code&gt;(&lt;/code&gt; and &lt;code&gt;)&lt;/code&gt; delimit the capture group.&lt;/p&gt;
&lt;p&gt;The month and day regex patterns are very similar. For the months, we want to
lazy-match the until we hit between one or two digits followed by
an &amp;rsquo;m&#39; and some number of other characters. We use the &lt;code&gt;i&lt;/code&gt; flag since
we cannot guarantee capitalization: &lt;code&gt;prxparse(&#39;/.*?(\d{1,2}).M.*/i&#39;)&lt;/code&gt;.
The day pattern is nearly identical: &lt;code&gt;prxparse(&#39;/.*?(\d{1,2}).D\D*$/i&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can extract our matches using the &lt;code&gt;prxposn&lt;/code&gt; function. We use the
&lt;code&gt;prxmatch&lt;/code&gt; function to check if we actually have a match:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;/* match into strings */
if prxmatch(year_rxid, STR_AGE)  then year_dig_str = prxposn(year_rxid,1,STR_AGE);
if prxmatch(month_rxid, STR_AGE) then month_dig_str = prxposn(month_rxid,1,STR_AGE);
if prxmatch(day_rxid, STR_AGE)   then day_dig_str = prxposn(day_rxid,1, STR_AGE);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The extracted strings can then be converted to numeric variables using
the &lt;code&gt;input&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;The last step is the calculation of the age from the three components.
Since not all three time units are specified for every row, we cannot use
the standard arithmetic of &lt;code&gt;years + months + days&lt;/code&gt;, because the missing
values would propagate. We need to use the &lt;code&gt;sum&lt;/code&gt; function instead.&lt;/p&gt;
&lt;p&gt;Putting it all together, we get the correct output:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-result&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/FIXED_DS_hu37f587f0a5cd00ab598beb8689b70b5f_34165_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;The Result&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/FIXED_DS_hu37f587f0a5cd00ab598beb8689b70b5f_34165_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;867&#34; height=&#34;251&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Result
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;complete-code&#34;&gt;Complete Code&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data fixed;
    set raw;
    
   /* define the regex patterns */
   year_rxid  = prxparse(&#39;/^(\d{2}).*/&#39;);
   month_rxid = prxparse(&#39;/.*?(\d{1,2}).M.*/i&#39;);
   day_rxid   = prxparse(&#39;/.*?(\d{1,2}).D\D*$/i&#39;);   /* match 2 digits followed by D and non-digit chars  */
  
   /* make sure we have enough space to store the extraction */
   length year_dig_str month_dig_str day_dig_str $4;
   
   /* match into strings */
   /* match into strings */
   if prxmatch(year_rxid, STR_AGE)  then year_dig_str = prxposn(year_rxid,1,STR_AGE);
   if prxmatch(month_rxid, STR_AGE) then month_dig_str = prxposn(month_rxid,1,STR_AGE);
   if prxmatch(day_rxid, STR_AGE)   then day_dig_str = prxposn(day_rxid,1, STR_AGE);
   
   /* use input to convert str -&amp;gt; numeric */
   years  = input(year_dig_str, ? 12.);
   months = input(month_dig_str, ? 12.);
   days   = input(day_dig_str, ? 12.);
   
   /* Use SUM function when calculating age
    to avoid missing values propagating  */
   age = sum(years,months/12,days/365.25);
   
   /* get rid of temporary variables */ 
   drop month_rxid month_dig_str year_rxid year_dig_str day_rxid day_dig_str;
   run;
   
proc print data=fixed; run;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>From Proc Import to a Data Step with Regex</title>
      <link>https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/</link>
      <pubDate>Thu, 29 Jul 2021 08:46:10 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/</guid>
      <description>&lt;p&gt;I find myself needing to import CSV files with a relatively large number of columns. In many cases, &lt;code&gt;proc import&lt;/code&gt; works surprisingly well in giving me what I want. But sometimes, I need to do some work while reading in the file and it would be nice to just use a data step to do so, but I don&amp;rsquo;t want to type it in by hand. That&amp;rsquo;s when a combination of &lt;code&gt;proc import&lt;/code&gt; and some regex substitution can come in handy.&lt;/p&gt;
&lt;p&gt;For the first step, run a &lt;code&gt;proc import&lt;/code&gt;, like this sample code that is provided by SAS Studio when you double click on a CSV file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;FILENAME REFFILE &#39;/path/to/file/data.csv&#39;;

PROC IMPORT DATAFILE=REFFILE
    DBMS=CSV
    OUT=WORK.IMPORT;
    GETNAMES=YES;
RUN;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you run this code, you will see that SAS generates a complete data step for you. This is what the beginning of one looks like:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-sample-log-output&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/log_hu417e5750fec5319adb043ca92305efb0_26856_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Sample log output.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/log_hu417e5750fec5319adb043ca92305efb0_26856_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;797&#34; height=&#34;461&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample log output.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;There will be be two lines for each variable, one giving the &lt;code&gt;informat&lt;/code&gt; and one giving the &lt;code&gt;format&lt;/code&gt; that SAS decided on. This will be followed by an &lt;code&gt;input&lt;/code&gt; statement. You can copy that from the log into a text editor such as VSCode, but unfortunately the line numbering of the LOG will carry over. One convenient way of fixing this is to use regex search-and-replace. Each line starts with a space followed by 1-3 digits, followed by a variable number of spaces until the next word. To capture this I use &lt;code&gt;^\s\d{1,3}\s+&lt;/code&gt; as my search term and replace with nothing. This will left align the whole data step, but this can be adjusted later.&lt;/p&gt;
&lt;p&gt;At this point the data step can be saved as a SAS file or copied back over to the file you are working within SAS Studio, but I like to do one more adjustment. I really like using the &lt;code&gt;attrib&lt;/code&gt; statement, 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lestmtsref/n1wxb7p9jkxycin16lz2db7idbnt.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see documentation&lt;/a&gt;, because it allows me to see the informat, format, and label of a variable all in one place. So I use regex to re-arrange my informat statement into the beginnings of an attribute statement. Use the search term &lt;code&gt;informat\s([^\s]+)\s([^\s]+)\s+;&lt;/code&gt; to capture each informat line and create two capture groups - the variable name as group 1 and the informat as group 2. If you use the replace code &lt;code&gt;$1 informat=$2 format=$2&lt;/code&gt;, you will see the beginnings of an attribute statement. In this replacement scheme, each informat matches each format. This is fine for date and character variables, but you may want to adjust the display format for some of your numeric variables.&lt;/p&gt;
&lt;p&gt;To clean this up, get rid of the format lines (you can search for &lt;code&gt;^format.+\n&lt;/code&gt; and replace with an empty replace to delete them), add the &lt;code&gt;attrib&lt;/code&gt; statement below the &lt;code&gt;infile&lt;/code&gt; and make sure to end the block of attributes with a semicolon, and indent your code as desired.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-sample-data-step-view&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/code_snip_hub5c78044ade6674b35a08b503d783f3d_19917_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Sample data step view.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/code_snip_hub5c78044ade6674b35a08b503d783f3d_19917_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;843&#34; height=&#34;190&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample data step view.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;And there you have it! The beginning of a nicely formatted data step that you can start to work with.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making VS Code and Python Play Nice on Windows</title>
      <link>https://dmsenter89.github.io/post/21-07-vsc-python-fix/</link>
      <pubDate>Wed, 21 Jul 2021 08:49:52 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-07-vsc-python-fix/</guid>
      <description>&lt;p&gt;One of the editors I use regularly is VS Code. I work a lot with Python, but when installing Anaconda
using default settings on a Windows machine already having VSC installed there&amp;rsquo;s a good chance you&amp;rsquo;ll run into
an issue. When attempting to run Python code straight from VSC you may get an error. This should be fixed
on some newer versions of Anaconda, but I&amp;rsquo;ve needed to do something about it often enough I feel it&amp;rsquo;s
useful to save the solution 
&lt;a href=&#34;https://stackoverflow.com/users/1072989/janh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;janh&lt;/a&gt; posted on

&lt;a href=&#34;https://stackoverflow.com/questions/54828713/working-with-anaconda-in-visual-studio-code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StackExchange&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Specifically, the issue can be fixed by manually changing VSC&amp;rsquo;s default shell from PowerShell to CMD.
Just open the command palette (CTRL+SHIFT+P), search &amp;ldquo;Terminal: Select Default Profile&amp;rdquo; and switch to
&amp;ldquo;Command Prompt&amp;rdquo;. Everything should work as expected from now on!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making INPUT and LABEL Statements with AWK</title>
      <link>https://dmsenter89.github.io/post/2021-07-awk-for-sas/</link>
      <pubDate>Tue, 06 Jul 2021 10:38:27 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/2021-07-awk-for-sas/</guid>
      <description>&lt;p&gt;I am currently working with a database provided by the North Carolina Department of Public Safety
that consists of several fixed-width files. Each of these has an associated codebook that gives the
internal variable name, a label of the variable, its data type, as well as the start column and
the length of the fields for each column. To import the data sets into SAS, I could copy and paste
part of that data into my INPUT and LABEL statements, but that gets tedious pretty fast when dealing
with dozens of lines. And since I have multiple data sets like that, I didn&amp;rsquo;t really want to do it that way.
In this post I show how a simple command-line script can be written to deal with this problem.&lt;/p&gt;
&lt;h2 id=&#34;introducing-awk&#34;&gt;Introducing AWK&lt;/h2&gt;
&lt;p&gt;Here are the first few lines of one of these files:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CMDORNUM      OFFENDER NC DOC ID NUMBER          CHAR      1       7     
CMCLBRTH      OFFENDER BIRTH DATE                DATE      8       10    
CMCLSEX       OFFENDER GENDER CODE               CHAR      18      30    
CMCLRACE      OFFENDER RACE CODE                 CHAR      48      30    
CMCLHITE      OFFENDER HEIGHT (IN INCHES)        CHAR      78      2     
CMWEIGHT      OFFENDER WEIGHT (IN LBS)           CHAR      80      3     
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the data is tabular and separated by multiple spaces. Linux programs often deal
with column data and a tool is available for manipulating column-based data on the command-line:
AWK, a program that can be used for complex text manipulation from the command-line. Some useful
tutorials on AWK in general are available at 
&lt;a href=&#34;https://www.grymoire.com/Unix/Awk.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;grymoire.com&lt;/a&gt;
and at 
&lt;a href=&#34;https://www.tutorialspoint.com/awk/index.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorialspoint&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For our purposes, we want to know about the &lt;code&gt;print&lt;/code&gt; and &lt;code&gt;printf&lt;/code&gt; commands for AWK. To illustrate
how this works, make a simple list of three lines with each term separated by a space:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cat &amp;lt;&amp;lt; EOF &amp;gt; list.txt
1 one apple pie
2 two orange cake
3 three banana shake
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To print the whole file, you&amp;rsquo;d use the print statement: &lt;code&gt;awk &#39;{print}&#39; list.txt&lt;/code&gt;. But I could do that with
&lt;code&gt;cat&lt;/code&gt;, so what&amp;rsquo;s the point? Well, what if I only want &lt;em&gt;one&lt;/em&gt; of the columns? By default, &lt;code&gt;$n&lt;/code&gt; refers to the
&lt;em&gt;n&lt;/em&gt;th column in AWK. So to print only the fruits I could write &lt;code&gt;awk &#39;{print $3}&#39; list.txt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Multiple columns can be printed by listing multiple columns separated by a comma:
&lt;code&gt;awk &#39;{print $2,$3}&#39; list.txt&lt;/code&gt;. Note that if you omit the comma the two columns get concatenated into
a single column.&lt;/p&gt;
&lt;p&gt;If additional formatting is required, we can use the &lt;code&gt;printf&lt;/code&gt; command. So to create a hyphenated
fruit and food-item column, we could use &lt;code&gt;awk &#39;{printf &amp;quot;%s-%s\n&amp;quot;, $3, $4}&#39; list.txt&lt;/code&gt;. Note that we
have to indicate the end-of line or else everything will be printed into a single line of text.&lt;/p&gt;
&lt;p&gt;Now we almost have all of the skills to create the label and input statements in SAS! Let&amp;rsquo;s create
a comma-delimited list for practice:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; list.txt
1,one,apple pie
2,two,orange cake
3,three,banana shake
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;-F&lt;/code&gt; flag is used to tell AWK to use a different column separator. So to print the
third column, we&amp;rsquo;d use &lt;code&gt;awk -F &#39;,&#39; &#39;{print $3}&#39; list.txt&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;making-the-sas-statements&#34;&gt;Making the SAS statements&lt;/h2&gt;
&lt;p&gt;Now we know everything we need to know about AWK to create code we want. First we note that
our coding file uses multiple spaces as column separators as opposed to single spaces. If
each item was a single word, this wouldn&amp;rsquo;t be a problem. Unfortunately, our second column
reads &amp;ldquo;OFFENDER NC DOC ID NUMBER&amp;rdquo; which would be split into five columns by default. So we
will need to use the column separator flag as &lt;code&gt;-F &#39;[[:space:]][[:space:]]+&#39;&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-label-statement&#34;&gt;The LABEL Statement&lt;/h3&gt;
&lt;p&gt;A SAS label has the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/v_011/lestmtsref/n1r8ub0jx34xfsn1ppcjfe0u16pc.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;general form&lt;/a&gt;
&lt;code&gt;LABEL variable-1=label-1&amp;lt;...variable-n=label-n&amp;gt;;&lt;/code&gt;, so for example&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;label score1=&amp;quot;Grade on April 1 Test&amp;quot;  
      score2=&amp;quot;Grade on May 1 Test&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is a valid label statement. In our file the variable names are given in column 1
and the appropriate labels in column 2. So an AWK script to print the appropriate
labels can be written like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;awk -F &#39;[[:space:]][[:space:]]+&#39; &#39;{printf &amp;quot;\t%s=\&amp;quot;%s\&amp;quot;\n&amp;quot;, $1, $2}&#39; FILE.DAT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what everything looks like given our code:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;label.PNG&#34; alt=&#34;Sample Code returned by AWK.&#34; title=&#34;Sample Code returned by AWK.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-input-statement&#34;&gt;The INPUT STATEMENT&lt;/h3&gt;
&lt;p&gt;The INPUT statement can be made in a similar way, it just requires some minor tweaking as
INPUT can be a bit more complex to handle a variety of data, see the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lestmtsref/n0oaql83drile0n141pdacojq97s.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;.
In our case we are dealing with a fixed-width record. The fourth column gives the starting column
of the data and the fifth gives us the width of that field. The third gives us the data type.
The majority of ours are character, so it seems easiest to just have the AWK script print each
line as though it were a character together with a SAS comment giving the name and &amp;ldquo;official&amp;rdquo; data
type. Then the few lines that need adjustment can be manually adjusted. The corresponding code would
look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;awk -F &#39;[[:space:]][[:space:]]+&#39; &#39;{printf &amp;quot;\t@%s %s $%s. /*%s - %s*/\n&amp;quot;,$4, $1, $5, $3, $2}&#39; FILE.DAT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what is returned by our code (highlighted part has been manually edited):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;input.PNG&#34; alt=&#34;Sample Code returned by AWK.&#34; title=&#34;Sample Code returned by AWK.&#34;&gt;&lt;/p&gt;
&lt;p&gt;I hope you all find this useful and that it will save you some typing!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SASPy Video Tutorial</title>
      <link>https://dmsenter89.github.io/post/2021-06-youtube-tutorial/</link>
      <pubDate>Tue, 29 Jun 2021 10:57:05 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/2021-06-youtube-tutorial/</guid>
      <description>&lt;p&gt;I have been using both SAS and Python extensively for a while now. With each having great features, it was very useful to combine my
skills in both languages by seamlessly moving between SAS and Python in
a single notebook. In the video below, fellow SAS intern Ariel Chien and I show how easy it is to connect the SAS and Python kernels using the open-source SASPy package together with SAS OnDemand for Academics.
I hope you will also find that this adds to your workflow!&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/6mcsbeKwSqM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The Jupyter notebook from the video can be viewed 
&lt;a href=&#34;https://github.com/sascommunities/sas-howto-tutorials/tree/master/sastopython&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;on GitHub&lt;/a&gt;. For installation instructions, check out the 
&lt;a href=&#34;https://github.com/sassoftware/saspy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SASPy GitHub page&lt;/a&gt;. Configuration for SASPy to connect to ODA can be found 
&lt;a href=&#34;https://support.sas.com/ondemand/saspy.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;at this support page&lt;/a&gt;. For more information on SAS OnDemand for Academics, 
&lt;a href=&#34;https://www.sas.com/en_us/software/on-demand-for-academics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Census 2020 Population Estimates Updated</title>
      <link>https://dmsenter89.github.io/post/21-06-covid-county-incidence/</link>
      <pubDate>Wed, 09 Jun 2021 16:00:34 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-06-covid-county-incidence/</guid>
      <description>&lt;p&gt;The Census Bureau has updated its population estimates for 2020 with county level data. This means any
projects that have had to rely on the 2019 estimates can now switch to the 2020 estimates.&lt;/p&gt;
&lt;p&gt;This is particularly useful for those of us who have been trying to track the development of COVID-19. The
average incidence rates are typically rescaled to new cases per 100,000 people. Previous graphs and maps I
have created used the 2019 estimates. I have now updated my code for mapping North Carolina developments to
use the 2020 estimates.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-county-level-data-for-north-carolina-using-the-nyt-covid-data-set-date-set-to-june-8-2021&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-06-covid-county-incidence/nc_avg_incidence_08jun2021_hu8396d2a41a978826522d96cfe881f35d_68326_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;County level data for North Carolina using the NYT COVID data set. Date set to June 8, 2021.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-06-covid-county-incidence/nc_avg_incidence_08jun2021_hu8396d2a41a978826522d96cfe881f35d_68326_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    County level data for North Carolina using the NYT COVID data set. Date set to June 8, 2021.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Below this post is my code for loading the necessary data using SAS.
Note that I&amp;rsquo;m using a macro called &lt;code&gt;mystate&lt;/code&gt; that can be set to the statecode abbreviation of your choice.
The conditional &lt;code&gt;County ne 0&lt;/code&gt; is in the code because the county level CSV includes both the county data as
well as the totals for each state.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;
filename popdat url &#39;https://www2.census.gov/programs-surveys/popest/datasets/2010-2020/counties/totals/co-est2020-alldata.csv&#39;;

data censusdata;
	infile POPDAT delimiter=&#39;,&#39; MISSOVER DSD lrecl=32767 firstobs=2;
	informat SUMLEV REGION DIVISION State County best32.
                         STNAME $20. CTYNAME $35. 
		CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020 best32.;
	format SUMLEV REGION DIVISION STATE best32. COUNTY 5. STNAME $20. CTYNAME $35. 
		CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020 
		COMMA12. StateCode $2.;
	input SUMLEV REGION DIVISION STATE COUNTY STNAME $ CTYNAME $
                        CENSUS2010POP ESTIMATESBASE2010 
		POPESTIMATE2010-POPESTIMATE2020;

	if (State ne 0) and (State ne 72) then
		do;
			FIPS=put(State, Z2.);
			Statecode=fipstate(FIPS);

			if Statecode eq &amp;amp;mystate and County ne 0 then
				output;
		end;
	keep STNAME CTYNAME County FIPS Statecode Popestimate2020;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The media release can be 
&lt;a href=&#34;https://www.census.gov/newsroom/press-releases/2021/2020-vintage-population-estimates.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;viewed here&lt;/a&gt;. The county-level data set can be downloaded 
&lt;a href=&#34;https://www.census.gov/programs-surveys/popest/technical-documentation/research/evaluation-estimates/2020-evaluation-estimates/2010s-counties-total.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;at this page&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Immersed Boundary Simulations and Tools for Studying Insect Flight and Other Applications</title>
      <link>https://dmsenter89.github.io/publication/phd-dissertation/</link>
      <pubDate>Sat, 15 May 2021 11:27:55 -0400</pubDate>
      <guid>https://dmsenter89.github.io/publication/phd-dissertation/</guid>
      <description>&lt;p&gt;All organisms must deal with fluid transport and interaction, whether it be internal, such as lungs moving air for the extraction of oxygen, or external, such as the expansion and contraction of a jellyfish bell for locomotion. Most organisms are highly deformable and their elastic deformations can be used to move fluid, move through fluid, and resist fluid forces. A particularly effective numerical method for biological fluid-structure interaction simulations is the immersed boundary (IB) method. An important feature of this method is that the fluid is discretized separately from the boundary interface, meaning that the two meshes do not need to conform with each other. This thesis covers the development of a new software tool for the semi-automated creation of finite difference meshes of complex 2D geometries for use with immersed boundary solvers IB2d and IBAMR, alongside two examples of locomotion - the flight of tiny insects and the metachronal paddling of brine shrimp.&lt;/p&gt;
&lt;p&gt;As mentioned, an advantage of the IB method is that complex geometries, e.g., internal or external morphology, can easily be handled without the need to generate matching grids for both the fluid and the structure. Consequently, the difficulty of modeling the structure lies often in discretizing the boundary of the complex geometry (morphology). Both commercial and open source mesh generators for finite element methods have long been established; however, the traditional immersed boundary method is based on a finite difference discretization of the structure. In chapter 2, I present a software library called MeshmerizeMe for obtaining finite difference discretizations of boundaries for direct use in the 2D immersed boundary method. This library provides tools for extracting such boundaries as discrete mesh points from digital images. Several examples of how the method can be applied are given to demonstrate the effectiveness of the software, including passing flow through the veins of insect wings, within lymphatic capillaries, and around starfish using open-source immersed boundary software.&lt;/p&gt;
&lt;p&gt;As an example of insect flight, I present a 3D model of clap and fling. Of the smallest insects filmed in flight, most if not all clap their wings together at the end of the upstroke and fling them apart at the beginning of the downstroke. This motion increases the strength of the leading edge vortices generated during the downstroke and augments the lift. At the Reynolds numbers ($Re$) relevant to flight in these insects (roughly $4&amp;lt;Re&amp;lt;40$), the drag produced during the fling is substantial, although this can be reduced through the presence of wing bristles, chordwise wing flexibility, and more complex wingbeat kinematics. It is not clear how flexibility in the spanwise direction of the wings can alter the lift and drag generated. In chapter 3, a hybrid version of the immersed boundary method with finite elements is used to simulate a 3D idealized clap and fling motion across a range of wing flexibilities. I find that spanwise flexibility, in addition to three-dimensional spanwise flow, can reduce the drag forces produced during the fling while maintaining lift, especially at lower $Re$. While the drag required to fling 2D wings apart may be more than an order of magnitude higher than the force required to translate the wings, this effect is significantly reduced in 3D. Similar to previous studies, dimensionless drag increases dramatically for Re&amp;lt;20, and only moderate increases in lift are observed. Both lift and drag decrease with increasing wing flexibility, but below some threshold, lift decreases much faster. This study highlights the importance of flexibility in both the chordwise and spanwise directions for low Re insect flight. The results also suggest that there is a large aerodynamic cost if insect wings are too flexible.&lt;/p&gt;
&lt;p&gt;My second application of locomotion pertains to a 2D model of swimming, specifically the method known as metachronal paddling. This method is used by a variety of organisms to propel themselves through a fluid. This mode of swimming is characterized by an array of appendages that beat out of phase, such as the swimmerets used by long-tailed crustaceans like crayfish and lobster. This form of locomotion is typically observed over a range of Reynolds numbers greater than 1 where the flow is dominated by inertia. The majority of experimental, modeling, and numerical work on metachronal paddling has been conducted on the higher Reynolds number regime (order 100). In this chapter, a simplified numerical model of one of the smaller metachronal swimmers, the brine shrimp, is constructed. Brine shrimp are particularly interesting since they swim at Reynolds numbers on the order of 10 and sprout additional paddling appendages as they grow. The immersed boundary method is used to numerically solve the fluid-structure interaction problem of multiple rigid paddles undergoing cycles of power and return strokes with a constant phase difference and spacing that are based on brine shrimp parameters. Using a phase difference of 8%, the volumetric flux and efficiency per paddle as a function of the Reynolds number and the spacing between legs is quantified. I find that the time to reach periodic steady state for adult brine shrimp is large (approx. 150 stroke cycles) and decreases with decreasing Reynolds number. Both efficiency and average flux increase with Reynolds number. In terms of leg spacing, the average flux decreases with increased spacing while the efficiency is maximized for intermediate leg spacing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dissertation Defense</title>
      <link>https://dmsenter89.github.io/talk/dissertation-defense/</link>
      <pubDate>Wed, 07 Apr 2021 10:00:00 -0500</pubDate>
      <guid>https://dmsenter89.github.io/talk/dissertation-defense/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Metachronal Paddling</title>
      <link>https://dmsenter89.github.io/project/metachronal-paddling/</link>
      <pubDate>Mon, 22 Mar 2021 13:00:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/project/metachronal-paddling/</guid>
      <description>&lt;p&gt;Metachronal paddling can be described as the sequential oscillation of appendages whereby adjacent paddles maintain a nearly constant phase difference.
This mechanism is widely used in nature, both in locomotion such as swimming in crustaceans and in fluid transport such as the clearance of mucus
in the mammalian lung. Aside from the wide range of applications, metachronal paddling can be observed across a wide range of
Reynolds number regimes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/c/cc/Artemia_monica.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I work on simulating the hydrodynamics of metachronal paddling in brine shrimp (&lt;em&gt;Artemia&lt;/em&gt;). Brine shrimp are small aquatic
crustaceans who lay dormat eggs and are widely used in aquaculture. Their thoracopods are spaced closely together and
beat with a small phase difference. We are interested in the hydrodynamics and efficiency of this swimming pattern, which has not
previously been rigorously explored.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Git with SAS Studio</title>
      <link>https://dmsenter89.github.io/post/git-with-sas-studio/</link>
      <pubDate>Mon, 11 Jan 2021 14:42:50 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/git-with-sas-studio/</guid>
      <description>&lt;p&gt;Git is a widely used version control system that allows users to track their software
development in both public and private repositories. It is also increasingly used to store
data in text formats, see for example the 
&lt;a href=&#34;https://github.com/nytimes/covid-19-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New York Times COVID-19 data set&lt;/a&gt;.
This post will briefly demonstrate how to clone and pull updates from a GitHub repository
using the git functions that are built into SAS Studio.&lt;/p&gt;
&lt;p&gt;Git functionality has been built into SAS Studio for a little while, so there are actually
two slightly different iterations of the git functions. The examples in this post will use the versions
compatible with SAS Studio 3.8, which is the current version available at SAS OnDemand for Academics.
All git functions use the same prefix. In older versions such as SAS Studio 3.8 the prefix is &lt;code&gt;gitfn_&lt;/code&gt;,
which is followed by a git command such as &amp;ldquo;clone&amp;rdquo; or &amp;ldquo;pull&amp;rdquo;. In SAS Studio 5, the prefix has been
simplified to just &lt;code&gt;git_&lt;/code&gt;. Most git functions have the same name between the&lt;br&gt;
two versions, so that the only difference is the prefix. A complete table of the old and new
versions of the git functions is available 
&lt;a href=&#34;https://go.documentation.sas.com/?cdcId=pgmsascdc&amp;amp;cdcVersion=9.4_3.5&amp;amp;docsetId=lefunctionsref&amp;amp;docsetTarget=n1mlc3f9w9zh9fn13qswiq6hrta0.htm&amp;amp;locale=en#p0evl64wd2dljrn1l43t739qtwba&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in the documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We use the git functions by calling them in an otherwise empty DATA step. In other words, we use the
format&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
    /* use your git functions here */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;cloning-a-repo&#34;&gt;Cloning a Repo&lt;/h2&gt;
&lt;p&gt;To clone a repo from github we use &lt;code&gt;gitfn_clone&lt;/code&gt;. It takes two arguments -
the URL of the repository of interest and the path to an &lt;em&gt;empty&lt;/em&gt; folder. You can
have SAS create the folder for you by using &lt;code&gt;OPTIONS DLCREATEDIR&lt;/code&gt;. The basic
syntax for the clone is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
    rc = gitfn_clone (
     &amp;quot;&amp;amp;repoURL.&amp;quot;,    /* URL to repo */
     &amp;quot;&amp;amp;targetDIR.&amp;quot;); /* folder to put repo in */
    put rc=;         /* equals 0 if successful */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It doesn&amp;rsquo;t matter if the URL you use ends in &amp;ldquo;.git&amp;rdquo; or not. In other words, the
following two macros would both work the same:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;%LET repoURL=https://github.com/nytimes/covid-19-data;
/* works the same as */
%LET repoURL=https://github.com/nytimes/covid-19-data.git;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use password based authentication to pull in private repositories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
    rc = gitfn_clone (
     &amp;quot;&amp;amp;repoURL.&amp;quot;,   
     &amp;quot;&amp;amp;targetDIR.&amp;quot;,
     &amp;quot;&amp;amp;githubUSER.&amp;quot;,   /* your GitHub username */
     &amp;quot;&amp;amp;githubPASSW.&amp;quot;); /* your GitHub password */
    put rc=;         /* equals 0 if successful */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; GitHub is &lt;em&gt;deprecating&lt;/em&gt; 
&lt;a href=&#34;https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;password-based authentication&lt;/a&gt;; you will need to switch to OAuth authentication or SSH keys
if you are not already using them. To access a repository using an SSH key, use the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;                             
 rc = gitfn_clone(
  &amp;quot;&amp;amp;repoURL.&amp;quot;,
  &amp;quot;&amp;amp;targetDIR.&amp;quot;,
  &amp;quot;&amp;amp;sshUSER.&amp;quot;,
  &amp;quot;&amp;amp;sshPASSW.&amp;quot;,
  &amp;quot;&amp;amp;sshPUBkey.&amp;quot;,
  &amp;quot;&amp;amp;sshPRIVkey.&amp;quot;);
 put rc=;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;pull-ing-in-updates&#34;&gt;Pull-ing in Updates&lt;/h2&gt;
&lt;p&gt;It is just as easy to pull in updates to a local repository by using
&lt;code&gt;gitfn_pull(&amp;quot;&amp;amp;repoDIR.&amp;quot;)&lt;/code&gt;. This also works with SSH keys for private
repositories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
 rc = gitfn_pull(
  &amp;quot;&amp;amp;repoDIR.&amp;quot;,
  &amp;quot;&amp;amp;sshUSER.&amp;quot;,
  &amp;quot;&amp;amp;sshPASSW.&amp;quot;,
  &amp;quot;&amp;amp;sshPUBkey.&amp;quot;,
  &amp;quot;&amp;amp;sshPRIVkey.&amp;quot;);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;other-functions&#34;&gt;Other Functions&lt;/h2&gt;
&lt;p&gt;SAS also offers other built-in functions, such as &lt;code&gt;_diff&lt;/code&gt;, &lt;code&gt;_status&lt;/code&gt;, &lt;code&gt;_push&lt;/code&gt;,
&lt;code&gt;_commit&lt;/code&gt;, and others. For a complete list, see the SAS documentation 
&lt;a href=&#34;https://go.documentation.sas.com/?cdcId=pgmsascdc&amp;amp;cdcVersion=9.4_3.5&amp;amp;docsetId=lefunctionsref&amp;amp;docsetTarget=n1mlc3f9w9zh9fn13qswiq6hrta0.htm&amp;amp;locale=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>North Carolina Housing Data</title>
      <link>https://dmsenter89.github.io/post/20-11-north-carolina-housing/</link>
      <pubDate>Fri, 06 Nov 2020 10:10:01 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/20-11-north-carolina-housing/</guid>
      <description>&lt;p&gt;A popular beginners machine learning problem is the prediction of housing prices. A frequently used data set for this purpose uses housing prices in California along some additional  gathered through the 1990 Census. One such data set is available 
&lt;a href=&#34;https://www.kaggle.com/camnugent/california-housing-prices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; at Kaggle. Unfortunately, that data set is rather old. And I live in North Carolina, not California! So I figured I might as well create a new housing data set, but this time with more up-to-date information and using North Carolina as the state to be analyzed. One thing that may be interesting about North Carlina as compared to California is the position of major populations centers. In California, major population centers are near the beach, while major population centers in North Carolina are in the interior of the state. Both large citites and proximity to the beach tend to correlate with higher housing prices. In California, unlike in North Carolina, both of these go together.&lt;/p&gt;
&lt;p&gt;This post will describe the Kaggle data set with California housing prices and then walk you through how the relevant data can be acquired from the Census Bureau. I&amp;rsquo;ll also show how to clean the data. For those who just want to explore the complete data set, I have made it available for download &lt;a href=&#34;https://dmsenter89.github.io/files/NC_Housing_Prices_2018.csv&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#the-source-data-set&#34;&gt;The Source Data Set&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acquiring-the-census-data-set&#34;&gt;Acquiring the Census Data Set&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#census-variables&#34;&gt;Census Variables&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#geography-considerations&#34;&gt;Geography Considerations&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acquiring-location-data&#34;&gt;Acquiring Location Data&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-merge-with-geoid-matching&#34;&gt;Data Merge with GEOID Matching&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-cleaning&#34;&gt;Data Cleaning&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;the-source-data-set&#34;&gt;The Source Data Set&lt;/h2&gt;
&lt;p&gt;The geographic unit of the Kaggle data set is the Census block group, which means we will have several thousand data points for our analysis. For a good big-picture overview of Census geography divisions, see this 
&lt;a href=&#34;https://pitt.libguides.com/uscensus/understandinggeography&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt; from the University of Pittsburgh library. The data set&amp;rsquo;s ten columns contain geographic, housing, and Census information that can be broken down as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;geographic information
&lt;ul&gt;
&lt;li&gt;longitude&lt;/li&gt;
&lt;li&gt;latitude&lt;/li&gt;
&lt;li&gt;ocean proximity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;housing information
&lt;ul&gt;
&lt;li&gt;median age of homes&lt;/li&gt;
&lt;li&gt;median value of homes&lt;/li&gt;
&lt;li&gt;total number of rooms in area&lt;/li&gt;
&lt;li&gt;total number of bedrooms in the area&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Census information
&lt;ul&gt;
&lt;li&gt;population&lt;/li&gt;
&lt;li&gt;number of households&lt;/li&gt;
&lt;li&gt;median income&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most of these exist directly in the Census API data that we have 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;covered previously&lt;/a&gt;. The ocean proximity variable is a categorical giving approximate distance from the beach. My data set will not include this last categorical variable.&lt;/p&gt;
&lt;h2 id=&#34;acquiring-the-census-data-set&#34;&gt;Acquiring the Census Data Set&lt;/h2&gt;
&lt;h3 id=&#34;census-variables&#34;&gt;Census Variables&lt;/h3&gt;
&lt;p&gt;The first, and most time consuming aspect, is to figure out where the data we want is located. We know that the US has a decennial census, so accurate information is available every ten years at every level of geography that the Census Bureau tracks. Since it is currently a census year 2020 and the newest information hasn&amp;rsquo;t been tabulated yet, that means the last census count that is available is from 2010. While this is 20 years more current than the California set from 1990, it still seems a bit outdated. Luckily, since the introduction of the American Community Survey (ACS) we have annually updated information available - but not for every level of geography. Only the 5-year ACS average gives us census block-level information for the whole state, making it comparable to the Kaggle data set. The most recent of these is the 2018.&lt;/p&gt;
&lt;p&gt;I start by creating a data dictionary from the 
&lt;a href=&#34;https://api.census.gov/data/2018/acs/acs5/groups.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;groups&lt;/a&gt; and 
&lt;a href=&#34;https://api.census.gov/data/2018/acs/acs5/variables.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;variables&lt;/a&gt; pages of the &amp;ldquo;American Community Survey: 1-Year Estimates: Detailed Tables 5-Year&amp;rdquo; data set. Note that median home age is not directly available. Instead, we will use the median year structures were built to calculate the median home age. Our data dictionary also does not include any data for the longitude and latitude of each row. We will get that data separately.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dictionary = {
    &#39;B01001_001E&#39; : &amp;quot;population&amp;quot;,
    &#39;B11001_001E&#39; : &amp;quot;households&amp;quot;,
    &#39;B19013_001E&#39; : &amp;quot;median_income&amp;quot;, 
    &#39;B25077_001E&#39; : &amp;quot;median_house_value&amp;quot;,
    &#39;B25035_001E&#39; : &amp;quot;median_year_structure_built&amp;quot;,
    &#39;B25041_001E&#39; : &amp;quot;total_bedrooms&amp;quot;,
    &#39;B25017_001E&#39; : &amp;quot;total_rooms&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;geography-considerations&#34;&gt;Geography Considerations&lt;/h3&gt;
&lt;p&gt;The next step is figuring out exactly what level of geography we want. Our data set goes down to the Census block level at its most granular. Unfortunately, the Census API won&amp;rsquo;t let us pull the data for all the Census blocks in a state at once. Census tracts on the other hand can be acquired in one go. If we were to shortcut and use only tract data, this would be a pretty quick API call build:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;primary_geo = &amp;quot;tract:*&amp;quot;
secondary_geo = &amp;quot;state:37&amp;quot;
query = base_URL + &amp;quot;?get=&amp;quot; + &amp;quot;,&amp;quot;.join(data_dictionary.keys()) + f&amp;quot;&amp;amp;for={primary_geo}&amp;amp;in={secondary_geo}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But let&amp;rsquo;s try and do it for the Census blocks instead. This will require us to build a sequence of API calls that loops over a larger geographic area, say the different counties in the state, and pull in the respective census block data for that geographic unit. While the FIPS codes for the state counties are sorted alphabetically, they are not contiguous. A full listing of North Carolina county FIPS codes is availalbe 
&lt;a href=&#34;https://www.lib.ncsu.edu/gis/countyfips&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;from NCSU here&lt;/a&gt;. It appears to be that the county FIPS codes are three digits long, starting at &lt;code&gt;001&lt;/code&gt; and go up to &lt;code&gt;199&lt;/code&gt; in increments of 2, meaning only odd numbers are in the county set. So it looks like we will be using &lt;code&gt;range(1,200,2)&lt;/code&gt; with zero-padding to create the list of county FIPS codes. So we could use a loop similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vars_requested = &amp;quot;,&amp;quot;.join(data_dictionary.keys())

for i in range(1,200,2):
    geo_request = f&amp;quot;for=block%20group:*&amp;amp;in=state:37%20county:{i:03}&amp;quot;
    query = base_URL + f&amp;quot;?get={vars_requested}&amp;amp;{geo_request}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While practicing to write the appropriate API call, you may find it useful to give it frequent, quick tests using curl. If you are using Jupyter or IPython, you can use &lt;code&gt;!curl &amp;quot;{query}&amp;quot;&lt;/code&gt; to test your API query. Don&amp;rsquo;t forget the quotation marks, since the ampersand has special meaning in the shell. It may be helpful to test the output of your call at the county or city level with that reported on the 
&lt;a href=&#34;https://www.census.gov/quickfacts/fact/table/US/PST045219&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Census Quickfacts page&lt;/a&gt;, if your variable is listed there. This can help make sure you are pulling the data you actually want.&lt;/p&gt;
&lt;p&gt;Now that we have figured out the loop necessary for creation of the API calls, we can put everything together and create a list of Pandas DataFrames which we then concatenate to create our master list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import requests

# create the base-URL
host_name = &amp;quot;https://api.census.gov/data&amp;quot;
year = &amp;quot;2018&amp;quot;
dataset_name = &amp;quot;acs/acs5&amp;quot;
base_URL = f&amp;quot;{host_name}/{year}/{dataset_name}&amp;quot;

# build the api calls as a list
query_vars = base_URL + &amp;quot;?get=&amp;quot; + &amp;quot;,&amp;quot;.join(list(data_dictionary.keys()) + [&amp;quot;NAME&amp;quot;,&amp;quot;GEO_ID&amp;quot;])
api_calls = [query_vars + f&amp;quot;&amp;amp;for=block%20group:*&amp;amp;in=state:37%20county:{i:03}&amp;quot; for i in range(1,200,2) ]

# running the API calls will take a moment
rjson_list = [requests.get(call).json() for call in api_calls]

# create the data frame by concatenation
df_list = [pd.DataFrame(data[1:], columns=data[0]) for data in rjson_list]
df = pd.concat(df_list, ignore_index=True)

# save the raw output to disk
df.to_csv(&amp;quot;raw_census.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we have the data set! We do still have to address the issue of our values all being imported as strings as mentioned in my 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;Census API post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;acquiring-location-data&#34;&gt;Acquiring Location Data&lt;/h2&gt;
&lt;p&gt;As mentioned above, we are still missing information regarding the latitude and longitude of the different block groups. The Census Bureau makes a lot of geographically coded data available on its 
&lt;a href=&#34;https://tigerweb.geo.census.gov/tigerwebmain/TIGERweb_main.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TIGERweb&lt;/a&gt; page. You can interact with it both using a REST API and its web-interface. A page with map information exists 
&lt;a href=&#34;https://tigerweb.geo.census.gov/arcgis/rest/services/Generalized_ACS2018/Tracts_Blocks/MapServer/4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Dealing with shapefiles and the TIGERweb API can get a little complicated. Luckily, I know someone with expertise in GIS and shapefiles so we will be using a CSV file of the geographic data we need courtesy of 
&lt;a href=&#34;https://www.linkedin.com/in/summer-faircloth-652797137&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Summer Faircloth&lt;/a&gt;, a GIS intern at the North Carolina Department of Transportation. She downloaded the TIGER/Line Shapefiles for the 20189 ACS 
&lt;a href=&#34;https://catalog.data.gov/dataset/tiger-line-shapefile-2018-state-north-carolina-current-block-group-state-based&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Block Groups&lt;/a&gt; and 
&lt;a href=&#34;https://catalog.data.gov/dataset/tiger-line-shapefile-2018-state-north-carolina-current-census-tract-state-based&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Census Tracts&lt;/a&gt; and joined the data sets in ArcMap, from where she exported our CSV file, which is now &lt;a href=&#34;https://dmsenter89.github.io/files/BlockGroup_Tract2018.csv&#34; target=&#34;_blank&#34;&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t need all of the columns in the CSV file, so we will limit the import to the parts we need with the &lt;code&gt;usecols&lt;/code&gt; keyword.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&amp;quot;raw_census.csv&amp;quot;, dtype={})
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;shapedata = pd.read_csv(&amp;quot;BlockGroup_Tract2018.csv&amp;quot;, 
                        dtype={&amp;quot;GEOID&amp;quot;: str},
                        usecols=[&#39;GEOID&#39;,&#39;NAMELSAD&#39;,&#39;INTPTLAT&#39;,&#39;INTPTLON&#39;,&#39;NAMELSAD_1&#39;] )

shapedata = shapedata.rename(columns={&#39;INTPTLAT&#39; : &#39;latitude&#39;, &#39;INTPTLON&#39; : &#39;longitude&#39; })
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-merge-with-geoid-matching&#34;&gt;Data Merge with GEOID Matching&lt;/h2&gt;
&lt;p&gt;At this stage we have two data frames - the first consists of all the Census information sans the geographic coordinates of the block groups, and a second data set containing the block groups&#39; location. Both data sets contain a GEOID column that can be used for merging. The GEOID returned by the Census API includes additional information to the regular FIPS code based GEOID used in the TIGERweb system. For example, &amp;ldquo;1500000US370010204005&amp;rdquo; in the census data set is actually GEOID &amp;ldquo;370010204005&amp;rdquo; for purposes of the TIGERweb data set. We&amp;rsquo;ll use a string split to make our GEO_ID variable from the Census API compatible with the FIPS code based GEOID from the TIGERweb service.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;GEO_ID&amp;quot;] = df[&amp;quot;GEO_ID&amp;quot;].str.split(&#39;US&#39;).str[1]

df = df.merge(shapedata, left_on=&#39;GEO_ID&#39;, right_on=&amp;quot;GEOID&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-cleaning&#34;&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;Now that our data set has been assembled, we can work on cleaning up the merged data set. We have the following tasks left:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;convert column data types to numeric&lt;/li&gt;
&lt;li&gt;drop unnecessary columns&lt;/li&gt;
&lt;li&gt;rename columns&lt;/li&gt;
&lt;li&gt;handle missing values&lt;/li&gt;
&lt;li&gt;calculate median age of homes&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for col in data_dictionary.keys():
    if col not in [&amp;quot;NAME&amp;quot;, &amp;quot;GEO_ID&amp;quot;]:
        df[col] = pd.to_numeric(df[col])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To indicate missing values, the Census API returns a value of &amp;ldquo;-666666666&amp;rdquo; in numeric columns. As all of our variables - except for longitude - ought to be positive, we can use the &lt;code&gt;mask&lt;/code&gt; function to convert all negative values to missing. We&amp;rsquo;ll start by filtering out the string columns that are no longer necessary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# filter down to our numerical columns
keeps = list(data_dictionary.keys()) +[&amp;quot;latitude&amp;quot;, &amp;quot;longitude&amp;quot;]
df = df.filter(items=keeps)

# replace vals &amp;lt; 0 with missing
k = df.loc[:, df.columns != &#39;longitude&#39;]
k = k.mask(k &amp;lt; 0)
df.loc[:, df.columns != &#39;longitude&#39;] = k
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the missing values have been handled, we can go ahead and calculate our median home age.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.rename(columns=data_dictionary, inplace=True)
df[&amp;quot;housing_median_age&amp;quot;] = 2018 - df[&amp;quot;median_year_structure_built&amp;quot;]
df.drop(columns=&amp;quot;median_year_structure_built&amp;quot;, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we&amp;rsquo;re done! We will save our output data set to disk for future analysis in a different post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.to_csv(&amp;quot;NC_Housing_Prices_2018.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Teacher Salaries</title>
      <link>https://dmsenter89.github.io/post/20-10-tabula/</link>
      <pubDate>Thu, 29 Oct 2020 22:39:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/20-10-tabula/</guid>
      <description>&lt;p&gt;After reading a news article about teacher pay in the US, I was curious and wanted to look into the source data myself. Unfortunately, the source that was mentioned was a publication by the 
&lt;a href=&#34;https://www.nea.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Education Association (NEA)&lt;/a&gt; which had the data as tables embedded inside a PDF report. As those who know me can attest, I don&amp;rsquo;t like hand-copying data. It is slow and error-prone. Instead, I decided to use the tabula package to extract the information from the PDFs directly into a Pandas dataframe. In this post, I will show you how to extract the data and how to clean it up for analysis.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#the-data-source&#34;&gt;The Data Source&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#loading-the-data&#34;&gt;Loading the Data&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#cleaning-the-data&#34;&gt;Cleaning the Data&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#numeric-conversion&#34;&gt;Numeric Conversion&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#table-b-6&#34;&gt;Table B-6&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;the-data-source&#34;&gt;The Data Source&lt;/h2&gt;
&lt;p&gt;Several years worth of data are available in PDF form on the 
&lt;a href=&#34;https://www.nea.org/research-publications&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NEA website&lt;/a&gt;. Reading through the technical notes, they highlight that they did not collect all of their own salary information. Some states&#39; information is calculated from the American Community Survey (ACS) done by the Census Bureau - a great resource whose API I have covered in a 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;different post&lt;/a&gt;. Each report includes accurate data for the previous school year, as well as estimates for the current school year. As of this post, the newest report is the 2020 report which includes data for the the 2018-2019 school year, as well as estimates of the 2019-2020 school year.&lt;/p&gt;
&lt;p&gt;The 2020 report has the desired teacher salary information in two separate locations. One is in table B-6 on page 26 of the PDF, which shows a ranking of the different states&#39; average salary in addition to the average salary:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table_B-6.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;A second location is in table E-7 on page 46, which gives salary data for the completed school year as well as different states&#39; estimates for the 2019-2020 school year:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table_E-7.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Note that table E-7 lacks the star-annotation marking NEA estimated values. This, and the lack of the ranking column, makes Table E-7 easier to parse. In the main example below, this will be the source of the five years of data. I will however also show how to parse table B-6 at the end of this post for completion.&lt;/p&gt;
&lt;h2 id=&#34;loading-the-data&#34;&gt;Loading the Data&lt;/h2&gt;
&lt;p&gt;As of October 2020, the NEA site has five years worth of reports online. Unfortunately, these are not labeled consistently for all five years. Similarly the page numbers differ for each report. Prior to the 2018 report, inconsistent formats were used for the tables which require previous years to be parsed separately from the newer tables. For this reason, I&amp;rsquo;ll make a dictionary for the 2018-2020 reports only, which will simplify the example below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;report = {
    &#39;2020&#39; : {
        &#39;url&#39;  : &amp;quot;https://www.nea.org/sites/default/files/2020-10/2020%20Rankings%20and%20Estimates%20Report.pdf&amp;quot;,
        &#39;page&#39; : 46,
    },
    &#39;2019&#39; : {
        &#39;url&#39;  : &amp;quot;https://www.nea.org/sites/default/files/2020-06/2019%20Rankings%20and%20Estimates%20Report.pdf&amp;quot;,
        &#39;page&#39; : 49,
    },
    &#39;2018&#39; : {
        &#39;url&#39;  : &amp;quot;https://www.nea.org/sites/default/files/2020-07/180413-Rankings_And_Estimates_Report_2018.pdf&amp;quot;,
        &#39;page&#39; : 51,
    },
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now use dictionary comprehension to fill in a dictionary with all the source tables of interest. We will be using the tabula package to extract data from the PDFs. If you don&amp;rsquo;t have it installed, you can use &lt;code&gt;pip install tabula-py&lt;/code&gt; to get a copy. The method that reads in a PDF is aptly called &lt;code&gt;read_pdf&lt;/code&gt;. Its first argument is a file path to the PDF. Since we want to use a URL, we will use the keyword argument &lt;code&gt;stream=True&lt;/code&gt; and then name the specific page in each PDF that contains the information we are after. By default, &lt;code&gt;read_pdf&lt;/code&gt; returns a list of dataframes, so we just save the first element from the list, which is the report we are interested in.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; if you are using WSL, depending on your settings, you may get the error &lt;code&gt;Exception in thread &amp;quot;main&amp;quot; java.awt.AWTError: Can&#39;t connect to X11 window server using &#39;XXX.XXX.XXX.XXX:0&#39; as the value of the DISPLAY variable.&lt;/code&gt; error when running &lt;code&gt;read_pdf&lt;/code&gt;. This is fixed by having an X11 server running.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tabula
import pandas as pd

source_df = {year : tabula.read_pdf(report[year][&#39;url&#39;], stream=True, pages=report[year][&#39;page&#39;])[0] 
             for year in report.keys()}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it in principle. How cool is that! Of course, we still need to clean our data a little bit.&lt;/p&gt;
&lt;h3 id=&#34;cleaning-the-data&#34;&gt;Cleaning the Data&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at the first and last few entries of the 2020 report:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.concat([source_df[&#39;2020&#39;].head(), 
           source_df[&#39;2020&#39;].tail()])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;2018-19&lt;/th&gt;
      &lt;th&gt;2019-20&lt;/th&gt;
      &lt;th&gt;From 2018-19 to 2019-20&lt;/th&gt;
      &lt;th&gt;From 2010-11 to 2019-20 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;State&lt;/td&gt;
      &lt;td&gt;Salary($)&lt;/td&gt;
      &lt;td&gt;Salary($)&lt;/td&gt;
      &lt;td&gt;Change(%)&lt;/td&gt;
      &lt;td&gt;Current Dollar Constant Dollar&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
      &lt;td&gt;54,095&lt;/td&gt;
      &lt;td&gt;4.01&lt;/td&gt;
      &lt;td&gt;13.16 -2.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
      &lt;td&gt;70,877&lt;/td&gt;
      &lt;td&gt;0.85&lt;/td&gt;
      &lt;td&gt;15.36 -0.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
      &lt;td&gt;50,381&lt;/td&gt;
      &lt;td&gt;0.06&lt;/td&gt;
      &lt;td&gt;8.03 -7.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
      &lt;td&gt;49,822&lt;/td&gt;
      &lt;td&gt;0.78&lt;/td&gt;
      &lt;td&gt;8.31 -6.75&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;48&lt;/th&gt;
      &lt;td&gt;Washington&lt;/td&gt;
      &lt;td&gt;73,049&lt;/td&gt;
      &lt;td&gt;72,965&lt;/td&gt;
      &lt;td&gt;-0.11&lt;/td&gt;
      &lt;td&gt;37.86 18.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;49&lt;/th&gt;
      &lt;td&gt;West Virginia&lt;/td&gt;
      &lt;td&gt;47,681&lt;/td&gt;
      &lt;td&gt;50,238&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;13.51 -2.28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50&lt;/th&gt;
      &lt;td&gt;Wisconsin&lt;/td&gt;
      &lt;td&gt;58,277&lt;/td&gt;
      &lt;td&gt;59,176&lt;/td&gt;
      &lt;td&gt;1.54&lt;/td&gt;
      &lt;td&gt;9.17 -6.02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;51&lt;/th&gt;
      &lt;td&gt;Wyoming&lt;/td&gt;
      &lt;td&gt;58,861&lt;/td&gt;
      &lt;td&gt;59,014&lt;/td&gt;
      &lt;td&gt;0.26&lt;/td&gt;
      &lt;td&gt;5.19 -9.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;52&lt;/th&gt;
      &lt;td&gt;United States&lt;/td&gt;
      &lt;td&gt;62,304&lt;/td&gt;
      &lt;td&gt;63,645&lt;/td&gt;
      &lt;td&gt;2.15&lt;/td&gt;
      &lt;td&gt;14.14 -1.73&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We see that each column is treated as a string object (which you can confirm by running &lt;code&gt;source_df[&#39;2020&#39;].dtypes&lt;/code&gt;) and that the first row of data is actually at index 1 due to the fact that the PDF report used a two-row header. This means we can safely drop the first row of every dataframe. We can also drop the last row of every dataframe since that just contains summary data of the US as a whole, which we can easily regenerate as necessary. So row indices 0 and 52 can go for all of our data sets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for df in source_df.values():
    df.drop([0, 52], inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next up I&amp;rsquo;d like to fix the column names. The fist column is clearly the name of the state (except in the case of Washington D.C.), while the next two columns give the years for which the salary information is given. Let&amp;rsquo;s rename the second and third columns according to the pattern &lt;code&gt;Salary %YYYY-YY&lt;/code&gt; using Python&amp;rsquo;s f-string syntax.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for df in source_df.values():
    df.rename(columns={
        df.columns[0] : &amp;quot;State&amp;quot;,
        df.columns[1] : f&amp;quot;Salary {str(df.columns[1])}&amp;quot;,
        df.columns[2] : f&amp;quot;Salary {str(df.columns[2])}&amp;quot;,
    }, 
              inplace=True)
    
source_df[&amp;quot;2020&amp;quot;].head()  # show the result of our edits so far
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Salary 2018-19&lt;/th&gt;
      &lt;th&gt;Salary 2019-20&lt;/th&gt;
      &lt;th&gt;From 2018-19 to 2019-20&lt;/th&gt;
      &lt;th&gt;From 2010-11 to 2019-20 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
      &lt;td&gt;54,095&lt;/td&gt;
      &lt;td&gt;4.01&lt;/td&gt;
      &lt;td&gt;13.16 -2.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
      &lt;td&gt;70,877&lt;/td&gt;
      &lt;td&gt;0.85&lt;/td&gt;
      &lt;td&gt;15.36 -0.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
      &lt;td&gt;50,381&lt;/td&gt;
      &lt;td&gt;0.06&lt;/td&gt;
      &lt;td&gt;8.03 -7.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
      &lt;td&gt;49,822&lt;/td&gt;
      &lt;td&gt;0.78&lt;/td&gt;
      &lt;td&gt;8.31 -6.75&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;83,059&lt;/td&gt;
      &lt;td&gt;84,659&lt;/td&gt;
      &lt;td&gt;1.93&lt;/td&gt;
      &lt;td&gt;24.74 7.39&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Looks like we&amp;rsquo;re almost done! Let&amp;rsquo;s drop the unnecessary columns and check our remaining column names:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for year, df in source_df.items():
    df.drop(df.columns[3:], axis=1, inplace=True)
    print(f&amp;quot;{year}:\t{df.columns}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020:	Index([&#39;State&#39;, &#39;Salary 2018-19&#39;, &#39;Salary 2019-20&#39;], dtype=&#39;object&#39;)
2019:	Index([&#39;State&#39;, &#39;Salary 2017-18&#39;, &#39;Salary 2018-19&#39;], dtype=&#39;object&#39;)
2018:	Index([&#39;State&#39;, &#39;Salary 2017&#39;, &#39;Salary 2018&#39;], dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the column naming scheme in 2018 was different than in the previous reports. To make them all compatible for our merge, we&amp;rsquo;re going to have to do some more editing. Based on the other reports, it appears as though the 2018 report used the calendar year of the &lt;em&gt;end&lt;/em&gt; of the school year, while the others utilized a range. This can easily be solved using regex substitution. We&amp;rsquo;ll do that now.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import re

for year, df in source_df.items():
    if year != &amp;quot;2018&amp;quot;:
        df.rename(columns={
            df.columns[1] : re.sub(r&amp;quot;\d{2}-&amp;quot;, &#39;&#39;, df.columns[1]),
            df.columns[2] : re.sub(r&amp;quot;\d{2}-&amp;quot;, &#39;&#39;, df.columns[2]),
        }, 
                  inplace=True)
    # print the output for verification
    print(f&amp;quot;{year}:\t{df.columns}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020:	Index([&#39;State&#39;, &#39;Salary 2019&#39;, &#39;Salary 2020&#39;], dtype=&#39;object&#39;)
2019:	Index([&#39;State&#39;, &#39;Salary 2018&#39;, &#39;Salary 2019&#39;], dtype=&#39;object&#39;)
2018:	Index([&#39;State&#39;, &#39;Salary 2017&#39;, &#39;Salary 2018&#39;], dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that everything works, we can do our merge to create a single dataframe with the information for all of the school years we have downloaded.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge_df = source_df[&amp;quot;2018&amp;quot;].drop([&amp;quot;Salary 2018&amp;quot;], axis=1).merge(
                    source_df[&amp;quot;2019&amp;quot;].drop([&amp;quot;Salary 2019&amp;quot;], axis=1)).merge(
                    source_df[&amp;quot;2020&amp;quot;])

merge_df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Salary 2017&lt;/th&gt;
      &lt;th&gt;Salary 2018&lt;/th&gt;
      &lt;th&gt;Salary 2019&lt;/th&gt;
      &lt;th&gt;Salary 2020&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;50,391&lt;/td&gt;
      &lt;td&gt;50,568&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
      &lt;td&gt;54,095&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;6 8,138&lt;/td&gt;
      &lt;td&gt;69,682&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
      &lt;td&gt;70,877&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;4 7,403&lt;/td&gt;
      &lt;td&gt;48,723&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
      &lt;td&gt;50,381&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;4 8,304&lt;/td&gt;
      &lt;td&gt;50,544&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
      &lt;td&gt;49,822&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;7 9,128&lt;/td&gt;
      &lt;td&gt;80,680&lt;/td&gt;
      &lt;td&gt;83,059&lt;/td&gt;
      &lt;td&gt;84,659&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;numeric-conversion&#34;&gt;Numeric Conversion&lt;/h3&gt;
&lt;p&gt;We&amp;rsquo;re almost done! Notice that we still have not dealt with the fact that every column is still treated as a string. Before we can use the &lt;code&gt;to_numeric&lt;/code&gt; function, we still need to take care of two issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The commas in the numbers. While they are nice for our human eyes, Pandas doesn&amp;rsquo;t like them.&lt;/li&gt;
&lt;li&gt;In the 2017 salary column, there appears to be extraneous white space after the first digit for some entries.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Luckily, both of these problems can be remedied with a simple string replacement operation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge_df.iloc[:,1:] = merge_df.iloc[:,1:].replace(r&amp;quot;[,| ]&amp;quot;, &#39;&#39;, regex=True)

for col in merge_df.columns[1:]:
    merge_df[col] = pd.to_numeric(merge_df[col])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we&amp;rsquo;re done! We have created an overview of annual teacher salaries from the 2016-17 school year until 2019-20 extracted from a series of PDFs published by the NEA. We have cleaned up the data and converted everything to numerical values. We can now get summary statistics and do any analysis of interest with this data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge_df.describe() # summary stats of our numeric columns
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Salary 2017&lt;/th&gt;
      &lt;th&gt;Salary 2018&lt;/th&gt;
      &lt;th&gt;Salary 2019&lt;/th&gt;
      &lt;th&gt;Salary 2020&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;56536.196078&lt;/td&gt;
      &lt;td&gt;57313.039216&lt;/td&gt;
      &lt;td&gt;58983.254902&lt;/td&gt;
      &lt;td&gt;60170.647059&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;9569.444674&lt;/td&gt;
      &lt;td&gt;9795.914601&lt;/td&gt;
      &lt;td&gt;10286.843230&lt;/td&gt;
      &lt;td&gt;10410.259274&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;42925.000000&lt;/td&gt;
      &lt;td&gt;44926.000000&lt;/td&gt;
      &lt;td&gt;45105.000000&lt;/td&gt;
      &lt;td&gt;45192.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;49985.000000&lt;/td&gt;
      &lt;td&gt;50451.500000&lt;/td&gt;
      &lt;td&gt;51100.500000&lt;/td&gt;
      &lt;td&gt;52441.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;54308.000000&lt;/td&gt;
      &lt;td&gt;53815.000000&lt;/td&gt;
      &lt;td&gt;54935.000000&lt;/td&gt;
      &lt;td&gt;57091.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;61038.000000&lt;/td&gt;
      &lt;td&gt;61853.000000&lt;/td&gt;
      &lt;td&gt;64393.500000&lt;/td&gt;
      &lt;td&gt;66366.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;81902.000000&lt;/td&gt;
      &lt;td&gt;84227.000000&lt;/td&gt;
      &lt;td&gt;85889.000000&lt;/td&gt;
      &lt;td&gt;87543.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;table-b-6&#34;&gt;Table B-6&lt;/h2&gt;
&lt;p&gt;As mentioned above, table B-6 in the 2020 Report presents slightly greater challenges. A lot of the cleaning is similar or identical, so I will not reproduce it in full. Instead, I have loaded a subsetted part of table B-6 and will show how this can be cleaned up as well. But first, let&amp;rsquo;s look at the first several entries:&lt;/p&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;2017-18 (Revised)&lt;/th&gt;
      &lt;th&gt;2018-19&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;State&lt;/td&gt;
      &lt;td&gt;Salary($) Rank&lt;/td&gt;
      &lt;td&gt;Salary($)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;50,568 36&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;69,682 7&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;48,315 45&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,096 44&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;80,680 2&lt;/td&gt;
      &lt;td&gt;83,059 *&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Colorado&lt;/td&gt;
      &lt;td&gt;52,695 32&lt;/td&gt;
      &lt;td&gt;54,935&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Connecticut&lt;/td&gt;
      &lt;td&gt;74,517 * 5&lt;/td&gt;
      &lt;td&gt;76,465 *&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Delaware&lt;/td&gt;
      &lt;td&gt;62,422 13&lt;/td&gt;
      &lt;td&gt;63,662&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can see that there is an additional hurdle compared to the previous tables: the second column now contains data from two columns, both the Salary information as well as a ranking of the salary as it compares to the different states. For a few states, there is additionally a &amp;lsquo;*&amp;rsquo; to denote values that were estimated as opposed to received. We can again use a simple regex replace together with a capture group to parse out only those values that we are interested in, while dropping the extraneous information using the code below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;b6.iloc[:,1:] = b6.iloc[:,1:].replace(r&amp;quot;([\d,]+).*&amp;quot;, r&amp;quot;\1&amp;quot;, regex=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we&amp;rsquo;re back to where we were above before we did the string conversion. This is what it looks like after also dropping the first row and renaming the columns:&lt;/p&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Salary 2018&lt;/th&gt;
      &lt;th&gt;Salary 2019&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;50,568&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;69,682&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;48,315&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,096&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;80,680&lt;/td&gt;
      &lt;td&gt;83,059&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Colorado&lt;/td&gt;
      &lt;td&gt;52,695&lt;/td&gt;
      &lt;td&gt;54,935&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Connecticut&lt;/td&gt;
      &lt;td&gt;74,517&lt;/td&gt;
      &lt;td&gt;76,465&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Delaware&lt;/td&gt;
      &lt;td&gt;62,422&lt;/td&gt;
      &lt;td&gt;63,662&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From here on out, we can proceed as in the previous example.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accessing Census Data via API</title>
      <link>https://dmsenter89.github.io/post/20-08-census-api/</link>
      <pubDate>Sat, 22 Aug 2020 08:53:55 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/20-08-census-api/</guid>
      <description>&lt;p&gt;The Census Bureau makes an incredible amount of data available online. In this post, I will summarize how to get access to this data via Python by using the Census Bureau&amp;rsquo;s API. The Census Bureau makes a pretty useful guide available 
&lt;a href=&#34;https://www.census.gov/data/developers/guidance/api-user-guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; - I recommend checking it out.&lt;/p&gt;
 &lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#api-basics&#34;&gt;API Basics&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#building-an-the-base-url&#34;&gt;Building an the Base URL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#building-the-query&#34;&gt;Building the Query&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#the-get-variables&#34;&gt;The &amp;lsquo;Get&amp;rsquo; Variables&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#location-variables&#34;&gt;Location Variables&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-complete-call&#34;&gt;The Complete Call&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#making-the-api-request&#34;&gt;Making the API Request&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#reading-the-json-into-pandas&#34;&gt;Reading the JSON into Pandas&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;api-basics&#34;&gt;API Basics&lt;/h2&gt;
&lt;p&gt;We can think of an API query of consisting of two main parts: a &lt;em&gt;base URL&lt;/em&gt; (also called a root URL) and a &lt;em&gt;query&lt;/em&gt; string. These two strings are joined together with the query character &amp;ldquo;?&amp;rdquo; to create an API call. The resulting API call can in theory be copy-and-pasted into the URL bar of your browser, and I recommend this when first playing around with a new API. Seeing the raw text returned in the browser can help you understand the structure of what is being returned. In the case of the Census Bureau&amp;rsquo;s API, it returns a string that essentially looks like a list of lists from a Python perspective. This can easily be turned into a Pandas dataset. Be aware that all values are returned as strings. You&amp;rsquo;ll have to convert number columns to numeric by yourself.&lt;/p&gt;
&lt;p&gt;To get an overview of all available data sets, you can go to the 
&lt;a href=&#34;https://api.census.gov/data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;data page&lt;/a&gt; which contains a long list of data sets. This data page is incredibly useful because it gives access to all of the information needed to build a correct API call, including the base URLs of all data sets and the variables available in each.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-a-snapshot-of-two-datasets-available-as-part-of-the-2018-american-community-survey-acs&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/20-08-census-api/census_overview_hu401560307af60bd2eb326d2765e64aa0_85308_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;A snapshot of two datasets available as part of the 2018 American Community Survey (ACS).&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/20-08-census-api/census_overview_hu401560307af60bd2eb326d2765e64aa0_85308_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1805&#34; height=&#34;372&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    A snapshot of two datasets available as part of the 2018 American Community Survey (ACS).
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;building-an-the-base-url&#34;&gt;Building an the Base URL&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s build a sample API call for the 2018 American Community Survey 1-Year Detailed Table. While we could just copy the base URL from the data page, I like to assemble mine manually from its component parts. This makes it easier to write a wrapper for the API calls if you plan on scraping the same data from multiple years.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;host_name = &amp;quot;https://api.census.gov/data&amp;quot;
year = &amp;quot;2018&amp;quot;
dataset_name = &amp;quot;acs/acs1&amp;quot;
base_URL = f&amp;quot;{host_name}/{year}/{dataset_name}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;building-the-query&#34;&gt;Building the Query&lt;/h2&gt;
&lt;p&gt;Now that we have the base URL, we can work on building the query. For purposes of the Census Bureau, you will need two components: the variables of interest, which are listed after the &lt;code&gt;get=&lt;/code&gt; keyword, and the geography for which you would like the data listed after the &lt;code&gt;for=&lt;/code&gt; keyword. For certain subdivisions, like counties, you can specify two levels of geography by adding an &lt;code&gt;in=&lt;/code&gt; keyword at at the end.&lt;/p&gt;
&lt;h3 id=&#34;the-get-variables&#34;&gt;The &amp;lsquo;Get&amp;rsquo; Variables&lt;/h3&gt;
&lt;p&gt;Since many of the data sets have a large amount of variables in them, it often makes sense to take a look at the &amp;ldquo;groups&amp;rdquo; page first. This page lists variables as groups, giving you a better overview of what data is available. This page is available at &lt;code&gt;{base_URL}/groups.html&lt;/code&gt;. A complete list of all variables in the data set is available at &lt;code&gt;{base_URL}/variables.html&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s find some variables. The most basic variable we&amp;rsquo;d expect to find here is total population. We can find this variable in group &amp;ldquo;B01003&amp;rdquo;. The total estimate is in sub-variable &amp;ldquo;001E&amp;rdquo;, meaning that the variable for total population is &amp;ldquo;B01003_001E&amp;rdquo;. Let&amp;rsquo;s also get household income (group &amp;ldquo;B19001&amp;rdquo;) not broken down by race: &amp;ldquo;B19001_001E&amp;rdquo;. There is also median monthly housing cost (group B25105) with variable &amp;ldquo;B25105_001E&amp;rdquo;. Since the variable names can be a little difficult to parse, I recommend making a data dictionary as you prepare the list of variables to fetch.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dictionary = {
    &amp;quot;B01003_001E&amp;quot; : &amp;quot;Total Population&amp;quot;,
    &amp;quot;B19001_001E&amp;quot; : &amp;quot;Household Income (12 Month)&amp;quot;,
    &amp;quot;B25105_001E&amp;quot; : &amp;quot;Median Monthly Housing Cost&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This way, the list of variables can easily be created from the data dictionary:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;get_vars = &#39;,&#39;.join(data_dictionary.keys())
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;location-variables&#34;&gt;Location Variables&lt;/h3&gt;
&lt;p&gt;Which geographic variables are available for a particular data set can be found &lt;code&gt;{base_URL}/geography.html&lt;/code&gt;. The Census Bureau uses FIPS codes to reference the different geographies. To find the relevant codes, see 
&lt;a href=&#34;https://www.census.gov/geographies/reference-files.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Delaware for example has FIPS code 10 while North Carolina is 37. So to get information for these two states, we&amp;rsquo;d use &lt;code&gt;for=state:10,37&lt;/code&gt;. You can also use &amp;lsquo;*&amp;rsquo; as a wildcard. So to get all the states&#39; info you&amp;rsquo;d write &lt;code&gt;for=state:*&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Subdivisions for similarly. To get information for Orange County (FIPS 135) in North Carolina (FIPS 37), you could write &lt;code&gt;for=county:135&lt;/code&gt; with the keyword &lt;code&gt;in=state:37&lt;/code&gt;. Let&amp;rsquo;s get the information for Orange and Alamance counties in North Carolina.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;county_dict = {
    &amp;quot;001&amp;quot; : &amp;quot;Alamance County&amp;quot;,
    &amp;quot;135&amp;quot; : &amp;quot;Orange County&amp;quot;,
}
county_fips = &#39;,&#39;.join(county_dict.keys())

state_dict = {&amp;quot;37&amp;quot; : &amp;quot;North Carolina&amp;quot;}
state_fips = &#39;,&#39;.join(state_dict.keys())

query_str = f&amp;quot;get={get_vars}&amp;amp;for=county:{county_fips}&amp;amp;in=state:{state_fips}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-complete-call&#34;&gt;The Complete Call&lt;/h3&gt;
&lt;p&gt;The complete API call can now be easily assembled from the previous two pieces:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;api_call = base_URL + &amp;quot;?&amp;quot; + query_str
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we copy-and-paste this output into our browser, we can see the result looks as follows:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-result-of-our-sample-api-query&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/20-08-census-api/API_return_hu0ee03133ff661cb0ce10ab25f3a6aced_3573_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;The result of our sample API query.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/20-08-census-api/API_return_hu0ee03133ff661cb0ce10ab25f3a6aced_3573_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;454&#34; height=&#34;55&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The result of our sample API query.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;making-the-api-request&#34;&gt;Making the API Request&lt;/h2&gt;
&lt;p&gt;We can make the API request with Python&amp;rsquo;s requests package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests

r = requests.get(api_call)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it! We now have the response we wanted. To interpret the response as JSON, we would call the json method of the response object: &lt;code&gt;r.json()&lt;/code&gt;. The result can then be fed into Pandas to generate our data set.&lt;/p&gt;
&lt;h2 id=&#34;reading-the-json-into-pandas&#34;&gt;Reading the JSON into Pandas&lt;/h2&gt;
&lt;p&gt;We can use Pandas&#39; DataFrame method directly on our data, making sure to specify that the first row consists of column headers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd

data = r.json()

df = pd.DataFrame(data[1:], columns=data[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then do any renaming based on the dictionaries we have created previously.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.rename(columns=data_dictionary, inplace=True)
df[&#39;county&#39;] = df[&#39;county&#39;].replace(county_dict)
df[&#39;state&#39;]  = df[&#39;state&#39;].replace(state_dict)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last step is to make sure our numeric columns are interpreted as such. Since all of the requested variables are in fact numeric,
we can use the dictionary of variables to convert what we need to numeric variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for col in data_dictionary.values():
    df[col] = pd.to_numeric(df[col])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it! We&amp;rsquo;re now ready to work with our data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A semi-automated finite difference mesh creation method for use with immersed boundary software IB2d and IBAMR</title>
      <link>https://dmsenter89.github.io/publication/senter-2020-meshmerizeme/</link>
      <pubDate>Mon, 03 Aug 2020 11:27:55 -0400</pubDate>
      <guid>https://dmsenter89.github.io/publication/senter-2020-meshmerizeme/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Basics of Web Scraping with Python</title>
      <link>https://dmsenter89.github.io/talk/webscraping-tutorial/</link>
      <pubDate>Thu, 30 Jul 2020 13:00:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/talk/webscraping-tutorial/</guid>
      <description>&lt;p&gt;This workshop covers data acquisition and basic data preparation with a focus on using Python with Jupyter Notebooks. To avoid having to install Python locally during the workshop, we will be utilizing an 
&lt;a href=&#34;https://notebooks.azure.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azure notebook&lt;/a&gt; project. The example files are located 
&lt;a href=&#34;https://notebooks.azure.com/dmsenter/projects/datacollectiontutorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Please note that the free Azure notebooks will only be available until early October. To continue using Python and Jupyter notebooks, you may want to consider using a local installation. For Windows and Mac users, I recommend using 
&lt;a href=&#34;https://www.anaconda.com/products/individual&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anaconda&lt;/a&gt;. For continued cloud usage, you may consider 
&lt;a href=&#34;https://cocalc.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cocalc&lt;/a&gt;. Please note that you will need a subscription for your Cocalc notebooks to be able to download data from external sources.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Additional Links:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://markummitchell.github.io/engauge-digitizer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Engauge Digitizer&lt;/a&gt; (software to extract data points from graphs).&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Markdown Cheatsheet&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Webscraping Tutorial</title>
      <link>https://dmsenter89.github.io/slides/webscraping-tutorial/</link>
      <pubDate>Thu, 30 Jul 2020 12:30:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/slides/webscraping-tutorial/</guid>
      <description>&lt;h1 id=&#34;basics-of-web-scraping-with-python&#34;&gt;Basics of Web Scraping with Python&lt;/h1&gt;
&lt;p&gt;Michael Senter&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;goals-for-today&#34;&gt;Goals for Today&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Understand what tools and methods are available.
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Be able to create a new project using Python and Jupyter.
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Be able to edit existing code snippets to gather data.
&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;easy to learn, reads like &amp;ldquo;pseudocode&amp;rdquo;&lt;/li&gt;
&lt;li&gt;widely used in a variety of fields&lt;/li&gt;
&lt;li&gt;many books, websites, etc. to help you learn&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Hello, world!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;data-sources&#34;&gt;Data Sources&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;csvexcel-downloads&#34;&gt;CSV/Excel Downloads&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;covid-related-data&#34;&gt;COVID Related Data&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;johns-hopkins-dashboard&#34;&gt;Johns Hopkins Dashboard&lt;/h2&gt;
&lt;p&gt;The Johns Hopkins data is published on 
&lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; and is updated regularly.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;using-sas&#34;&gt;Using SAS&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;filename outfile &amp;quot;~/import-data-nyt.sas&amp;quot;;

/* download official SAS script to above filename */
proc http url=&amp;quot;https://raw.githubusercontent.com/sassoftware/covid-19-sas/master/Data/import-data-nyt.sas&amp;quot; 
  method=&amp;quot;get&amp;quot; out=outfile;
run;

/* run the downloaded script */
%include &amp;quot;~/import-data-nyt.sas&amp;quot;;
/* state and county level data are now in memory */
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Porting Forward</title>
      <link>https://dmsenter89.github.io/post/porting-forward/</link>
      <pubDate>Mon, 27 Jul 2020 11:31:07 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/porting-forward/</guid>
      <description>&lt;p&gt;My website is back up and running! Some incompatabilities between my old site and updates to both Hugo and the Academic Theme have led to some downtime on this page as I didn&amp;rsquo;t have time to look through how to rebuild my site without loosing previous content. I&amp;rsquo;m currently in the process of updating everything and will try to bring back some material as well. Stay tuned!&lt;/p&gt;
&lt;p&gt;This page is currently using the Academic theme from Hugo. Docs and other templates are available at 
&lt;a href=&#34;https://wowchemy.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wowchemy&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Thesis Proposal</title>
      <link>https://dmsenter89.github.io/talk/thesis-proposal/</link>
      <pubDate>Fri, 17 Apr 2020 09:00:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/talk/thesis-proposal/</guid>
      <description>&lt;p&gt;Please join me as I present the work I have done so far in my graduate career and discuss
avenues for future study.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Clap and Fling</title>
      <link>https://dmsenter89.github.io/project/clap-and-fling/</link>
      <pubDate>Fri, 01 Jun 2018 11:23:23 -0400</pubDate>
      <guid>https://dmsenter89.github.io/project/clap-and-fling/</guid>
      <description>&lt;p&gt;Insects are ubiquitious throughout the world. Most of us are familiar with winged insects such as butterflies and bees. Insect flight is an interesting topic from a biomechanics perspective. Unlike birds, most insects (with some eceptions, such as dragonflies and others) do not have flight muscles attached to their wings. Instead, their flight muscles oscillate their thorax, which in turn makes the wings move. Furthermore, they beat their wings at a very high speed. The aerodynamics of insect flight are also very interesting. Larger insects are able to fly by creating a leading edge vortex. This method does not work in the smallest insect fliers. Such insects include the thrips and chalcid wasps, some of which have wingspans as small as 1 mm. These insects have unusual wing structures, as can be seen in this image:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/b/b8/Thysanoptera-thripidae-sp.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The solid part of the wing is rather small and narrow, with many large bristles projecting from the solid part of the wing. Insects such as thrips do not create a leading edge vortex; instead, they fly using the &amp;ldquo;clap and fling&amp;rdquo; method. This method is common amongst insects who fly in the intermediate Reynolds number regime, $1\leq \mathrm{Re} \leq 100$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MeshmerizeMe</title>
      <link>https://dmsenter89.github.io/project/meshmerizeme/</link>
      <pubDate>Wed, 20 Sep 2017 11:35:06 -0400</pubDate>
      <guid>https://dmsenter89.github.io/project/meshmerizeme/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://github.com/nickabattista/IB2d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IB2d&lt;/a&gt; and

&lt;a href=&#34;https://github.com/IBAMR/IBAMR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IBAMR&lt;/a&gt; are two software packages implementing
the immersed boundary method (see below). These packages model fluid-structure
interaction problems based on user given parameters and geometry. The manual
creation of the initial geometry mesh can be difficult and time consuming,
especially for the complex shapes encountered in biological applications.
Oftentimes we have images of the geometry we wish to explore.
I am developing software to help automate the creation of such CFD meshes for
2D simulations with a file-format suitable for use with IB2d and IBAMR from
images. An initial prototype version is available on

&lt;a href=&#34;https://github.com/dmsenter89/MeshmerizeMe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;. A paper
exploring the use of MeshmerizeMe in conjuction with IB2d for simulations is
in preparation.&lt;/p&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;MeshmerizeMe needs two input files per experimental geometry: an SVG image file
with the geometry of interest and an input2d file with the experiment parameters.
When selecting an SVG for use with MeshmerizeMe it will automatically look for
the input2d file in the same folder. It will then parse the paths, transform
them into the correct coordinate system and appropriately sample the paths based
on the size of the Cartesian grid set in the input2d file. The geometry will be
exported as a vertex file. This file is readable by both IB2d and IBAMR.&lt;/p&gt;
&lt;p&gt;SVGs were chosen as the image source as the are an open, text-based format
making them very accesible to work with. They are standardized for web use and
many tools exist for creating and manipulating SVG images. They can be created
from source images such as photographs or scans by means of edge detection tools
and by manually tracing the outline of a shape of interest
Consider optimizing the SVG prior to processing to save time.&lt;/p&gt;
&lt;p&gt;As the current version of MeshmerizeMe only handles a subset of SVG, tools that
optimize the SVG files created by your editor are very useful. Examples of such
software include  
&lt;a href=&#34;https://github.com/svg/svgo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SVGO&lt;/a&gt;, which also offers a
webapp  called 
&lt;a href=&#34;https://jakearchibald.github.io/svgomg/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SVGOMG&lt;/a&gt;.
Another software is 
&lt;a href=&#34;https://github.com/RazrFalcon/svgcleaner&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;svgcleaner&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;ibm-background&#34;&gt;IBM Background&lt;/h2&gt;
&lt;p&gt;One aspect of computational fluid dynamics is the investigation of
fluid-structure interactions. One method developed for the study of such
interactions is the immersed boundary method (IBM) developed by Peskin&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
It is well known that fluids can be studied from both a Eulerian and a
Lagrangian view. The IBM combines these - the domain of the problem is resolved
as a Cartesian grid on which Eulerian equations are solved for fluid velocity
and pressure. In the case of Newtonian fluids the incompressible Navier-Stokes
equations comprising of&lt;/p&gt;
&lt;p&gt;$$ \rho  \left( \frac{\partial \mathbf{u}}{\partial t} + \mathbf{u} \cdot \nabla \mathbf{u} \right)  = - \nabla \mathbf{p} + \mu \nabla^2 \mathbf{u} + \mathbf{f}$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\nabla \cdot \mathbf{u} = 0$$&lt;/p&gt;
&lt;p&gt;need to be solved.&lt;/p&gt;
&lt;p&gt;The immersed structures are modeled as fibers in the form of parametric
curves $X(s,t)$, where $s$ is a parameter and $t$ is time. The fiber experiences
force distributions $F(s,t)$, and we can derive the force the fiber exerts on
the fluid from the momentum equation. For the fibers we then solve&lt;/p&gt;
&lt;p&gt;$$\mathbf{f} = \int_\Gamma \mathbf{F}(s,t),\delta\left(\mathbf{x}-\mathbf{X}(s,t)\right),ds$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\frac{\partial \mathbf{X}}{\partial t} = \int_\Omega \mathbf{u}(\mathbf{x},t), \delta \left( \mathbf{x}-\mathbf{X}(s,t)\right),d\mathbf{x}.$$&lt;/p&gt;
&lt;p&gt;Here, $\Gamma$ is the immersed structure and $\Omega$ is the fluid domain.&lt;/p&gt;
&lt;p&gt;The immersed structures are discretized not on a Cartesian grid but on a
separate Lagrangian grid on the fiber itself. Of import to CFD software users
is that the initial discretization of the immersed structure has to be
supplied by the user. While this is not too difficult for simple geometries,
the often complex structures encountered in mathematical biology can present
a significant time investment. This is the part where MeshmerizeMe comes in
handy.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Charles S Peskin. 2002. &amp;ldquo;The immersed boundary method.&amp;rdquo; &lt;em&gt;Acta numerica&lt;/em&gt; 11:479-517. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Mean first passage time in a thermally fluctuating viscoelastic fluid</title>
      <link>https://dmsenter89.github.io/publication/hohenegger-2017-mean/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/publication/hohenegger-2017-mean/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
