<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Michael&#39;s Site</title>
    <link>https://dmsenter89.github.io/</link>
      <atom:link href="https://dmsenter89.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Michael&#39;s Site</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 13 Apr 2022 08:27:35 -0400</lastBuildDate>
    <image>
      <url>https://dmsenter89.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Michael&#39;s Site</title>
      <link>https://dmsenter89.github.io/</link>
    </image>
    
    <item>
      <title>Working with the Census API Directly from SAS</title>
      <link>https://dmsenter89.github.io/post/22-04-census-api-with-sas/</link>
      <pubDate>Wed, 13 Apr 2022 08:27:35 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-04-census-api-with-sas/</guid>
      <description>&lt;p&gt;In a previous 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;post&lt;/a&gt;, I have shown how to connect to the Census API and load data
with Python. In this post, I will do the same using SAS instead. Before we get started, two important links
from last time: a guide to the API can be found 
&lt;a href=&#34;https://www.census.gov/data/developers/guidance/api-user-guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and a list of the
available data sets can be accessed 
&lt;a href=&#34;https://api.census.gov/data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;picking-the-data&#34;&gt;Picking the Data&lt;/h2&gt;
&lt;p&gt;For this post, I&amp;rsquo;ll use the same data as last time. There we used the 2018 American Community Survey 1-Year Detailed Table
and asked for three variables - total population, household income, and median monthly cost for Alamance and Orange
counties in North Carolina (FIPS codes 37001 and 37135). The variable names are not very intuitive, so I highly recommend starting
your code with a comment section that includes a markdown-style table of the variables that you want to use. Here is
an example table for our data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;Label&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;B01003_001E&lt;/td&gt;
&lt;td&gt;Total Population&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B19001_001E&lt;/td&gt;
&lt;td&gt;Household Income (12 Month)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B25105_001E&lt;/td&gt;
&lt;td&gt;Median Monthly Housing Cost&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;building-the-query&#34;&gt;Building the Query&lt;/h2&gt;
&lt;p&gt;The next step is to build the query. Like last time, the API consists of a base
URL that points us to the data set we are looking for, a list of the variables
we want to request, and a description of the geography for which we want to
request those variables. Just like last time, I&amp;rsquo;ll build the query using several
macros for flexibility purposes. Note that since &lt;code&gt;&amp;amp;&lt;/code&gt; has a special meaning in SAS,
we need to use &lt;code&gt;%str(&amp;amp;)&lt;/code&gt; when referring to it to avoid having the log clobbered with
warnings about unresolved macros.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;%let baseurl=https://api.census.gov/data/2018/acs/acs1;
%let varlist=NAME,B01003_001E,B19001_001E,B25105_001E;
%let geolist=for=county:001,135%str(&amp;amp;)in=state:37;
%let fullurl=&amp;amp;baseurl.?get=&amp;amp;varlist.%str(&amp;amp;)&amp;amp;geolist.;
%put &amp;amp;=fullurl;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Your log should now show the full query URL:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FULLURL=https://api.census.gov/data/2018/acs/acs1?get=NAME,B01003_001E,B19001_001E,B25105_001E&amp;amp;for=county:001,135&amp;amp;in=state:37
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;making-the-api-request&#34;&gt;Making the API Request&lt;/h2&gt;
&lt;p&gt;The API call is achieved with a simple PROC HTTP call using a temporary file to hold the response from the server.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;filename response temp;

proc http url=&amp;quot;&amp;amp;fullurl.&amp;quot; method=&amp;quot;GET&amp;quot; out=response;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;handling-the-json-response&#34;&gt;Handling the JSON Response&lt;/h2&gt;
&lt;p&gt;We read the JSON response by utilizing the

&lt;a href=&#34;https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lestmtsglobal/n1jfdetszx99ban1rl4zll6tej7j.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LIBNAME JSON Engine&lt;/a&gt;
in SAS.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;libname manual JSON fileref=response;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now run &lt;code&gt;proc datasets lib=manual; quit;&lt;/code&gt;. You&amp;rsquo;ll see two data sets that were created: ALLDATA which contains the whole JSON file&amp;rsquo;s contents
in a single data set, and ROOT which is a data set of all the root-level data. The latter one is the one we want. Here&amp;rsquo;s what the
first few observations in each look like:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-first-few-observations-in-the-automatically-created-data-sets&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/auto_datasets_hu58f00ccbf67d7d2f36e2c3ae0591a33b_44045_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;First few observations in the automatically created data sets.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/auto_datasets_hu58f00ccbf67d7d2f36e2c3ae0591a33b_44045_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1073&#34; height=&#34;490&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    First few observations in the automatically created data sets.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Just like with Python, all columns are treated as character variables at first. Because of the way the Census API is structured,
the first row consists of headers, which SAS didn&amp;rsquo;t use. This is something we&amp;rsquo;ll need to fix. At this point we have two main routes we can use to fix
these issues - we can manually create a new data set from ROOT with PROC SQL and address the issues in that way, or we can take
advantage of SAS&#39; JSON map feature to define how we want to load the JSON when the LIBNAME statement is executed. There are good use cases for each,
so I will show both methods.&lt;/p&gt;
&lt;h3 id=&#34;cleaning-up-via-proc-sql&#34;&gt;Cleaning up via PROC SQL&lt;/h3&gt;
&lt;p&gt;Using PROC SQL, you can rename all the character variables you want to keep. To change from character to numeric,
you&amp;rsquo;ll use the &lt;code&gt;input&lt;/code&gt; function. You can then assign formats and labels as desired. To get rid of the first row,
you can just add a conditional &lt;code&gt;having ordinal_root ne 1&lt;/code&gt; to avoid loading that line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;proc sql;
	create table census as
	select
		element1 as Name,
		input(element2, best12.) as B01003_001E format=COMMA12.  label=&#39;Total Population&#39;,
		input(element3, best12.) as B19001_001E format=DOLLAR12. label=&#39;Household Income (12 Month)&#39;,
		input(element4, best12.) as B25105_001E format=DOLLAR12. label=&#39;Median Monthly Housing Cost&#39;,
		element5 as state,
		element6 as county
	from manual.root
	having ordinal_root ne 1;
quit;
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-result-from-the-proc-sql-method&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/CensusData_SQL_hu13384c4c5502c575dbc7b9e51c49ebcb_20401_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Result from the PROC SQL method.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/CensusData_SQL_hu13384c4c5502c575dbc7b9e51c49ebcb_20401_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1022&#34; height=&#34;124&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Result from the PROC SQL method.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;A benefit of this method is that as you fix the input table, you can already begin to work with
it thanks to the &lt;code&gt;calculated&lt;/code&gt; keyword in PROC SQL. Say we weren&amp;rsquo;t actually interested in housing cost and
household income, but instead would like to know what percent of their annual income a household spends on
housing in a given county. We could just add a new variable to our PROC SQL call and build our table like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;proc sql;
	create table census as
	select
		element1 as Name,
		input(element2, best12.) as B01003_001E format=COMMA12. label=&#39;Total Population&#39;,
		input(element3, best12.) as B19001_001E format=DOLLAR12. label=&#39;Household Income (12 Month)&#39;,
		input(element4, best12.) as B25105_001E format=DOLLAR12. label=&#39;Median Monthly Housing Cost&#39;,
		/* Now calculate what we want from the new columns: */
		12*(calculated B25105_001E)/calculated B19001_001E as HousingCostPCT format=PERCENT10.2,
		element5 as state,
		element6 as county
	from manual.root
	having ordinal_root ne 1;
quit;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;using-a-json-map&#34;&gt;Using a JSON MAP&lt;/h3&gt;
&lt;p&gt;Alternatively, we could change the way SAS reads the JSON data by editing the JSON map it uses to decode
the JSON file. The first step is to ask SAS to create a map for us to edit:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;filename automap &amp;quot;sas.map&amp;quot;;
libname autodata JSON fileref=response map=automap automap=create;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The map will look something like this:





  
  











&lt;figure id=&#34;figure-beginning-of-the-automatically-created-json-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_hu793d98e625aa154a8d9815a25890d65f_22860_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Beginning of the automatically created JSON map.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_hu793d98e625aa154a8d9815a25890d65f_22860_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;460&#34; height=&#34;426&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Beginning of the automatically created JSON map.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Note that this is also a JSON file which you can edit in a text editor. With this map, you can change the names
of the data sets and variables, assign labels and formats, and also re-format incoming data. Variables and data sets
you don&amp;rsquo;t want to read can simply be deleted from the map. Here&amp;rsquo;s the beginning of my edited file:





  
  











&lt;figure id=&#34;figure-beginning-of-my-edited-json-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_EDITED_hua840053fe8872e6ad1fe911a8f4abee6_59487_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Beginning of my edited JSON map.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/AUTO_MAP_EDITED_hua840053fe8872e6ad1fe911a8f4abee6_59487_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;475&#34; height=&#34;575&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Beginning of my edited JSON map.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Since the first row of observations in the JSON are actually a header and non-numeric, I add &lt;code&gt;?&lt;/code&gt; prior to the
specified informat. This prevents errors in the log and simply replaces non-matching variables with missing values.
We can now reload the JSON using our custom map by dropping the &lt;code&gt;automap=create&lt;/code&gt; option from the LIBNAME statement:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;libname autodata JSON fileref=response map=automap;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When I now print the resulting data set, the header row is still there, but replaced by missing values in numeric
columns:





  
  











&lt;figure id=&#34;figure-the-data-set-as-a-result-of-the-edited-json-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/MAP_RESULT_hua2828dd6a29f70dd2548d0bd22c856b1_25610_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;The data set as a result of the edited JSON map.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/22-04-census-api-with-sas/MAP_RESULT_hua2828dd6a29f70dd2548d0bd22c856b1_25610_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1026&#34; height=&#34;165&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The data set as a result of the edited JSON map.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This means we&amp;rsquo;ll need to additionally drop this row in a separate step using a delete statement either in
a PROC SQL or DATA step.&lt;/p&gt;
&lt;p&gt;Whichever method you choose, you now can access data via an API call from SAS. Happy exploring!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multline Bash Variable Replacement</title>
      <link>https://dmsenter89.github.io/post/22-03-multiline-replacement/</link>
      <pubDate>Wed, 16 Mar 2022 09:23:12 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-03-multiline-replacement/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve recently needed to append several lines of data to a SAS data step that I collected and built
via a shell script. For search-and-replace in bash I typically use sed, but this time I ran into a problem -
sed does not like multiline shell variables. Thanks to Stack, I found a way to accomplish this task using awk instead.&lt;/p&gt;
&lt;p&gt;Suppose you have a file called data.sas with the following contents:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data person;
   infile datalines delimiter=&#39;,&#39;; 
   input name :$10. dept :$30.;
   datalines4;                      
John,Sales
Mary,Accounting
Theresa,Management
Stewart,HR
;;;;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I am using a datalines4 statement so that I get an easy to identify target for the substitution.
I want to insert a multiline shell variable before the &lt;code&gt;;;;;&lt;/code&gt; to add my data to this data step. Say I have the
following variable:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;NEWDATA=$(cat &amp;lt;&amp;lt;-END
Will,Compliance
Sidney,Management
END
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I try to use sed (&lt;code&gt;sed &amp;quot;s/\;\{4\}/$DATA\n;;;;/&amp;quot; data.sas&lt;/code&gt;) I will get an error about an unterminated s command.
Instead of sed, I can use awk with a variable to achieve the same goal:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;awk -v r=&amp;quot;$NEWDATA\n;;;;&amp;quot; &#39;{gsub(/;{4}/, r)}1&#39; data.sas
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The one downside is that awk does not have an in-place option like sed, and if I try to redirect to the same file
I&amp;rsquo;m reading from I get an empty file out. So you&amp;rsquo;ll have to rename the original file in your processing script to
achieve a similar effect as with the inplace option in sed.&lt;/p&gt;
&lt;p&gt;For additional approaches, see this 
&lt;a href=&#34;https://stackoverflow.com/q/10107459&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StackOverflow Question&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Easy SASPy Setup from Jupyter</title>
      <link>https://dmsenter89.github.io/post/22-03-saspy-setup/</link>
      <pubDate>Fri, 11 Mar 2022 08:30:29 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/22-03-saspy-setup/</guid>
      <description>&lt;p&gt;I love using SASPy, but the setup can take a minute. I used to do the setup via the CLI until I
started thinking I might be able to just do it straight from a Jupyter notebook. Having just a
couple of cells in Jupyter notebook makes for easy copy-and-paste and reduces setup time. The code
below has been tested on both Windows and Linux. As a bonus,
this also works on  
&lt;a href=&#34;https://colab.research.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can easily install packages via pip from Jupyter either by using a shell cell (&lt;code&gt;!&lt;/code&gt;) or by
using the pip magic command: &lt;code&gt;%pip install saspy&lt;/code&gt;. Once done, copy and paste the following into
a code cell and run to create the sascfg_personal.py file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import saspy, platform
from pathlib import Path

# get path for configuration file
cfgpath = saspy.__file__.replace(&#39;__init__.py&#39;,&#39;sascfg_personal.py&#39;)

# To pick the path for Java, we need to know whether we&#39;re on Windows or not
if platform.system()==&#39;Windows&#39;:
    print(&amp;quot;Windows detected.&amp;quot;)
    javapath = !where java
    authfile = Path(Path.home(),&amp;quot;_authinfo&amp;quot;)
else:
    javapath = !which java
    authfile = Path(Path.home(),&amp;quot;.authinfo&amp;quot;)
    
# the `!` command returns a string list, we want only the string
javapath = javapath[0]
print(f&amp;quot;Java is present at {javapath}&amp;quot;)

# US home Region configuration string set up via string-replacement.
# For other server addresses, see https://support.sas.com/ondemand/saspy.html
cfgtext = f&amp;quot;&amp;quot;&amp;quot;SAS_config_names=[&#39;oda&#39;]
oda = {{&#39;java&#39; : &#39;{repr(javapath).strip(&amp;quot;&#39;&amp;quot;)}&#39;,
#US Home Region
&#39;iomhost&#39; : [&#39;odaws01-usw2.oda.sas.com&#39;,&#39;odaws02-usw2.oda.sas.com&#39;,&#39;odaws03-usw2.oda.sas.com&#39;,&#39;odaws04-usw2.oda.sas.com&#39;],
&#39;iomport&#39; : 8591,
&#39;authkey&#39; : &#39;oda&#39;,
&#39;encoding&#39; : &#39;utf-8&#39;
}}&amp;quot;&amp;quot;&amp;quot;

# write the configuration file
with open(cfgpath, &#39;w&#39;) as file:
    file.write(cfgtext)
    print(f&amp;quot;Wrote configuration file to {cfgpath}&amp;quot;)
    print(f&amp;quot;Content of file: \n```\n{cfgtext}\n```&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Optionally, you can set up an authentication file with your username and password. Without this file,
you&amp;rsquo;ll be prompted for your username and password each time you log in.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# change variables to match your username and password
omr_user_id = r&amp;quot;max.mustermann@sample.com&amp;quot;
omr_user_password = r&amp;quot;K5d7#QBPw&amp;quot;
with open(authfile, &amp;quot;w&amp;quot;) as file:
    file.write(f&amp;quot;oda user {omr_user_id} password {omr_user_password}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it! You&amp;rsquo;re now ready to connect to SASPy. In my experience you don&amp;rsquo;t even need to restart
the kernel to begin work with SAS on ODA. You can try the following snippet in a new cell:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# starts a new SAS session with the `oda` configuration we set up
sas_session = saspy.SASsession(cfgname=&#39;oda&#39;)

# load a SAS data set and make a scatter plot
cars = sas_session.sasdata(&#39;cars&#39;, &#39;sashelp&#39;)
cars.scatter(x=&#39;msrp&#39;, y=&#39;horsepower&#39;)

# directly run SAS code to print a table
sas_session.submitLST(&amp;quot;proc print data=sashelp.cars(obs=6); run;&amp;quot;)

# quit SAS connection
sas_session.endsas()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Cleaning up a Date String with RegEx in SAS</title>
      <link>https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/</link>
      <pubDate>Wed, 29 Sep 2021 13:41:36 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/</guid>
      <description>&lt;p&gt;Sometimes we have to deal with manually entered data, which means there is a good chance that the data needs to be cleaned for consistency due to the
inevitable errors that creep in when typing in data, not to speak of any inconsistencies between individuals entering data.&lt;/p&gt;
&lt;p&gt;In my particular case, I was recently dealing with a data set that included
manually calculated ages that had been entered as a complete string
of the number of years, months, and days of an individual. Such a string
is not particularly useful for analysis and I wanted to have the age as
a numeric variable instead. Regular expressions can help out a lot in this
type of situation. In this post, we will look at a few representative examples
of the type of entries I&amp;rsquo;ve encountered and how to read them using RegEx in SAS.&lt;/p&gt;
&lt;h2 id=&#34;lets-look-at-the-data&#34;&gt;Let&amp;rsquo;s Look at the Data&lt;/h2&gt;





  
  











&lt;figure id=&#34;figure-what-were-starting-from&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/RAW_DS_hu83735545411e40d8d73c711ad73aa038_21697_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;What we&amp;amp;rsquo;re starting from.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/RAW_DS_hu83735545411e40d8d73c711ad73aa038_21697_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;508&#34; height=&#34;250&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    What we&amp;rsquo;re starting from.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;If we look at our sample data, we notice a few things. The data is consistently
ordered from largest to smallest, in the order of year, month, and day.
For some lines, only the year variable is available. In all cases, the string
starts with two digits.&lt;/p&gt;
&lt;p&gt;Separation of the time units is inconsistent; occasionally they are separated
by commas, sometimes by hyphens, and in some cases by spaces alone. The terms
indicating the units are spelled and capitalized inconsistently as well. There
are some abbreviations and occasionally the plural &amp;rsquo;s&#39; in days is wrapped in
parentheses.&lt;/p&gt;
&lt;p&gt;If you want to follow along, you can create the sample data with the
following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data raw;
    infile datalines delimiter = &#39;,&#39; MISSOVER DSD;
    attrib
        ID     informat=best32. format=1.
        STR_AGE informat=$500.   format=$500. label=&#39;Age String&#39;
        VAR1   informat=best32. format=1.;
    input ID STR_AGE $ VAR1;

    datalines;
    1,&amp;quot;62 Years, 5 Months, 8 Days&amp;quot;,1
    2,43 Yrs. -2 Months -4 Day(s), 2
    3,33 years * months 24 days, 1
    4,58,1
    5,&amp;quot;47 Yrs. -11 Months -27 Day(s)&amp;quot;,2
    ;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;the-regex-patterns&#34;&gt;The RegEx Patterns&lt;/h2&gt;
&lt;p&gt;We will use a total of three regex patterns, one for each of the time units:
year, month, day.  SAS uses Pearl regex and the function &lt;code&gt;prxparse&lt;/code&gt; to define
the regex patterns that are supposed to be searched for.&lt;/p&gt;
&lt;p&gt;For the year variable, we need to match the first two digits in our string.
Therefore, the correct call is &lt;code&gt;prxparse(&#39;/^(\d{2}).*/&#39;)&lt;/code&gt;. Note that the
&lt;code&gt;(&lt;/code&gt; and &lt;code&gt;)&lt;/code&gt; delimit the capture group.&lt;/p&gt;
&lt;p&gt;The month and day regex patterns are very similar. For the months, we want to
lazy-match the until we hit between one or two digits followed by
an &amp;rsquo;m&#39; and some number of other characters. We use the &lt;code&gt;i&lt;/code&gt; flag since
we cannot guarantee capitalization: &lt;code&gt;prxparse(&#39;/.*?(\d{1,2}).M.*/i&#39;)&lt;/code&gt;.
The day pattern is nearly identical: &lt;code&gt;prxparse(&#39;/.*?(\d{1,2}).D\D*$/i&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can extract our matches using the &lt;code&gt;prxposn&lt;/code&gt; function. We use the
&lt;code&gt;prxmatch&lt;/code&gt; function to check if we actually have a match:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;/* match into strings */
if prxmatch(year_rxid, STR_AGE)  then year_dig_str = prxposn(year_rxid,1,STR_AGE);
if prxmatch(month_rxid, STR_AGE) then month_dig_str = prxposn(month_rxid,1,STR_AGE);
if prxmatch(day_rxid, STR_AGE)   then day_dig_str = prxposn(day_rxid,1, STR_AGE);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The extracted strings can then be converted to numeric variables using
the &lt;code&gt;input&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;The last step is the calculation of the age from the three components.
Since not all three time units are specified for every row, we cannot use
the standard arithmetic of &lt;code&gt;years + months + days&lt;/code&gt;, because the missing
values would propagate. We need to use the &lt;code&gt;sum&lt;/code&gt; function instead.&lt;/p&gt;
&lt;p&gt;Putting it all together, we get the correct output:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-result&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/FIXED_DS_hu37f587f0a5cd00ab598beb8689b70b5f_34165_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;The Result&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-09-sas-regex-date-cleanup/FIXED_DS_hu37f587f0a5cd00ab598beb8689b70b5f_34165_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;867&#34; height=&#34;251&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Result
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;complete-code&#34;&gt;Complete Code&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;data fixed;
    set raw;
    
   /* define the regex patterns */
   year_rxid  = prxparse(&#39;/^(\d{2}).*/&#39;);
   month_rxid = prxparse(&#39;/.*?(\d{1,2}).M.*/i&#39;);
   day_rxid   = prxparse(&#39;/.*?(\d{1,2}).D\D*$/i&#39;);   /* match 2 digits followed by D and non-digit chars  */
  
   /* make sure we have enough space to store the extraction */
   length year_dig_str month_dig_str day_dig_str $4;
   
   /* match into strings */
   /* match into strings */
   if prxmatch(year_rxid, STR_AGE)  then year_dig_str = prxposn(year_rxid,1,STR_AGE);
   if prxmatch(month_rxid, STR_AGE) then month_dig_str = prxposn(month_rxid,1,STR_AGE);
   if prxmatch(day_rxid, STR_AGE)   then day_dig_str = prxposn(day_rxid,1, STR_AGE);
   
   /* use input to convert str -&amp;gt; numeric */
   years  = input(year_dig_str, ? 12.);
   months = input(month_dig_str, ? 12.);
   days   = input(day_dig_str, ? 12.);
   
   /* Use SUM function when calculating age
    to avoid missing values propagating  */
   age = sum(years,months/12,days/365.25);
   
   /* get rid of temporary variables */ 
   drop month_rxid month_dig_str year_rxid year_dig_str day_rxid day_dig_str;
   run;
   
proc print data=fixed; run;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>From Proc Import to a Data Step with Regex</title>
      <link>https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/</link>
      <pubDate>Thu, 29 Jul 2021 08:46:10 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/</guid>
      <description>&lt;p&gt;I find myself needing to import CSV files with a relatively large number of columns. In many cases, &lt;code&gt;proc import&lt;/code&gt; works surprisingly well in giving me what I want. But sometimes, I need to do some work while reading in the file and it would be nice to just use a data step to do so, but I don&amp;rsquo;t want to type it in by hand. That&amp;rsquo;s when a combination of &lt;code&gt;proc import&lt;/code&gt; and some regex substitution can come in handy.&lt;/p&gt;
&lt;p&gt;For the first step, run a &lt;code&gt;proc import&lt;/code&gt;, like this sample code that is provided by SAS Studio when you double click on a CSV file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;FILENAME REFFILE &#39;/path/to/file/data.csv&#39;;

PROC IMPORT DATAFILE=REFFILE
    DBMS=CSV
    OUT=WORK.IMPORT;
    GETNAMES=YES;
RUN;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you run this code, you will see that SAS generates a complete data step for you. This is what the beginning of one looks like:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-sample-log-output&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/log_hu417e5750fec5319adb043ca92305efb0_26856_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Sample log output.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/log_hu417e5750fec5319adb043ca92305efb0_26856_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;797&#34; height=&#34;461&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample log output.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;There will be be two lines for each variable, one giving the &lt;code&gt;informat&lt;/code&gt; and one giving the &lt;code&gt;format&lt;/code&gt; that SAS decided on. This will be followed by an &lt;code&gt;input&lt;/code&gt; statement. You can copy that from the log into a text editor such as VSCode, but unfortunately the line numbering of the LOG will carry over. One convenient way of fixing this is to use regex search-and-replace. Each line starts with a space followed by 1-3 digits, followed by a variable number of spaces until the next word. To capture this I use &lt;code&gt;^\s\d{1,3}\s+&lt;/code&gt; as my search term and replace with nothing. This will left align the whole data step, but this can be adjusted later.&lt;/p&gt;
&lt;p&gt;At this point the data step can be saved as a SAS file or copied back over to the file you are working within SAS Studio, but I like to do one more adjustment. I really like using the &lt;code&gt;attrib&lt;/code&gt; statement, 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lestmtsref/n1wxb7p9jkxycin16lz2db7idbnt.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see documentation&lt;/a&gt;, because it allows me to see the informat, format, and label of a variable all in one place. So I use regex to re-arrange my informat statement into the beginnings of an attribute statement. Use the search term &lt;code&gt;informat\s([^\s]+)\s([^\s]+)\s+;&lt;/code&gt; to capture each informat line and create two capture groups - the variable name as group 1 and the informat as group 2. If you use the replace code &lt;code&gt;$1 informat=$2 format=$2&lt;/code&gt;, you will see the beginnings of an attribute statement. In this replacement scheme, each informat matches each format. This is fine for date and character variables, but you may want to adjust the display format for some of your numeric variables.&lt;/p&gt;
&lt;p&gt;To clean this up, get rid of the format lines (you can search for &lt;code&gt;^format.+\n&lt;/code&gt; and replace with an empty replace to delete them), add the &lt;code&gt;attrib&lt;/code&gt; statement below the &lt;code&gt;infile&lt;/code&gt; and make sure to end the block of attributes with a semicolon, and indent your code as desired.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-sample-data-step-view&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/code_snip_hub5c78044ade6674b35a08b503d783f3d_19917_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;Sample data step view.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-07-proc-import-to-data-step-with-regex/code_snip_hub5c78044ade6674b35a08b503d783f3d_19917_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;843&#34; height=&#34;190&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample data step view.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;And there you have it! The beginning of a nicely formatted data step that you can start to work with.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making VS Code and Python Play Nice on Windows</title>
      <link>https://dmsenter89.github.io/post/21-07-vsc-python-fix/</link>
      <pubDate>Wed, 21 Jul 2021 08:49:52 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-07-vsc-python-fix/</guid>
      <description>&lt;p&gt;One of the editors I use regularly is VS Code. I work a lot with Python, but when installing Anaconda
using default settings on a Windows machine already having VSC installed there&amp;rsquo;s a good chance you&amp;rsquo;ll run into
an issue. When attempting to run Python code straight from VSC you may get an error. This should be fixed
on some newer versions of Anaconda, but I&amp;rsquo;ve needed to do something about it often enough I feel it&amp;rsquo;s
useful to save the solution 
&lt;a href=&#34;https://stackoverflow.com/users/1072989/janh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;janh&lt;/a&gt; posted on

&lt;a href=&#34;https://stackoverflow.com/questions/54828713/working-with-anaconda-in-visual-studio-code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StackExchange&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Specifically, the issue can be fixed by manually changing VSC&amp;rsquo;s default shell from PowerShell to CMD.
Just open the command palette (CTRL+SHIFT+P), search &amp;ldquo;Terminal: Select Default Profile&amp;rdquo; and switch to
&amp;ldquo;Command Prompt&amp;rdquo;. Everything should work as expected from now on!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making INPUT and LABEL Statements with AWK</title>
      <link>https://dmsenter89.github.io/post/2021-07-awk-for-sas/</link>
      <pubDate>Tue, 06 Jul 2021 10:38:27 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/2021-07-awk-for-sas/</guid>
      <description>&lt;p&gt;I am currently working with a database provided by the North Carolina Department of Public Safety
that consists of several fixed-width files. Each of these has an associated codebook that gives the
internal variable name, a label of the variable, its data type, as well as the start column and
the length of the fields for each column. To import the data sets into SAS, I could copy and paste
part of that data into my INPUT and LABEL statements, but that gets tedious pretty fast when dealing
with dozens of lines. And since I have multiple data sets like that, I didn&amp;rsquo;t really want to do it that way.
In this post I show how a simple command-line script can be written to deal with this problem.&lt;/p&gt;
&lt;h2 id=&#34;introducing-awk&#34;&gt;Introducing AWK&lt;/h2&gt;
&lt;p&gt;Here are the first few lines of one of these files:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CMDORNUM      OFFENDER NC DOC ID NUMBER          CHAR      1       7     
CMCLBRTH      OFFENDER BIRTH DATE                DATE      8       10    
CMCLSEX       OFFENDER GENDER CODE               CHAR      18      30    
CMCLRACE      OFFENDER RACE CODE                 CHAR      48      30    
CMCLHITE      OFFENDER HEIGHT (IN INCHES)        CHAR      78      2     
CMWEIGHT      OFFENDER WEIGHT (IN LBS)           CHAR      80      3     
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the data is tabular and separated by multiple spaces. Linux programs often deal
with column data and a tool is available for manipulating column-based data on the command-line:
AWK, a program that can be used for complex text manipulation from the command-line. Some useful
tutorials on AWK in general are available at 
&lt;a href=&#34;https://www.grymoire.com/Unix/Awk.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;grymoire.com&lt;/a&gt;
and at 
&lt;a href=&#34;https://www.tutorialspoint.com/awk/index.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorialspoint&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For our purposes, we want to know about the &lt;code&gt;print&lt;/code&gt; and &lt;code&gt;printf&lt;/code&gt; commands for AWK. To illustrate
how this works, make a simple list of three lines with each term separated by a space:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cat &amp;lt;&amp;lt; EOF &amp;gt; list.txt
1 one apple pie
2 two orange cake
3 three banana shake
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To print the whole file, you&amp;rsquo;d use the print statement: &lt;code&gt;awk &#39;{print}&#39; list.txt&lt;/code&gt;. But I could do that with
&lt;code&gt;cat&lt;/code&gt;, so what&amp;rsquo;s the point? Well, what if I only want &lt;em&gt;one&lt;/em&gt; of the columns? By default, &lt;code&gt;$n&lt;/code&gt; refers to the
&lt;em&gt;n&lt;/em&gt;th column in AWK. So to print only the fruits I could write &lt;code&gt;awk &#39;{print $3}&#39; list.txt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Multiple columns can be printed by listing multiple columns separated by a comma:
&lt;code&gt;awk &#39;{print $2,$3}&#39; list.txt&lt;/code&gt;. Note that if you omit the comma the two columns get concatenated into
a single column.&lt;/p&gt;
&lt;p&gt;If additional formatting is required, we can use the &lt;code&gt;printf&lt;/code&gt; command. So to create a hyphenated
fruit and food-item column, we could use &lt;code&gt;awk &#39;{printf &amp;quot;%s-%s\n&amp;quot;, $3, $4}&#39; list.txt&lt;/code&gt;. Note that we
have to indicate the end-of line or else everything will be printed into a single line of text.&lt;/p&gt;
&lt;p&gt;Now we almost have all of the skills to create the label and input statements in SAS! Let&amp;rsquo;s create
a comma-delimited list for practice:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; list.txt
1,one,apple pie
2,two,orange cake
3,three,banana shake
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;-F&lt;/code&gt; flag is used to tell AWK to use a different column separator. So to print the
third column, we&amp;rsquo;d use &lt;code&gt;awk -F &#39;,&#39; &#39;{print $3}&#39; list.txt&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;making-the-sas-statements&#34;&gt;Making the SAS statements&lt;/h2&gt;
&lt;p&gt;Now we know everything we need to know about AWK to create code we want. First we note that
our coding file uses multiple spaces as column separators as opposed to single spaces. If
each item was a single word, this wouldn&amp;rsquo;t be a problem. Unfortunately, our second column
reads &amp;ldquo;OFFENDER NC DOC ID NUMBER&amp;rdquo; which would be split into five columns by default. So we
will need to use the column separator flag as &lt;code&gt;-F &#39;[[:space:]][[:space:]]+&#39;&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-label-statement&#34;&gt;The LABEL Statement&lt;/h3&gt;
&lt;p&gt;A SAS label has the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/v_011/lestmtsref/n1r8ub0jx34xfsn1ppcjfe0u16pc.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;general form&lt;/a&gt;
&lt;code&gt;LABEL variable-1=label-1&amp;lt;...variable-n=label-n&amp;gt;;&lt;/code&gt;, so for example&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;label score1=&amp;quot;Grade on April 1 Test&amp;quot;  
      score2=&amp;quot;Grade on May 1 Test&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is a valid label statement. In our file the variable names are given in column 1
and the appropriate labels in column 2. So an AWK script to print the appropriate
labels can be written like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;awk -F &#39;[[:space:]][[:space:]]+&#39; &#39;{printf &amp;quot;\t%s=\&amp;quot;%s\&amp;quot;\n&amp;quot;, $1, $2}&#39; FILE.DAT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what everything looks like given our code:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;label.PNG&#34; alt=&#34;Sample Code returned by AWK.&#34; title=&#34;Sample Code returned by AWK.&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-input-statement&#34;&gt;The INPUT STATEMENT&lt;/h3&gt;
&lt;p&gt;The INPUT statement can be made in a similar way, it just requires some minor tweaking as
INPUT can be a bit more complex to handle a variety of data, see the 
&lt;a href=&#34;https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/lestmtsref/n0oaql83drile0n141pdacojq97s.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;.
In our case we are dealing with a fixed-width record. The fourth column gives the starting column
of the data and the fifth gives us the width of that field. The third gives us the data type.
The majority of ours are character, so it seems easiest to just have the AWK script print each
line as though it were a character together with a SAS comment giving the name and &amp;ldquo;official&amp;rdquo; data
type. Then the few lines that need adjustment can be manually adjusted. The corresponding code would
look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;awk -F &#39;[[:space:]][[:space:]]+&#39; &#39;{printf &amp;quot;\t@%s %s $%s. /*%s - %s*/\n&amp;quot;,$4, $1, $5, $3, $2}&#39; FILE.DAT
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what is returned by our code (highlighted part has been manually edited):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;input.PNG&#34; alt=&#34;Sample Code returned by AWK.&#34; title=&#34;Sample Code returned by AWK.&#34;&gt;&lt;/p&gt;
&lt;p&gt;I hope you all find this useful and that it will save you some typing!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SASPy Video Tutorial</title>
      <link>https://dmsenter89.github.io/post/2021-06-youtube-tutorial/</link>
      <pubDate>Tue, 29 Jun 2021 10:57:05 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/2021-06-youtube-tutorial/</guid>
      <description>&lt;p&gt;I have been using both SAS and Python extensively for a while now. With each having great features, it was very useful to combine my
skills in both languages by seamlessly moving between SAS and Python in
a single notebook. In the video below, fellow SAS intern Ariel Chien and I show how easy it is to connect the SAS and Python kernels using the open-source SASPy package together with SAS OnDemand for Academics.
I hope you will also find that this adds to your workflow!&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/6mcsbeKwSqM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The Jupyter notebook from the video can be viewed 
&lt;a href=&#34;https://github.com/sascommunities/sas-howto-tutorials/tree/master/sastopython&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;on GitHub&lt;/a&gt;. For installation instructions, check out the 
&lt;a href=&#34;https://github.com/sassoftware/saspy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SASPy GitHub page&lt;/a&gt;. Configuration for SASPy to connect to ODA can be found 
&lt;a href=&#34;https://support.sas.com/ondemand/saspy.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;at this support page&lt;/a&gt;. For more information on SAS OnDemand for Academics, 
&lt;a href=&#34;https://www.sas.com/en_us/software/on-demand-for-academics.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Census 2020 Population Estimates Updated</title>
      <link>https://dmsenter89.github.io/post/21-06-covid-county-incidence/</link>
      <pubDate>Wed, 09 Jun 2021 16:00:34 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/21-06-covid-county-incidence/</guid>
      <description>&lt;p&gt;The Census Bureau has updated its population estimates for 2020 with county level data. This means any
projects that have had to rely on the 2019 estimates can now switch to the 2020 estimates.&lt;/p&gt;
&lt;p&gt;This is particularly useful for those of us who have been trying to track the development of COVID-19. The
average incidence rates are typically rescaled to new cases per 100,000 people. Previous graphs and maps I
have created used the 2019 estimates. I have now updated my code for mapping North Carolina developments to
use the 2020 estimates.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-county-level-data-for-north-carolina-using-the-nyt-covid-data-set-date-set-to-june-8-2021&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/21-06-covid-county-incidence/nc_avg_incidence_08jun2021_hu8396d2a41a978826522d96cfe881f35d_68326_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;County level data for North Carolina using the NYT COVID data set. Date set to June 8, 2021.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/21-06-covid-county-incidence/nc_avg_incidence_08jun2021_hu8396d2a41a978826522d96cfe881f35d_68326_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    County level data for North Carolina using the NYT COVID data set. Date set to June 8, 2021.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Below this post is my code for loading the necessary data using SAS.
Note that I&amp;rsquo;m using a macro called &lt;code&gt;mystate&lt;/code&gt; that can be set to the statecode abbreviation of your choice.
The conditional &lt;code&gt;County ne 0&lt;/code&gt; is in the code because the county level CSV includes both the county data as
well as the totals for each state.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;
filename popdat url &#39;https://www2.census.gov/programs-surveys/popest/datasets/2010-2020/counties/totals/co-est2020-alldata.csv&#39;;

data censusdata;
	infile POPDAT delimiter=&#39;,&#39; MISSOVER DSD lrecl=32767 firstobs=2;
	informat SUMLEV REGION DIVISION State County best32.
                         STNAME $20. CTYNAME $35. 
		CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020 best32.;
	format SUMLEV REGION DIVISION STATE best32. COUNTY 5. STNAME $20. CTYNAME $35. 
		CENSUS2010POP ESTIMATESBASE2010 POPESTIMATE2010-POPESTIMATE2020 
		COMMA12. StateCode $2.;
	input SUMLEV REGION DIVISION STATE COUNTY STNAME $ CTYNAME $
                        CENSUS2010POP ESTIMATESBASE2010 
		POPESTIMATE2010-POPESTIMATE2020;

	if (State ne 0) and (State ne 72) then
		do;
			FIPS=put(State, Z2.);
			Statecode=fipstate(FIPS);

			if Statecode eq &amp;amp;mystate and County ne 0 then
				output;
		end;
	keep STNAME CTYNAME County FIPS Statecode Popestimate2020;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The media release can be 
&lt;a href=&#34;https://www.census.gov/newsroom/press-releases/2021/2020-vintage-population-estimates.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;viewed here&lt;/a&gt;. The county-level data set can be downloaded 
&lt;a href=&#34;https://www.census.gov/programs-surveys/popest/technical-documentation/research/evaluation-estimates/2020-evaluation-estimates/2010s-counties-total.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;at this page&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dissertation Defense</title>
      <link>https://dmsenter89.github.io/talk/dissertation-defense/</link>
      <pubDate>Wed, 07 Apr 2021 10:00:00 -0500</pubDate>
      <guid>https://dmsenter89.github.io/talk/dissertation-defense/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Metachronal Paddling</title>
      <link>https://dmsenter89.github.io/project/metachronal-paddling/</link>
      <pubDate>Mon, 22 Mar 2021 13:00:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/project/metachronal-paddling/</guid>
      <description>&lt;p&gt;Metachronal paddling can be described as the sequential oscillation of appendages whereby adjacent paddles maintain a nearly constant phase difference.
This mechanism is widely used in nature, both in locomotion such as swimming in crustaceans and in fluid transport such as the clearance of mucus
in the mammalian lung. Aside from the wide range of applications, metachronal paddling can be observed across a wide range of
Reynolds number regimes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/c/cc/Artemia_monica.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I work on simulating the hydrodynamics of metachronal paddling in brine shrimp (&lt;em&gt;Artemia&lt;/em&gt;). Brine shrimp are small aquatic
crustaceans who lay dormat eggs and are widely used in aquaculture. Their thoracopods are spaced closely together and
beat with a small phase difference. We are interested in the hydrodynamics and efficiency of this swimming pattern, which has not
previously been rigorously explored.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Git with SAS Studio</title>
      <link>https://dmsenter89.github.io/post/git-with-sas-studio/</link>
      <pubDate>Mon, 11 Jan 2021 14:42:50 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/git-with-sas-studio/</guid>
      <description>&lt;p&gt;Git is a widely used version control system that allows users to track their software
development in both public and private repositories. It is also increasingly used to store
data in text formats, see for example the 
&lt;a href=&#34;https://github.com/nytimes/covid-19-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New York Times COVID-19 data set&lt;/a&gt;.
This post will briefly demonstrate how to clone and pull updates from a GitHub repository
using the git functions that are built into SAS Studio.&lt;/p&gt;
&lt;p&gt;Git functionality has been built into SAS Studio for a little while, so there are actually
two slightly different iterations of the git functions. The examples in this post will use the versions
compatible with SAS Studio 3.8, which is the current version available at SAS OnDemand for Academics.
All git functions use the same prefix. In older versions such as SAS Studio 3.8 the prefix is &lt;code&gt;gitfn_&lt;/code&gt;,
which is followed by a git command such as &amp;ldquo;clone&amp;rdquo; or &amp;ldquo;pull&amp;rdquo;. In SAS Studio 5, the prefix has been
simplified to just &lt;code&gt;git_&lt;/code&gt;. Most git functions have the same name between the&lt;br&gt;
two versions, so that the only difference is the prefix. A complete table of the old and new
versions of the git functions is available 
&lt;a href=&#34;https://go.documentation.sas.com/?cdcId=pgmsascdc&amp;amp;cdcVersion=9.4_3.5&amp;amp;docsetId=lefunctionsref&amp;amp;docsetTarget=n1mlc3f9w9zh9fn13qswiq6hrta0.htm&amp;amp;locale=en#p0evl64wd2dljrn1l43t739qtwba&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in the documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We use the git functions by calling them in an otherwise empty DATA step. In other words, we use the
format&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
    /* use your git functions here */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;cloning-a-repo&#34;&gt;Cloning a Repo&lt;/h2&gt;
&lt;p&gt;To clone a repo from github we use &lt;code&gt;gitfn_clone&lt;/code&gt;. It takes two arguments -
the URL of the repository of interest and the path to an &lt;em&gt;empty&lt;/em&gt; folder. You can
have SAS create the folder for you by using &lt;code&gt;OPTIONS DLCREATEDIR&lt;/code&gt;. The basic
syntax for the clone is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
    rc = gitfn_clone (
     &amp;quot;&amp;amp;repoURL.&amp;quot;,    /* URL to repo */
     &amp;quot;&amp;amp;targetDIR.&amp;quot;); /* folder to put repo in */
    put rc=;         /* equals 0 if successful */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It doesn&amp;rsquo;t matter if the URL you use ends in &amp;ldquo;.git&amp;rdquo; or not. In other words, the
following two macros would both work the same:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;%LET repoURL=https://github.com/nytimes/covid-19-data;
/* works the same as */
%LET repoURL=https://github.com/nytimes/covid-19-data.git;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use password based authentication to pull in private repositories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
    rc = gitfn_clone (
     &amp;quot;&amp;amp;repoURL.&amp;quot;,   
     &amp;quot;&amp;amp;targetDIR.&amp;quot;,
     &amp;quot;&amp;amp;githubUSER.&amp;quot;,   /* your GitHub username */
     &amp;quot;&amp;amp;githubPASSW.&amp;quot;); /* your GitHub password */
    put rc=;         /* equals 0 if successful */
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; GitHub is &lt;em&gt;deprecating&lt;/em&gt; 
&lt;a href=&#34;https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;password-based authentication&lt;/a&gt;; you will need to switch to OAuth authentication or SSH keys
if you are not already using them. To access a repository using an SSH key, use the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;                             
 rc = gitfn_clone(
  &amp;quot;&amp;amp;repoURL.&amp;quot;,
  &amp;quot;&amp;amp;targetDIR.&amp;quot;,
  &amp;quot;&amp;amp;sshUSER.&amp;quot;,
  &amp;quot;&amp;amp;sshPASSW.&amp;quot;,
  &amp;quot;&amp;amp;sshPUBkey.&amp;quot;,
  &amp;quot;&amp;amp;sshPRIVkey.&amp;quot;);
 put rc=;
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;pull-ing-in-updates&#34;&gt;Pull-ing in Updates&lt;/h2&gt;
&lt;p&gt;It is just as easy to pull in updates to a local repository by using
&lt;code&gt;gitfn_pull(&amp;quot;&amp;amp;repoDIR.&amp;quot;)&lt;/code&gt;. This also works with SSH keys for private
repositories:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SAS&#34;&gt;data _null_;
 rc = gitfn_pull(
  &amp;quot;&amp;amp;repoDIR.&amp;quot;,
  &amp;quot;&amp;amp;sshUSER.&amp;quot;,
  &amp;quot;&amp;amp;sshPASSW.&amp;quot;,
  &amp;quot;&amp;amp;sshPUBkey.&amp;quot;,
  &amp;quot;&amp;amp;sshPRIVkey.&amp;quot;);
run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;other-functions&#34;&gt;Other Functions&lt;/h2&gt;
&lt;p&gt;SAS also offers other built-in functions, such as &lt;code&gt;_diff&lt;/code&gt;, &lt;code&gt;_status&lt;/code&gt;, &lt;code&gt;_push&lt;/code&gt;,
&lt;code&gt;_commit&lt;/code&gt;, and others. For a complete list, see the SAS documentation 
&lt;a href=&#34;https://go.documentation.sas.com/?cdcId=pgmsascdc&amp;amp;cdcVersion=9.4_3.5&amp;amp;docsetId=lefunctionsref&amp;amp;docsetTarget=n1mlc3f9w9zh9fn13qswiq6hrta0.htm&amp;amp;locale=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>North Carolina Housing Data</title>
      <link>https://dmsenter89.github.io/post/20-11-north-carolina-housing/</link>
      <pubDate>Fri, 06 Nov 2020 10:10:01 -0500</pubDate>
      <guid>https://dmsenter89.github.io/post/20-11-north-carolina-housing/</guid>
      <description>&lt;p&gt;A popular beginners machine learning problem is the prediction of housing prices. A frequently used data set for this purpose uses housing prices in California along some additional  gathered through the 1990 Census. One such data set is available 
&lt;a href=&#34;https://www.kaggle.com/camnugent/california-housing-prices&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; at Kaggle. Unfortunately, that data set is rather old. And I live in North Carolina, not California! So I figured I might as well create a new housing data set, but this time with more up-to-date information and using North Carolina as the state to be analyzed. One thing that may be interesting about North Carlina as compared to California is the position of major populations centers. In California, major population centers are near the beach, while major population centers in North Carolina are in the interior of the state. Both large citites and proximity to the beach tend to correlate with higher housing prices. In California, unlike in North Carolina, both of these go together.&lt;/p&gt;
&lt;p&gt;This post will describe the Kaggle data set with California housing prices and then walk you through how the relevant data can be acquired from the Census Bureau. I&amp;rsquo;ll also show how to clean the data. For those who just want to explore the complete data set, I have made it available for download &lt;a href=&#34;https://dmsenter89.github.io/files/NC_Housing_Prices_2018.csv&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#the-source-data-set&#34;&gt;The Source Data Set&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acquiring-the-census-data-set&#34;&gt;Acquiring the Census Data Set&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#census-variables&#34;&gt;Census Variables&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#geography-considerations&#34;&gt;Geography Considerations&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acquiring-location-data&#34;&gt;Acquiring Location Data&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-merge-with-geoid-matching&#34;&gt;Data Merge with GEOID Matching&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#data-cleaning&#34;&gt;Data Cleaning&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;the-source-data-set&#34;&gt;The Source Data Set&lt;/h2&gt;
&lt;p&gt;The geographic unit of the Kaggle data set is the Census block group, which means we will have several thousand data points for our analysis. For a good big-picture overview of Census geography divisions, see this 
&lt;a href=&#34;https://pitt.libguides.com/uscensus/understandinggeography&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt; from the University of Pittsburgh library. The data set&amp;rsquo;s ten columns contain geographic, housing, and Census information that can be broken down as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;geographic information
&lt;ul&gt;
&lt;li&gt;longitude&lt;/li&gt;
&lt;li&gt;latitude&lt;/li&gt;
&lt;li&gt;ocean proximity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;housing information
&lt;ul&gt;
&lt;li&gt;median age of homes&lt;/li&gt;
&lt;li&gt;median value of homes&lt;/li&gt;
&lt;li&gt;total number of rooms in area&lt;/li&gt;
&lt;li&gt;total number of bedrooms in the area&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Census information
&lt;ul&gt;
&lt;li&gt;population&lt;/li&gt;
&lt;li&gt;number of households&lt;/li&gt;
&lt;li&gt;median income&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most of these exist directly in the Census API data that we have 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;covered previously&lt;/a&gt;. The ocean proximity variable is a categorical giving approximate distance from the beach. My data set will not include this last categorical variable.&lt;/p&gt;
&lt;h2 id=&#34;acquiring-the-census-data-set&#34;&gt;Acquiring the Census Data Set&lt;/h2&gt;
&lt;h3 id=&#34;census-variables&#34;&gt;Census Variables&lt;/h3&gt;
&lt;p&gt;The first, and most time consuming aspect, is to figure out where the data we want is located. We know that the US has a decennial census, so accurate information is available every ten years at every level of geography that the Census Bureau tracks. Since it is currently a census year 2020 and the newest information hasn&amp;rsquo;t been tabulated yet, that means the last census count that is available is from 2010. While this is 20 years more current than the California set from 1990, it still seems a bit outdated. Luckily, since the introduction of the American Community Survey (ACS) we have annually updated information available - but not for every level of geography. Only the 5-year ACS average gives us census block-level information for the whole state, making it comparable to the Kaggle data set. The most recent of these is the 2018.&lt;/p&gt;
&lt;p&gt;I start by creating a data dictionary from the 
&lt;a href=&#34;https://api.census.gov/data/2018/acs/acs5/groups.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;groups&lt;/a&gt; and 
&lt;a href=&#34;https://api.census.gov/data/2018/acs/acs5/variables.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;variables&lt;/a&gt; pages of the &amp;ldquo;American Community Survey: 1-Year Estimates: Detailed Tables 5-Year&amp;rdquo; data set. Note that median home age is not directly available. Instead, we will use the median year structures were built to calculate the median home age. Our data dictionary also does not include any data for the longitude and latitude of each row. We will get that data separately.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dictionary = {
    &#39;B01001_001E&#39; : &amp;quot;population&amp;quot;,
    &#39;B11001_001E&#39; : &amp;quot;households&amp;quot;,
    &#39;B19013_001E&#39; : &amp;quot;median_income&amp;quot;, 
    &#39;B25077_001E&#39; : &amp;quot;median_house_value&amp;quot;,
    &#39;B25035_001E&#39; : &amp;quot;median_year_structure_built&amp;quot;,
    &#39;B25041_001E&#39; : &amp;quot;total_bedrooms&amp;quot;,
    &#39;B25017_001E&#39; : &amp;quot;total_rooms&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;geography-considerations&#34;&gt;Geography Considerations&lt;/h3&gt;
&lt;p&gt;The next step is figuring out exactly what level of geography we want. Our data set goes down to the Census block level at its most granular. Unfortunately, the Census API won&amp;rsquo;t let us pull the data for all the Census blocks in a state at once. Census tracts on the other hand can be acquired in one go. If we were to shortcut and use only tract data, this would be a pretty quick API call build:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;primary_geo = &amp;quot;tract:*&amp;quot;
secondary_geo = &amp;quot;state:37&amp;quot;
query = base_URL + &amp;quot;?get=&amp;quot; + &amp;quot;,&amp;quot;.join(data_dictionary.keys()) + f&amp;quot;&amp;amp;for={primary_geo}&amp;amp;in={secondary_geo}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But let&amp;rsquo;s try and do it for the Census blocks instead. This will require us to build a sequence of API calls that loops over a larger geographic area, say the different counties in the state, and pull in the respective census block data for that geographic unit. While the FIPS codes for the state counties are sorted alphabetically, they are not contiguous. A full listing of North Carolina county FIPS codes is availalbe 
&lt;a href=&#34;https://www.lib.ncsu.edu/gis/countyfips&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;from NCSU here&lt;/a&gt;. It appears to be that the county FIPS codes are three digits long, starting at &lt;code&gt;001&lt;/code&gt; and go up to &lt;code&gt;199&lt;/code&gt; in increments of 2, meaning only odd numbers are in the county set. So it looks like we will be using &lt;code&gt;range(1,200,2)&lt;/code&gt; with zero-padding to create the list of county FIPS codes. So we could use a loop similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vars_requested = &amp;quot;,&amp;quot;.join(data_dictionary.keys())

for i in range(1,200,2):
    geo_request = f&amp;quot;for=block%20group:*&amp;amp;in=state:37%20county:{i:03}&amp;quot;
    query = base_URL + f&amp;quot;?get={vars_requested}&amp;amp;{geo_request}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While practicing to write the appropriate API call, you may find it useful to give it frequent, quick tests using curl. If you are using Jupyter or IPython, you can use &lt;code&gt;!curl &amp;quot;{query}&amp;quot;&lt;/code&gt; to test your API query. Don&amp;rsquo;t forget the quotation marks, since the ampersand has special meaning in the shell. It may be helpful to test the output of your call at the county or city level with that reported on the 
&lt;a href=&#34;https://www.census.gov/quickfacts/fact/table/US/PST045219&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Census Quickfacts page&lt;/a&gt;, if your variable is listed there. This can help make sure you are pulling the data you actually want.&lt;/p&gt;
&lt;p&gt;Now that we have figured out the loop necessary for creation of the API calls, we can put everything together and create a list of Pandas DataFrames which we then concatenate to create our master list.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import requests

# create the base-URL
host_name = &amp;quot;https://api.census.gov/data&amp;quot;
year = &amp;quot;2018&amp;quot;
dataset_name = &amp;quot;acs/acs5&amp;quot;
base_URL = f&amp;quot;{host_name}/{year}/{dataset_name}&amp;quot;

# build the api calls as a list
query_vars = base_URL + &amp;quot;?get=&amp;quot; + &amp;quot;,&amp;quot;.join(list(data_dictionary.keys()) + [&amp;quot;NAME&amp;quot;,&amp;quot;GEO_ID&amp;quot;])
api_calls = [query_vars + f&amp;quot;&amp;amp;for=block%20group:*&amp;amp;in=state:37%20county:{i:03}&amp;quot; for i in range(1,200,2) ]

# running the API calls will take a moment
rjson_list = [requests.get(call).json() for call in api_calls]

# create the data frame by concatenation
df_list = [pd.DataFrame(data[1:], columns=data[0]) for data in rjson_list]
df = pd.concat(df_list, ignore_index=True)

# save the raw output to disk
df.to_csv(&amp;quot;raw_census.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we have the data set! We do still have to address the issue of our values all being imported as strings as mentioned in my 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;Census API post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;acquiring-location-data&#34;&gt;Acquiring Location Data&lt;/h2&gt;
&lt;p&gt;As mentioned above, we are still missing information regarding the latitude and longitude of the different block groups. The Census Bureau makes a lot of geographically coded data available on its 
&lt;a href=&#34;https://tigerweb.geo.census.gov/tigerwebmain/TIGERweb_main.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TIGERweb&lt;/a&gt; page. You can interact with it both using a REST API and its web-interface. A page with map information exists 
&lt;a href=&#34;https://tigerweb.geo.census.gov/arcgis/rest/services/Generalized_ACS2018/Tracts_Blocks/MapServer/4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Dealing with shapefiles and the TIGERweb API can get a little complicated. Luckily, I know someone with expertise in GIS and shapefiles so we will be using a CSV file of the geographic data we need courtesy of 
&lt;a href=&#34;https://www.linkedin.com/in/summer-faircloth-652797137&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Summer Faircloth&lt;/a&gt;, a GIS intern at the North Carolina Department of Transportation. She downloaded the TIGER/Line Shapefiles for the 20189 ACS 
&lt;a href=&#34;https://catalog.data.gov/dataset/tiger-line-shapefile-2018-state-north-carolina-current-block-group-state-based&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Block Groups&lt;/a&gt; and 
&lt;a href=&#34;https://catalog.data.gov/dataset/tiger-line-shapefile-2018-state-north-carolina-current-census-tract-state-based&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Census Tracts&lt;/a&gt; and joined the data sets in ArcMap, from where she exported our CSV file, which is now &lt;a href=&#34;https://dmsenter89.github.io/files/BlockGroup_Tract2018.csv&#34; target=&#34;_blank&#34;&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t need all of the columns in the CSV file, so we will limit the import to the parts we need with the &lt;code&gt;usecols&lt;/code&gt; keyword.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&amp;quot;raw_census.csv&amp;quot;, dtype={})
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;shapedata = pd.read_csv(&amp;quot;BlockGroup_Tract2018.csv&amp;quot;, 
                        dtype={&amp;quot;GEOID&amp;quot;: str},
                        usecols=[&#39;GEOID&#39;,&#39;NAMELSAD&#39;,&#39;INTPTLAT&#39;,&#39;INTPTLON&#39;,&#39;NAMELSAD_1&#39;] )

shapedata = shapedata.rename(columns={&#39;INTPTLAT&#39; : &#39;latitude&#39;, &#39;INTPTLON&#39; : &#39;longitude&#39; })
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-merge-with-geoid-matching&#34;&gt;Data Merge with GEOID Matching&lt;/h2&gt;
&lt;p&gt;At this stage we have two data frames - the first consists of all the Census information sans the geographic coordinates of the block groups, and a second data set containing the block groups&#39; location. Both data sets contain a GEOID column that can be used for merging. The GEOID returned by the Census API includes additional information to the regular FIPS code based GEOID used in the TIGERweb system. For example, &amp;ldquo;1500000US370010204005&amp;rdquo; in the census data set is actually GEOID &amp;ldquo;370010204005&amp;rdquo; for purposes of the TIGERweb data set. We&amp;rsquo;ll use a string split to make our GEO_ID variable from the Census API compatible with the FIPS code based GEOID from the TIGERweb service.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;GEO_ID&amp;quot;] = df[&amp;quot;GEO_ID&amp;quot;].str.split(&#39;US&#39;).str[1]

df = df.merge(shapedata, left_on=&#39;GEO_ID&#39;, right_on=&amp;quot;GEOID&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-cleaning&#34;&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;Now that our data set has been assembled, we can work on cleaning up the merged data set. We have the following tasks left:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;convert column data types to numeric&lt;/li&gt;
&lt;li&gt;drop unnecessary columns&lt;/li&gt;
&lt;li&gt;rename columns&lt;/li&gt;
&lt;li&gt;handle missing values&lt;/li&gt;
&lt;li&gt;calculate median age of homes&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for col in data_dictionary.keys():
    if col not in [&amp;quot;NAME&amp;quot;, &amp;quot;GEO_ID&amp;quot;]:
        df[col] = pd.to_numeric(df[col])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To indicate missing values, the Census API returns a value of &amp;ldquo;-666666666&amp;rdquo; in numeric columns. As all of our variables - except for longitude - ought to be positive, we can use the &lt;code&gt;mask&lt;/code&gt; function to convert all negative values to missing. We&amp;rsquo;ll start by filtering out the string columns that are no longer necessary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# filter down to our numerical columns
keeps = list(data_dictionary.keys()) +[&amp;quot;latitude&amp;quot;, &amp;quot;longitude&amp;quot;]
df = df.filter(items=keeps)

# replace vals &amp;lt; 0 with missing
k = df.loc[:, df.columns != &#39;longitude&#39;]
k = k.mask(k &amp;lt; 0)
df.loc[:, df.columns != &#39;longitude&#39;] = k
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the missing values have been handled, we can go ahead and calculate our median home age.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.rename(columns=data_dictionary, inplace=True)
df[&amp;quot;housing_median_age&amp;quot;] = 2018 - df[&amp;quot;median_year_structure_built&amp;quot;]
df.drop(columns=&amp;quot;median_year_structure_built&amp;quot;, inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we&amp;rsquo;re done! We will save our output data set to disk for future analysis in a different post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.to_csv(&amp;quot;NC_Housing_Prices_2018.csv&amp;quot;, index=False)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Teacher Salaries</title>
      <link>https://dmsenter89.github.io/post/20-10-tabula/</link>
      <pubDate>Thu, 29 Oct 2020 22:39:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/20-10-tabula/</guid>
      <description>&lt;p&gt;After reading a news article about teacher pay in the US, I was curious and wanted to look into the source data myself. Unfortunately, the source that was mentioned was a publication by the 
&lt;a href=&#34;https://www.nea.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Education Association (NEA)&lt;/a&gt; which had the data as tables embedded inside a PDF report. As those who know me can attest, I don&amp;rsquo;t like hand-copying data. It is slow and error-prone. Instead, I decided to use the tabula package to extract the information from the PDFs directly into a Pandas dataframe. In this post, I will show you how to extract the data and how to clean it up for analysis.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#the-data-source&#34;&gt;The Data Source&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#loading-the-data&#34;&gt;Loading the Data&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#cleaning-the-data&#34;&gt;Cleaning the Data&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#numeric-conversion&#34;&gt;Numeric Conversion&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#table-b-6&#34;&gt;Table B-6&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;the-data-source&#34;&gt;The Data Source&lt;/h2&gt;
&lt;p&gt;Several years worth of data are available in PDF form on the 
&lt;a href=&#34;https://www.nea.org/research-publications&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NEA website&lt;/a&gt;. Reading through the technical notes, they highlight that they did not collect all of their own salary information. Some states&#39; information is calculated from the American Community Survey (ACS) done by the Census Bureau - a great resource whose API I have covered in a 
&lt;a href=&#34;https://dmsenter89.github.io/post/20-08-census-api/&#34;&gt;different post&lt;/a&gt;. Each report includes accurate data for the previous school year, as well as estimates for the current school year. As of this post, the newest report is the 2020 report which includes data for the the 2018-2019 school year, as well as estimates of the 2019-2020 school year.&lt;/p&gt;
&lt;p&gt;The 2020 report has the desired teacher salary information in two separate locations. One is in table B-6 on page 26 of the PDF, which shows a ranking of the different states&#39; average salary in addition to the average salary:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table_B-6.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;A second location is in table E-7 on page 46, which gives salary data for the completed school year as well as different states&#39; estimates for the 2019-2020 school year:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table_E-7.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Note that table E-7 lacks the star-annotation marking NEA estimated values. This, and the lack of the ranking column, makes Table E-7 easier to parse. In the main example below, this will be the source of the five years of data. I will however also show how to parse table B-6 at the end of this post for completion.&lt;/p&gt;
&lt;h2 id=&#34;loading-the-data&#34;&gt;Loading the Data&lt;/h2&gt;
&lt;p&gt;As of October 2020, the NEA site has five years worth of reports online. Unfortunately, these are not labeled consistently for all five years. Similarly the page numbers differ for each report. Prior to the 2018 report, inconsistent formats were used for the tables which require previous years to be parsed separately from the newer tables. For this reason, I&amp;rsquo;ll make a dictionary for the 2018-2020 reports only, which will simplify the example below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;report = {
    &#39;2020&#39; : {
        &#39;url&#39;  : &amp;quot;https://www.nea.org/sites/default/files/2020-10/2020%20Rankings%20and%20Estimates%20Report.pdf&amp;quot;,
        &#39;page&#39; : 46,
    },
    &#39;2019&#39; : {
        &#39;url&#39;  : &amp;quot;https://www.nea.org/sites/default/files/2020-06/2019%20Rankings%20and%20Estimates%20Report.pdf&amp;quot;,
        &#39;page&#39; : 49,
    },
    &#39;2018&#39; : {
        &#39;url&#39;  : &amp;quot;https://www.nea.org/sites/default/files/2020-07/180413-Rankings_And_Estimates_Report_2018.pdf&amp;quot;,
        &#39;page&#39; : 51,
    },
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now use dictionary comprehension to fill in a dictionary with all the source tables of interest. We will be using the tabula package to extract data from the PDFs. If you don&amp;rsquo;t have it installed, you can use &lt;code&gt;pip install tabula-py&lt;/code&gt; to get a copy. The method that reads in a PDF is aptly called &lt;code&gt;read_pdf&lt;/code&gt;. Its first argument is a file path to the PDF. Since we want to use a URL, we will use the keyword argument &lt;code&gt;stream=True&lt;/code&gt; and then name the specific page in each PDF that contains the information we are after. By default, &lt;code&gt;read_pdf&lt;/code&gt; returns a list of dataframes, so we just save the first element from the list, which is the report we are interested in.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; if you are using WSL, depending on your settings, you may get the error &lt;code&gt;Exception in thread &amp;quot;main&amp;quot; java.awt.AWTError: Can&#39;t connect to X11 window server using &#39;XXX.XXX.XXX.XXX:0&#39; as the value of the DISPLAY variable.&lt;/code&gt; error when running &lt;code&gt;read_pdf&lt;/code&gt;. This is fixed by having an X11 server running.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tabula
import pandas as pd

source_df = {year : tabula.read_pdf(report[year][&#39;url&#39;], stream=True, pages=report[year][&#39;page&#39;])[0] 
             for year in report.keys()}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it in principle. How cool is that! Of course, we still need to clean our data a little bit.&lt;/p&gt;
&lt;h3 id=&#34;cleaning-the-data&#34;&gt;Cleaning the Data&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at the first and last few entries of the 2020 report:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.concat([source_df[&#39;2020&#39;].head(), 
           source_df[&#39;2020&#39;].tail()])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;2018-19&lt;/th&gt;
      &lt;th&gt;2019-20&lt;/th&gt;
      &lt;th&gt;From 2018-19 to 2019-20&lt;/th&gt;
      &lt;th&gt;From 2010-11 to 2019-20 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;State&lt;/td&gt;
      &lt;td&gt;Salary($)&lt;/td&gt;
      &lt;td&gt;Salary($)&lt;/td&gt;
      &lt;td&gt;Change(%)&lt;/td&gt;
      &lt;td&gt;Current Dollar Constant Dollar&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
      &lt;td&gt;54,095&lt;/td&gt;
      &lt;td&gt;4.01&lt;/td&gt;
      &lt;td&gt;13.16 -2.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
      &lt;td&gt;70,877&lt;/td&gt;
      &lt;td&gt;0.85&lt;/td&gt;
      &lt;td&gt;15.36 -0.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
      &lt;td&gt;50,381&lt;/td&gt;
      &lt;td&gt;0.06&lt;/td&gt;
      &lt;td&gt;8.03 -7.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
      &lt;td&gt;49,822&lt;/td&gt;
      &lt;td&gt;0.78&lt;/td&gt;
      &lt;td&gt;8.31 -6.75&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;48&lt;/th&gt;
      &lt;td&gt;Washington&lt;/td&gt;
      &lt;td&gt;73,049&lt;/td&gt;
      &lt;td&gt;72,965&lt;/td&gt;
      &lt;td&gt;-0.11&lt;/td&gt;
      &lt;td&gt;37.86 18.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;49&lt;/th&gt;
      &lt;td&gt;West Virginia&lt;/td&gt;
      &lt;td&gt;47,681&lt;/td&gt;
      &lt;td&gt;50,238&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;13.51 -2.28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50&lt;/th&gt;
      &lt;td&gt;Wisconsin&lt;/td&gt;
      &lt;td&gt;58,277&lt;/td&gt;
      &lt;td&gt;59,176&lt;/td&gt;
      &lt;td&gt;1.54&lt;/td&gt;
      &lt;td&gt;9.17 -6.02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;51&lt;/th&gt;
      &lt;td&gt;Wyoming&lt;/td&gt;
      &lt;td&gt;58,861&lt;/td&gt;
      &lt;td&gt;59,014&lt;/td&gt;
      &lt;td&gt;0.26&lt;/td&gt;
      &lt;td&gt;5.19 -9.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;52&lt;/th&gt;
      &lt;td&gt;United States&lt;/td&gt;
      &lt;td&gt;62,304&lt;/td&gt;
      &lt;td&gt;63,645&lt;/td&gt;
      &lt;td&gt;2.15&lt;/td&gt;
      &lt;td&gt;14.14 -1.73&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We see that each column is treated as a string object (which you can confirm by running &lt;code&gt;source_df[&#39;2020&#39;].dtypes&lt;/code&gt;) and that the first row of data is actually at index 1 due to the fact that the PDF report used a two-row header. This means we can safely drop the first row of every dataframe. We can also drop the last row of every dataframe since that just contains summary data of the US as a whole, which we can easily regenerate as necessary. So row indices 0 and 52 can go for all of our data sets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for df in source_df.values():
    df.drop([0, 52], inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next up I&amp;rsquo;d like to fix the column names. The fist column is clearly the name of the state (except in the case of Washington D.C.), while the next two columns give the years for which the salary information is given. Let&amp;rsquo;s rename the second and third columns according to the pattern &lt;code&gt;Salary %YYYY-YY&lt;/code&gt; using Python&amp;rsquo;s f-string syntax.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for df in source_df.values():
    df.rename(columns={
        df.columns[0] : &amp;quot;State&amp;quot;,
        df.columns[1] : f&amp;quot;Salary {str(df.columns[1])}&amp;quot;,
        df.columns[2] : f&amp;quot;Salary {str(df.columns[2])}&amp;quot;,
    }, 
              inplace=True)
    
source_df[&amp;quot;2020&amp;quot;].head()  # show the result of our edits so far
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Salary 2018-19&lt;/th&gt;
      &lt;th&gt;Salary 2019-20&lt;/th&gt;
      &lt;th&gt;From 2018-19 to 2019-20&lt;/th&gt;
      &lt;th&gt;From 2010-11 to 2019-20 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
      &lt;td&gt;54,095&lt;/td&gt;
      &lt;td&gt;4.01&lt;/td&gt;
      &lt;td&gt;13.16 -2.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
      &lt;td&gt;70,877&lt;/td&gt;
      &lt;td&gt;0.85&lt;/td&gt;
      &lt;td&gt;15.36 -0.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
      &lt;td&gt;50,381&lt;/td&gt;
      &lt;td&gt;0.06&lt;/td&gt;
      &lt;td&gt;8.03 -7.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
      &lt;td&gt;49,822&lt;/td&gt;
      &lt;td&gt;0.78&lt;/td&gt;
      &lt;td&gt;8.31 -6.75&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;83,059&lt;/td&gt;
      &lt;td&gt;84,659&lt;/td&gt;
      &lt;td&gt;1.93&lt;/td&gt;
      &lt;td&gt;24.74 7.39&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Looks like we&amp;rsquo;re almost done! Let&amp;rsquo;s drop the unnecessary columns and check our remaining column names:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for year, df in source_df.items():
    df.drop(df.columns[3:], axis=1, inplace=True)
    print(f&amp;quot;{year}:\t{df.columns}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020:	Index([&#39;State&#39;, &#39;Salary 2018-19&#39;, &#39;Salary 2019-20&#39;], dtype=&#39;object&#39;)
2019:	Index([&#39;State&#39;, &#39;Salary 2017-18&#39;, &#39;Salary 2018-19&#39;], dtype=&#39;object&#39;)
2018:	Index([&#39;State&#39;, &#39;Salary 2017&#39;, &#39;Salary 2018&#39;], dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the column naming scheme in 2018 was different than in the previous reports. To make them all compatible for our merge, we&amp;rsquo;re going to have to do some more editing. Based on the other reports, it appears as though the 2018 report used the calendar year of the &lt;em&gt;end&lt;/em&gt; of the school year, while the others utilized a range. This can easily be solved using regex substitution. We&amp;rsquo;ll do that now.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import re

for year, df in source_df.items():
    if year != &amp;quot;2018&amp;quot;:
        df.rename(columns={
            df.columns[1] : re.sub(r&amp;quot;\d{2}-&amp;quot;, &#39;&#39;, df.columns[1]),
            df.columns[2] : re.sub(r&amp;quot;\d{2}-&amp;quot;, &#39;&#39;, df.columns[2]),
        }, 
                  inplace=True)
    # print the output for verification
    print(f&amp;quot;{year}:\t{df.columns}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020:	Index([&#39;State&#39;, &#39;Salary 2019&#39;, &#39;Salary 2020&#39;], dtype=&#39;object&#39;)
2019:	Index([&#39;State&#39;, &#39;Salary 2018&#39;, &#39;Salary 2019&#39;], dtype=&#39;object&#39;)
2018:	Index([&#39;State&#39;, &#39;Salary 2017&#39;, &#39;Salary 2018&#39;], dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that everything works, we can do our merge to create a single dataframe with the information for all of the school years we have downloaded.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge_df = source_df[&amp;quot;2018&amp;quot;].drop([&amp;quot;Salary 2018&amp;quot;], axis=1).merge(
                    source_df[&amp;quot;2019&amp;quot;].drop([&amp;quot;Salary 2019&amp;quot;], axis=1)).merge(
                    source_df[&amp;quot;2020&amp;quot;])

merge_df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Salary 2017&lt;/th&gt;
      &lt;th&gt;Salary 2018&lt;/th&gt;
      &lt;th&gt;Salary 2019&lt;/th&gt;
      &lt;th&gt;Salary 2020&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;50,391&lt;/td&gt;
      &lt;td&gt;50,568&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
      &lt;td&gt;54,095&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;6 8,138&lt;/td&gt;
      &lt;td&gt;69,682&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
      &lt;td&gt;70,877&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;4 7,403&lt;/td&gt;
      &lt;td&gt;48,723&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
      &lt;td&gt;50,381&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;4 8,304&lt;/td&gt;
      &lt;td&gt;50,544&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
      &lt;td&gt;49,822&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;7 9,128&lt;/td&gt;
      &lt;td&gt;80,680&lt;/td&gt;
      &lt;td&gt;83,059&lt;/td&gt;
      &lt;td&gt;84,659&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;numeric-conversion&#34;&gt;Numeric Conversion&lt;/h3&gt;
&lt;p&gt;We&amp;rsquo;re almost done! Notice that we still have not dealt with the fact that every column is still treated as a string. Before we can use the &lt;code&gt;to_numeric&lt;/code&gt; function, we still need to take care of two issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The commas in the numbers. While they are nice for our human eyes, Pandas doesn&amp;rsquo;t like them.&lt;/li&gt;
&lt;li&gt;In the 2017 salary column, there appears to be extraneous white space after the first digit for some entries.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Luckily, both of these problems can be remedied with a simple string replacement operation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge_df.iloc[:,1:] = merge_df.iloc[:,1:].replace(r&amp;quot;[,| ]&amp;quot;, &#39;&#39;, regex=True)

for col in merge_df.columns[1:]:
    merge_df[col] = pd.to_numeric(merge_df[col])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we&amp;rsquo;re done! We have created an overview of annual teacher salaries from the 2016-17 school year until 2019-20 extracted from a series of PDFs published by the NEA. We have cleaned up the data and converted everything to numerical values. We can now get summary statistics and do any analysis of interest with this data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge_df.describe() # summary stats of our numeric columns
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Salary 2017&lt;/th&gt;
      &lt;th&gt;Salary 2018&lt;/th&gt;
      &lt;th&gt;Salary 2019&lt;/th&gt;
      &lt;th&gt;Salary 2020&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;56536.196078&lt;/td&gt;
      &lt;td&gt;57313.039216&lt;/td&gt;
      &lt;td&gt;58983.254902&lt;/td&gt;
      &lt;td&gt;60170.647059&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;9569.444674&lt;/td&gt;
      &lt;td&gt;9795.914601&lt;/td&gt;
      &lt;td&gt;10286.843230&lt;/td&gt;
      &lt;td&gt;10410.259274&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;42925.000000&lt;/td&gt;
      &lt;td&gt;44926.000000&lt;/td&gt;
      &lt;td&gt;45105.000000&lt;/td&gt;
      &lt;td&gt;45192.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;49985.000000&lt;/td&gt;
      &lt;td&gt;50451.500000&lt;/td&gt;
      &lt;td&gt;51100.500000&lt;/td&gt;
      &lt;td&gt;52441.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;54308.000000&lt;/td&gt;
      &lt;td&gt;53815.000000&lt;/td&gt;
      &lt;td&gt;54935.000000&lt;/td&gt;
      &lt;td&gt;57091.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;61038.000000&lt;/td&gt;
      &lt;td&gt;61853.000000&lt;/td&gt;
      &lt;td&gt;64393.500000&lt;/td&gt;
      &lt;td&gt;66366.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;81902.000000&lt;/td&gt;
      &lt;td&gt;84227.000000&lt;/td&gt;
      &lt;td&gt;85889.000000&lt;/td&gt;
      &lt;td&gt;87543.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;table-b-6&#34;&gt;Table B-6&lt;/h2&gt;
&lt;p&gt;As mentioned above, table B-6 in the 2020 Report presents slightly greater challenges. A lot of the cleaning is similar or identical, so I will not reproduce it in full. Instead, I have loaded a subsetted part of table B-6 and will show how this can be cleaned up as well. But first, let&amp;rsquo;s look at the first several entries:&lt;/p&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;2017-18 (Revised)&lt;/th&gt;
      &lt;th&gt;2018-19&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;State&lt;/td&gt;
      &lt;td&gt;Salary($) Rank&lt;/td&gt;
      &lt;td&gt;Salary($)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;50,568 36&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;69,682 7&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;48,315 45&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,096 44&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;80,680 2&lt;/td&gt;
      &lt;td&gt;83,059 *&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Colorado&lt;/td&gt;
      &lt;td&gt;52,695 32&lt;/td&gt;
      &lt;td&gt;54,935&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Connecticut&lt;/td&gt;
      &lt;td&gt;74,517 * 5&lt;/td&gt;
      &lt;td&gt;76,465 *&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Delaware&lt;/td&gt;
      &lt;td&gt;62,422 13&lt;/td&gt;
      &lt;td&gt;63,662&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can see that there is an additional hurdle compared to the previous tables: the second column now contains data from two columns, both the Salary information as well as a ranking of the salary as it compares to the different states. For a few states, there is additionally a &amp;lsquo;*&amp;rsquo; to denote values that were estimated as opposed to received. We can again use a simple regex replace together with a capture group to parse out only those values that we are interested in, while dropping the extraneous information using the code below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;b6.iloc[:,1:] = b6.iloc[:,1:].replace(r&amp;quot;([\d,]+).*&amp;quot;, r&amp;quot;\1&amp;quot;, regex=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we&amp;rsquo;re back to where we were above before we did the string conversion. This is what it looks like after also dropping the first row and renaming the columns:&lt;/p&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Salary 2018&lt;/th&gt;
      &lt;th&gt;Salary 2019&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;50,568&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;69,682&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;48,315&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,096&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;80,680&lt;/td&gt;
      &lt;td&gt;83,059&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Colorado&lt;/td&gt;
      &lt;td&gt;52,695&lt;/td&gt;
      &lt;td&gt;54,935&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Connecticut&lt;/td&gt;
      &lt;td&gt;74,517&lt;/td&gt;
      &lt;td&gt;76,465&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Delaware&lt;/td&gt;
      &lt;td&gt;62,422&lt;/td&gt;
      &lt;td&gt;63,662&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From here on out, we can proceed as in the previous example.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accessing Census Data via API</title>
      <link>https://dmsenter89.github.io/post/20-08-census-api/</link>
      <pubDate>Sat, 22 Aug 2020 08:53:55 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/20-08-census-api/</guid>
      <description>&lt;p&gt;The Census Bureau makes an incredible amount of data available online. In this post, I will summarize how to get access to this data via Python by using the Census Bureau&amp;rsquo;s API. The Census Bureau makes a pretty useful guide available 
&lt;a href=&#34;https://www.census.gov/data/developers/guidance/api-user-guide.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; - I recommend checking it out.&lt;/p&gt;
 &lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#api-basics&#34;&gt;API Basics&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#building-an-the-base-url&#34;&gt;Building an the Base URL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#building-the-query&#34;&gt;Building the Query&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#the-get-variables&#34;&gt;The &amp;lsquo;Get&amp;rsquo; Variables&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#location-variables&#34;&gt;Location Variables&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-complete-call&#34;&gt;The Complete Call&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#making-the-api-request&#34;&gt;Making the API Request&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#reading-the-json-into-pandas&#34;&gt;Reading the JSON into Pandas&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;api-basics&#34;&gt;API Basics&lt;/h2&gt;
&lt;p&gt;We can think of an API query of consisting of two main parts: a &lt;em&gt;base URL&lt;/em&gt; (also called a root URL) and a &lt;em&gt;query&lt;/em&gt; string. These two strings are joined together with the query character &amp;ldquo;?&amp;rdquo; to create an API call. The resulting API call can in theory be copy-and-pasted into the URL bar of your browser, and I recommend this when first playing around with a new API. Seeing the raw text returned in the browser can help you understand the structure of what is being returned. In the case of the Census Bureau&amp;rsquo;s API, it returns a string that essentially looks like a list of lists from a Python perspective. This can easily be turned into a Pandas dataset. Be aware that all values are returned as strings. You&amp;rsquo;ll have to convert number columns to numeric by yourself.&lt;/p&gt;
&lt;p&gt;To get an overview of all available data sets, you can go to the 
&lt;a href=&#34;https://api.census.gov/data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;data page&lt;/a&gt; which contains a long list of data sets. This data page is incredibly useful because it gives access to all of the information needed to build a correct API call, including the base URLs of all data sets and the variables available in each.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-a-snapshot-of-two-datasets-available-as-part-of-the-2018-american-community-survey-acs&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/20-08-census-api/census_overview_hu401560307af60bd2eb326d2765e64aa0_85308_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;A snapshot of two datasets available as part of the 2018 American Community Survey (ACS).&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/20-08-census-api/census_overview_hu401560307af60bd2eb326d2765e64aa0_85308_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1805&#34; height=&#34;372&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    A snapshot of two datasets available as part of the 2018 American Community Survey (ACS).
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;building-an-the-base-url&#34;&gt;Building an the Base URL&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s build a sample API call for the 2018 American Community Survey 1-Year Detailed Table. While we could just copy the base URL from the data page, I like to assemble mine manually from its component parts. This makes it easier to write a wrapper for the API calls if you plan on scraping the same data from multiple years.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;host_name = &amp;quot;https://api.census.gov/data&amp;quot;
year = &amp;quot;2018&amp;quot;
dataset_name = &amp;quot;acs/acs1&amp;quot;
base_URL = f&amp;quot;{host_name}/{year}/{dataset_name}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;building-the-query&#34;&gt;Building the Query&lt;/h2&gt;
&lt;p&gt;Now that we have the base URL, we can work on building the query. For purposes of the Census Bureau, you will need two components: the variables of interest, which are listed after the &lt;code&gt;get=&lt;/code&gt; keyword, and the geography for which you would like the data listed after the &lt;code&gt;for=&lt;/code&gt; keyword. For certain subdivisions, like counties, you can specify two levels of geography by adding an &lt;code&gt;in=&lt;/code&gt; keyword at at the end.&lt;/p&gt;
&lt;h3 id=&#34;the-get-variables&#34;&gt;The &amp;lsquo;Get&amp;rsquo; Variables&lt;/h3&gt;
&lt;p&gt;Since many of the data sets have a large amount of variables in them, it often makes sense to take a look at the &amp;ldquo;groups&amp;rdquo; page first. This page lists variables as groups, giving you a better overview of what data is available. This page is available at &lt;code&gt;{base_URL}/groups.html&lt;/code&gt;. A complete list of all variables in the data set is available at &lt;code&gt;{base_URL}/variables.html&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s find some variables. The most basic variable we&amp;rsquo;d expect to find here is total population. We can find this variable in group &amp;ldquo;B01003&amp;rdquo;. The total estimate is in sub-variable &amp;ldquo;001E&amp;rdquo;, meaning that the variable for total population is &amp;ldquo;B01003_001E&amp;rdquo;. Let&amp;rsquo;s also get household income (group &amp;ldquo;B19001&amp;rdquo;) not broken down by race: &amp;ldquo;B19001_001E&amp;rdquo;. There is also median monthly housing cost (group B25105) with variable &amp;ldquo;B25105_001E&amp;rdquo;. Since the variable names can be a little difficult to parse, I recommend making a data dictionary as you prepare the list of variables to fetch.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_dictionary = {
    &amp;quot;B01003_001E&amp;quot; : &amp;quot;Total Population&amp;quot;,
    &amp;quot;B19001_001E&amp;quot; : &amp;quot;Household Income (12 Month)&amp;quot;,
    &amp;quot;B25105_001E&amp;quot; : &amp;quot;Median Monthly Housing Cost&amp;quot;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This way, the list of variables can easily be created from the data dictionary:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;get_vars = &#39;,&#39;.join(data_dictionary.keys())
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;location-variables&#34;&gt;Location Variables&lt;/h3&gt;
&lt;p&gt;Which geographic variables are available for a particular data set can be found &lt;code&gt;{base_URL}/geography.html&lt;/code&gt;. The Census Bureau uses FIPS codes to reference the different geographies. To find the relevant codes, see 
&lt;a href=&#34;https://www.census.gov/geographies/reference-files.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Delaware for example has FIPS code 10 while North Carolina is 37. So to get information for these two states, we&amp;rsquo;d use &lt;code&gt;for=state:10,37&lt;/code&gt;. You can also use &amp;lsquo;*&amp;rsquo; as a wildcard. So to get all the states&#39; info you&amp;rsquo;d write &lt;code&gt;for=state:*&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Subdivisions for similarly. To get information for Orange County (FIPS 135) in North Carolina (FIPS 37), you could write &lt;code&gt;for=county:135&lt;/code&gt; with the keyword &lt;code&gt;in=state:37&lt;/code&gt;. Let&amp;rsquo;s get the information for Orange and Alamance counties in North Carolina.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;county_dict = {
    &amp;quot;001&amp;quot; : &amp;quot;Alamance County&amp;quot;,
    &amp;quot;135&amp;quot; : &amp;quot;Orange County&amp;quot;,
}
county_fips = &#39;,&#39;.join(county_dict.keys())

state_dict = {&amp;quot;37&amp;quot; : &amp;quot;North Carolina&amp;quot;}
state_fips = &#39;,&#39;.join(state_dict.keys())

query_str = f&amp;quot;get={get_vars}&amp;amp;for=county:{county_fips}&amp;amp;in=state:{state_fips}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-complete-call&#34;&gt;The Complete Call&lt;/h3&gt;
&lt;p&gt;The complete API call can now be easily assembled from the previous two pieces:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;api_call = base_URL + &amp;quot;?&amp;quot; + query_str
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we copy-and-paste this output into our browser, we can see the result looks as follows:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-the-result-of-our-sample-api-query&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://dmsenter89.github.io/post/20-08-census-api/API_return_hu0ee03133ff661cb0ce10ab25f3a6aced_3573_2000x2000_fit_lanczos_2.PNG&#34; data-caption=&#34;The result of our sample API query.&#34;&gt;


  &lt;img data-src=&#34;https://dmsenter89.github.io/post/20-08-census-api/API_return_hu0ee03133ff661cb0ce10ab25f3a6aced_3573_2000x2000_fit_lanczos_2.PNG&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;454&#34; height=&#34;55&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The result of our sample API query.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;making-the-api-request&#34;&gt;Making the API Request&lt;/h2&gt;
&lt;p&gt;We can make the API request with Python&amp;rsquo;s requests package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import requests

r = requests.get(api_call)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it! We now have the response we wanted. To interpret the response as JSON, we would call the json method of the response object: &lt;code&gt;r.json()&lt;/code&gt;. The result can then be fed into Pandas to generate our data set.&lt;/p&gt;
&lt;h2 id=&#34;reading-the-json-into-pandas&#34;&gt;Reading the JSON into Pandas&lt;/h2&gt;
&lt;p&gt;We can use Pandas&#39; DataFrame method directly on our data, making sure to specify that the first row consists of column headers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import pandas as pd

data = r.json()

df = pd.DataFrame(data[1:], columns=data[0])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then do any renaming based on the dictionaries we have created previously.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.rename(columns=data_dictionary, inplace=True)
df[&#39;county&#39;] = df[&#39;county&#39;].replace(county_dict)
df[&#39;state&#39;]  = df[&#39;state&#39;].replace(state_dict)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last step is to make sure our numeric columns are interpreted as such. Since all of the requested variables are in fact numeric,
we can use the dictionary of variables to convert what we need to numeric variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for col in data_dictionary.values():
    df[col] = pd.to_numeric(df[col])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it! We&amp;rsquo;re now ready to work with our data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A semi-automated finite difference mesh creation method for use with immersed boundary software IB2d and IBAMR</title>
      <link>https://dmsenter89.github.io/publication/senter-2020-meshmerizeme/</link>
      <pubDate>Mon, 03 Aug 2020 11:27:55 -0400</pubDate>
      <guid>https://dmsenter89.github.io/publication/senter-2020-meshmerizeme/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Basics of Web Scraping with Python</title>
      <link>https://dmsenter89.github.io/talk/webscraping-tutorial/</link>
      <pubDate>Thu, 30 Jul 2020 13:00:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/talk/webscraping-tutorial/</guid>
      <description>&lt;p&gt;This workshop covers data acquisition and basic data preparation with a focus on using Python with Jupyter Notebooks. To avoid having to install Python locally during the workshop, we will be utilizing an 
&lt;a href=&#34;https://notebooks.azure.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azure notebook&lt;/a&gt; project. The example files are located 
&lt;a href=&#34;https://notebooks.azure.com/dmsenter/projects/datacollectiontutorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Please note that the free Azure notebooks will only be available until early October. To continue using Python and Jupyter notebooks, you may want to consider using a local installation. For Windows and Mac users, I recommend using 
&lt;a href=&#34;https://www.anaconda.com/products/individual&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anaconda&lt;/a&gt;. For continued cloud usage, you may consider 
&lt;a href=&#34;https://cocalc.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cocalc&lt;/a&gt;. Please note that you will need a subscription for your Cocalc notebooks to be able to download data from external sources.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Additional Links:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://markummitchell.github.io/engauge-digitizer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Engauge Digitizer&lt;/a&gt; (software to extract data points from graphs).&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Markdown Cheatsheet&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Webscraping Tutorial</title>
      <link>https://dmsenter89.github.io/slides/webscraping-tutorial/</link>
      <pubDate>Thu, 30 Jul 2020 12:30:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/slides/webscraping-tutorial/</guid>
      <description>&lt;h1 id=&#34;basics-of-web-scraping-with-python&#34;&gt;Basics of Web Scraping with Python&lt;/h1&gt;
&lt;p&gt;Michael Senter&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;goals-for-today&#34;&gt;Goals for Today&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Understand what tools and methods are available.
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Be able to create a new project using Python and Jupyter.
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Be able to edit existing code snippets to gather data.
&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;easy to learn, reads like &amp;ldquo;pseudocode&amp;rdquo;&lt;/li&gt;
&lt;li&gt;widely used in a variety of fields&lt;/li&gt;
&lt;li&gt;many books, websites, etc. to help you learn&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Hello, world!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;data-sources&#34;&gt;Data Sources&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;csvexcel-downloads&#34;&gt;CSV/Excel Downloads&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;covid-related-data&#34;&gt;COVID Related Data&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;johns-hopkins-dashboard&#34;&gt;Johns Hopkins Dashboard&lt;/h2&gt;
&lt;p&gt;The Johns Hopkins data is published on 
&lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; and is updated regularly.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;using-sas&#34;&gt;Using SAS&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;filename outfile &amp;quot;~/import-data-nyt.sas&amp;quot;;

/* download official SAS script to above filename */
proc http url=&amp;quot;https://raw.githubusercontent.com/sassoftware/covid-19-sas/master/Data/import-data-nyt.sas&amp;quot; 
  method=&amp;quot;get&amp;quot; out=outfile;
run;

/* run the downloaded script */
%include &amp;quot;~/import-data-nyt.sas&amp;quot;;
/* state and county level data are now in memory */
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Porting Forward</title>
      <link>https://dmsenter89.github.io/post/porting-forward/</link>
      <pubDate>Mon, 27 Jul 2020 11:31:07 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/porting-forward/</guid>
      <description>&lt;p&gt;My website is back up and running! Some incompatabilities between my old site and updates to both Hugo and the Academic Theme have led to some downtime on this page as I didn&amp;rsquo;t have time to look through how to rebuild my site without loosing previous content. I&amp;rsquo;m currently in the process of updating everything and will try to bring back some material as well. Stay tuned!&lt;/p&gt;
&lt;p&gt;This page is currently using the Academic theme from Hugo. Docs and other templates are available at 
&lt;a href=&#34;https://wowchemy.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wowchemy&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Thesis Proposal</title>
      <link>https://dmsenter89.github.io/talk/thesis-proposal/</link>
      <pubDate>Fri, 17 Apr 2020 09:00:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/talk/thesis-proposal/</guid>
      <description>&lt;p&gt;Please join me as I present the work I have done so far in my graduate career and discuss
avenues for future study.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Clap and Fling</title>
      <link>https://dmsenter89.github.io/project/clap-and-fling/</link>
      <pubDate>Fri, 01 Jun 2018 11:23:23 -0400</pubDate>
      <guid>https://dmsenter89.github.io/project/clap-and-fling/</guid>
      <description>&lt;p&gt;Insects are ubiquitious throughout the world. Most of us are familiar with winged insects such as butterflies and bees. Insect flight is an interesting topic from a biomechanics perspective. Unlike birds, most insects (with some eceptions, such as dragonflies and others) do not have flight muscles attached to their wings. Instead, their flight muscles oscillate their thorax, which in turn makes the wings move. Furthermore, they beat their wings at a very high speed. The aerodynamics of insect flight are also very interesting. Larger insects are able to fly by creating a leading edge vortex. This method does not work in the smallest insect fliers. Such insects include the thrips and chalcid wasps, some of which have wingspans as small as 1 mm. These insects have unusual wing structures, as can be seen in this image:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/b/b8/Thysanoptera-thripidae-sp.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The solid part of the wing is rather small and narrow, with many large bristles projecting from the solid part of the wing. Insects such as thrips do not create a leading edge vortex; instead, they fly using the &amp;ldquo;clap and fling&amp;rdquo; method. This method is common amongst insects who fly in the intermediate Reynolds number regime, $1\leq \mathrm{Re} \leq 100$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MeshmerizeMe</title>
      <link>https://dmsenter89.github.io/project/meshmerizeme/</link>
      <pubDate>Wed, 20 Sep 2017 11:35:06 -0400</pubDate>
      <guid>https://dmsenter89.github.io/project/meshmerizeme/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://github.com/nickabattista/IB2d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IB2d&lt;/a&gt; and

&lt;a href=&#34;https://github.com/IBAMR/IBAMR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IBAMR&lt;/a&gt; are two software packages implementing
the immersed boundary method (see below). These packages model fluid-structure
interaction problems based on user given parameters and geometry. The manual
creation of the initial geometry mesh can be difficult and time consuming,
especially for the complex shapes encountered in biological applications.
Oftentimes we have images of the geometry we wish to explore.
I am developing software to help automate the creation of such CFD meshes for
2D simulations with a file-format suitable for use with IB2d and IBAMR from
images. An initial prototype version is available on

&lt;a href=&#34;https://github.com/dmsenter89/MeshmerizeMe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;. A paper
exploring the use of MeshmerizeMe in conjuction with IB2d for simulations is
in preparation.&lt;/p&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;MeshmerizeMe needs two input files per experimental geometry: an SVG image file
with the geometry of interest and an input2d file with the experiment parameters.
When selecting an SVG for use with MeshmerizeMe it will automatically look for
the input2d file in the same folder. It will then parse the paths, transform
them into the correct coordinate system and appropriately sample the paths based
on the size of the Cartesian grid set in the input2d file. The geometry will be
exported as a vertex file. This file is readable by both IB2d and IBAMR.&lt;/p&gt;
&lt;p&gt;SVGs were chosen as the image source as the are an open, text-based format
making them very accesible to work with. They are standardized for web use and
many tools exist for creating and manipulating SVG images. They can be created
from source images such as photographs or scans by means of edge detection tools
and by manually tracing the outline of a shape of interest
Consider optimizing the SVG prior to processing to save time.&lt;/p&gt;
&lt;p&gt;As the current version of MeshmerizeMe only handles a subset of SVG, tools that
optimize the SVG files created by your editor are very useful. Examples of such
software include  
&lt;a href=&#34;https://github.com/svg/svgo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SVGO&lt;/a&gt;, which also offers a
webapp  called 
&lt;a href=&#34;https://jakearchibald.github.io/svgomg/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SVGOMG&lt;/a&gt;.
Another software is 
&lt;a href=&#34;https://github.com/RazrFalcon/svgcleaner&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;svgcleaner&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;ibm-background&#34;&gt;IBM Background&lt;/h2&gt;
&lt;p&gt;One aspect of computational fluid dynamics is the investigation of
fluid-structure interactions. One method developed for the study of such
interactions is the immersed boundary method (IBM) developed by Peskin&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
It is well known that fluids can be studied from both a Eulerian and a
Lagrangian view. The IBM combines these - the domain of the problem is resolved
as a Cartesian grid on which Eulerian equations are solved for fluid velocity
and pressure. In the case of Newtonian fluids the incompressible Navier-Stokes
equations comprising of&lt;/p&gt;
&lt;p&gt;$$ \rho  \left( \frac{\partial \mathbf{u}}{\partial t} + \mathbf{u} \cdot \nabla \mathbf{u} \right)  = - \nabla \mathbf{p} + \mu \nabla^2 \mathbf{u} + \mathbf{f}$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\nabla \cdot \mathbf{u} = 0$$&lt;/p&gt;
&lt;p&gt;need to be solved.&lt;/p&gt;
&lt;p&gt;The immersed structures are modeled as fibers in the form of parametric
curves $X(s,t)$, where $s$ is a parameter and $t$ is time. The fiber experiences
force distributions $F(s,t)$, and we can derive the force the fiber exerts on
the fluid from the momentum equation. For the fibers we then solve&lt;/p&gt;
&lt;p&gt;$$\mathbf{f} = \int_\Gamma \mathbf{F}(s,t),\delta\left(\mathbf{x}-\mathbf{X}(s,t)\right),ds$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\frac{\partial \mathbf{X}}{\partial t} = \int_\Omega \mathbf{u}(\mathbf{x},t), \delta \left( \mathbf{x}-\mathbf{X}(s,t)\right),d\mathbf{x}.$$&lt;/p&gt;
&lt;p&gt;Here, $\Gamma$ is the immersed structure and $\Omega$ is the fluid domain.&lt;/p&gt;
&lt;p&gt;The immersed structures are discretized not on a Cartesian grid but on a
separate Lagrangian grid on the fiber itself. Of import to CFD software users
is that the initial discretization of the immersed structure has to be
supplied by the user. While this is not too difficult for simple geometries,
the often complex structures encountered in mathematical biology can present
a significant time investment. This is the part where MeshmerizeMe comes in
handy.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Charles S Peskin. 2002. &amp;ldquo;The immersed boundary method.&amp;rdquo; &lt;em&gt;Acta numerica&lt;/em&gt; 11:479-517. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Mean first passage time in a thermally fluctuating viscoelastic fluid</title>
      <link>https://dmsenter89.github.io/publication/hohenegger-2017-mean/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/publication/hohenegger-2017-mean/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
