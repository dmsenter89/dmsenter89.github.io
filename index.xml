<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Michael&#39;s Site</title>
    <link>https://dmsenter89.github.io/</link>
      <atom:link href="https://dmsenter89.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Michael&#39;s Site</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 29 Oct 2020 22:39:00 -0400</lastBuildDate>
    <image>
      <url>https://dmsenter89.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Michael&#39;s Site</title>
      <link>https://dmsenter89.github.io/</link>
    </image>
    
    <item>
      <title>Teacher Salaries</title>
      <link>https://dmsenter89.github.io/post/20-10-tabula/</link>
      <pubDate>Thu, 29 Oct 2020 22:39:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/20-10-tabula/</guid>
      <description>&lt;p&gt;After reading a news article about teacher pay in the US, I was curious and wanted to look into the source data myself. Unfortunately, the source that was mentioned was a publication by the 
&lt;a href=&#34;https://www.nea.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Education Association (NEA)&lt;/a&gt; which had the data as tables embedded inside a PDF report. As those who know me can attest, I don&amp;rsquo;t like hand-copying data. It is slow and error-prone. Instead, I decided to use tabula to extract the information from the PDFs directly into a Pandas dataframe. In this post, I will show you how I did that and how I cleaned up the data for analysis.&lt;/p&gt;
&lt;h1 id=&#34;the-data-source&#34;&gt;The Data Source&lt;/h1&gt;
&lt;p&gt;Several years worth of data are available in PDF form on the 
&lt;a href=&#34;https://www.nea.org/research-publications&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NEA website&lt;/a&gt;. Reading through the technical notes, they highlight that they did not collect all of their own salary information. Some states&#39; information is calculated from the American Community Survey (ACS) done by the Census bureau - a great resource whose API I have covered in a different post. Each report includes accurate data for the previous school year, as well as estimates for the current school year. As of this post, the newest report is the 2020 report which includes data for the the 2018-2019 school year, as well as estimates of the 2019-2020 school year.&lt;/p&gt;
&lt;p&gt;The 2020 report has the desired teacher salary information in two separate locations. One is in table B-6 on page 26 of the PDF, which shows a ranking of the different states&#39; average salary in addition to the average salary:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table_B-6.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;A second location is in table E-7 on page 46, which gives salary data for the completed school year as well as different states&#39; estimates for the 2019-2020 school year:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table_E-7.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Note that table E-7 lacks the star-annotation marking NEA estimated values. This, and the lack of the ranking column, makes Table E-7 easier to parse. In the main example below, this will be the source of the five years of data. I will however also show how to parse table B-6 at the end of this post for completion.&lt;/p&gt;
&lt;h2 id=&#34;loading-the-data&#34;&gt;Loading the Data&lt;/h2&gt;
&lt;p&gt;As of October 2020, the NEA site has five years worth of reports online. Unfortunately, these are not labeled consistently for all five years. Similarly the page numbers differ for each report. Prior to the 2018 report, inconsistent formats were used for the tables which require previous years to be parsed separately from the newer tables. For this reason, I&amp;rsquo;ll make a dictionary for the 2018-2020 reports only, which will simplify the example below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;report = {
    &#39;2020&#39; : {
        &#39;url&#39;  : &amp;quot;https://www.nea.org/sites/default/files/2020-10/2020%20Rankings%20and%20Estimates%20Report.pdf&amp;quot;,
        &#39;page&#39; : 46,
    },
    &#39;2019&#39; : {
        &#39;url&#39;  : &amp;quot;https://www.nea.org/sites/default/files/2020-06/2019%20Rankings%20and%20Estimates%20Report.pdf&amp;quot;,
        &#39;page&#39; : 49,
    },
    &#39;2018&#39; : {
        &#39;url&#39;  : &amp;quot;https://www.nea.org/sites/default/files/2020-07/180413-Rankings_And_Estimates_Report_2018.pdf&amp;quot;,
        &#39;page&#39; : 51,
    },
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now use dictionary comprehension to fill in a dictionary with all the source tables of interest. We will be using the tabula package to extract data from the PDFs. If you don&amp;rsquo;t have it installed, you can use &lt;code&gt;pip install tabula-py&lt;/code&gt; to get a copy. The method that reads in a PDF is aptly called &lt;code&gt;read_pdf&lt;/code&gt;. It&amp;rsquo;s first argument is a file path to the PDF. Since we want to use a URL, we will use the keyword argument &lt;code&gt;stream=True&lt;/code&gt; and then name the specific page in each PDF that contains the information we are after. By default, &lt;code&gt;read_pdf&lt;/code&gt; returns a list of dataframes, so we just save the first element from the list, which is the report we are interested in.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; if you are using WSL, depending on your settings, you may the &lt;code&gt;Exception in thread &amp;quot;main&amp;quot; java.awt.AWTError: Can&#39;t connect to X11 window server using &#39;172.17.16.17:0&#39; as the value of the DISPLAY variable.&lt;/code&gt; error when running &lt;code&gt;read_pdf&lt;/code&gt;. This is fixed by having an X11 server running.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tabula
import pandas as pd

source_df = {year : tabula.read_pdf(report[year][&#39;url&#39;], stream=True, pages=report[year][&#39;page&#39;])[0] 
             for year in report.keys()}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&amp;rsquo;s it in principle. How cool is that! Of course, we still need to clean our data a little bit.&lt;/p&gt;
&lt;h2 id=&#34;cleaning-the-data&#34;&gt;Cleaning the Data&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at the first and last few entries of the 2020 report:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.concat([source_df[&#39;2020&#39;].head(), 
           source_df[&#39;2020&#39;].tail()])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;2018-19&lt;/th&gt;
      &lt;th&gt;2019-20&lt;/th&gt;
      &lt;th&gt;From 2018-19 to 2019-20&lt;/th&gt;
      &lt;th&gt;From 2010-11 to 2019-20 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;State&lt;/td&gt;
      &lt;td&gt;Salary($)&lt;/td&gt;
      &lt;td&gt;Salary($)&lt;/td&gt;
      &lt;td&gt;Change(%)&lt;/td&gt;
      &lt;td&gt;Current Dollar Constant Dollar&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
      &lt;td&gt;54,095&lt;/td&gt;
      &lt;td&gt;4.01&lt;/td&gt;
      &lt;td&gt;13.16 -2.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
      &lt;td&gt;70,877&lt;/td&gt;
      &lt;td&gt;0.85&lt;/td&gt;
      &lt;td&gt;15.36 -0.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
      &lt;td&gt;50,381&lt;/td&gt;
      &lt;td&gt;0.06&lt;/td&gt;
      &lt;td&gt;8.03 -7.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
      &lt;td&gt;49,822&lt;/td&gt;
      &lt;td&gt;0.78&lt;/td&gt;
      &lt;td&gt;8.31 -6.75&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;48&lt;/th&gt;
      &lt;td&gt;Washington&lt;/td&gt;
      &lt;td&gt;73,049&lt;/td&gt;
      &lt;td&gt;72,965&lt;/td&gt;
      &lt;td&gt;-0.11&lt;/td&gt;
      &lt;td&gt;37.86 18.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;49&lt;/th&gt;
      &lt;td&gt;West Virginia&lt;/td&gt;
      &lt;td&gt;47,681&lt;/td&gt;
      &lt;td&gt;50,238&lt;/td&gt;
      &lt;td&gt;5.36&lt;/td&gt;
      &lt;td&gt;13.51 -2.28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50&lt;/th&gt;
      &lt;td&gt;Wisconsin&lt;/td&gt;
      &lt;td&gt;58,277&lt;/td&gt;
      &lt;td&gt;59,176&lt;/td&gt;
      &lt;td&gt;1.54&lt;/td&gt;
      &lt;td&gt;9.17 -6.02&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;51&lt;/th&gt;
      &lt;td&gt;Wyoming&lt;/td&gt;
      &lt;td&gt;58,861&lt;/td&gt;
      &lt;td&gt;59,014&lt;/td&gt;
      &lt;td&gt;0.26&lt;/td&gt;
      &lt;td&gt;5.19 -9.44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;52&lt;/th&gt;
      &lt;td&gt;United States&lt;/td&gt;
      &lt;td&gt;62,304&lt;/td&gt;
      &lt;td&gt;63,645&lt;/td&gt;
      &lt;td&gt;2.15&lt;/td&gt;
      &lt;td&gt;14.14 -1.73&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We see that each column is treated as a string object (which you can confirm by running &lt;code&gt;source_df[&#39;2020&#39;].dtypes&lt;/code&gt;) and that the first row of data is actually at index 1 due to the fact that the PDF report used a two-row header. This means we can safely drop the first row of every dataframe. We can also drop the last row of every dataframe since that just contains summary data of the US as a whole, which we can easily regenerate as necessary. So row indices 0 and 52 can go for all of our data sets.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for df in source_df.values():
    df.drop([0, 52], inplace=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next up I&amp;rsquo;d like to fix the column names. The fist column is clearly the name of the state (except in the case of Washington D.C.), while the next two columns give the years for which the salary information is given. Let&amp;rsquo;s rename the second and third columns according to the pattern &lt;code&gt;Salary %YYYY-YY&lt;/code&gt; using Python&amp;rsquo;s f-string syntax.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for df in source_df.values():
    df.rename(columns={
        df.columns[0] : &amp;quot;State&amp;quot;,
        df.columns[1] : f&amp;quot;Salary {str(df.columns[1])}&amp;quot;,
        df.columns[2] : f&amp;quot;Salary {str(df.columns[2])}&amp;quot;,
    }, 
              inplace=True)
    
source_df[&amp;quot;2020&amp;quot;].head()  # show the result of our edits so far
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Salary 2018-19&lt;/th&gt;
      &lt;th&gt;Salary 2019-20&lt;/th&gt;
      &lt;th&gt;From 2018-19 to 2019-20&lt;/th&gt;
      &lt;th&gt;From 2010-11 to 2019-20 (%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
      &lt;td&gt;54,095&lt;/td&gt;
      &lt;td&gt;4.01&lt;/td&gt;
      &lt;td&gt;13.16 -2.58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
      &lt;td&gt;70,877&lt;/td&gt;
      &lt;td&gt;0.85&lt;/td&gt;
      &lt;td&gt;15.36 -0.69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
      &lt;td&gt;50,381&lt;/td&gt;
      &lt;td&gt;0.06&lt;/td&gt;
      &lt;td&gt;8.03 -7.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
      &lt;td&gt;49,822&lt;/td&gt;
      &lt;td&gt;0.78&lt;/td&gt;
      &lt;td&gt;8.31 -6.75&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;83,059&lt;/td&gt;
      &lt;td&gt;84,659&lt;/td&gt;
      &lt;td&gt;1.93&lt;/td&gt;
      &lt;td&gt;24.74 7.39&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Looks like we&amp;rsquo;re almost done! Let&amp;rsquo;s drop the unnecessary columns to and check our remaining column names:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for year, df in source_df.items():
    df.drop(df.columns[3:], axis=1, inplace=True)
    print(f&amp;quot;{year}:\t{df.columns}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020:	Index([&#39;State&#39;, &#39;Salary 2018-19&#39;, &#39;Salary 2019-20&#39;], dtype=&#39;object&#39;)
2019:	Index([&#39;State&#39;, &#39;Salary 2017-18&#39;, &#39;Salary 2018-19&#39;], dtype=&#39;object&#39;)
2018:	Index([&#39;State&#39;, &#39;Salary 2017&#39;, &#39;Salary 2018&#39;], dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the column naming scheme in 2018 was different than in the previous reports. To make them all compatible for our merge, we&amp;rsquo;re going to have to do some more editing. Based on the other reports, it appears as though the 2018 report used the calendar year of the &lt;em&gt;end&lt;/em&gt; of the school year, while the others utilized a range. This can easily be solved using regex substitution. We&amp;rsquo;ll do that now.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import re

for year, df in source_df.items():
    if year != &amp;quot;2018&amp;quot;:
        df.rename(columns={
            df.columns[1] : re.sub(r&amp;quot;\d{2}-&amp;quot;, &#39;&#39;, df.columns[1]),
            df.columns[2] : re.sub(r&amp;quot;\d{2}-&amp;quot;, &#39;&#39;, df.columns[2]),
        }, 
                  inplace=True)
    # always print the output for verification
    print(f&amp;quot;{year}:\t{df.columns}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020:	Index([&#39;State&#39;, &#39;Salary 2019&#39;, &#39;Salary 2020&#39;], dtype=&#39;object&#39;)
2019:	Index([&#39;State&#39;, &#39;Salary 2018&#39;, &#39;Salary 2019&#39;], dtype=&#39;object&#39;)
2018:	Index([&#39;State&#39;, &#39;Salary 2017&#39;, &#39;Salary 2018&#39;], dtype=&#39;object&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that everything works, we can do our merge to create a single dataframe with the information for all of the school years we have downloaded.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge_df = source_df[&amp;quot;2018&amp;quot;].drop([&amp;quot;Salary 2018&amp;quot;], axis=1).merge(
                    source_df[&amp;quot;2019&amp;quot;].drop([&amp;quot;Salary 2019&amp;quot;], axis=1)).merge(
                    source_df[&amp;quot;2020&amp;quot;])

merge_df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Salary 2017&lt;/th&gt;
      &lt;th&gt;Salary 2018&lt;/th&gt;
      &lt;th&gt;Salary 2019&lt;/th&gt;
      &lt;th&gt;Salary 2020&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;50,391&lt;/td&gt;
      &lt;td&gt;50,568&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
      &lt;td&gt;54,095&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;6 8,138&lt;/td&gt;
      &lt;td&gt;69,682&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
      &lt;td&gt;70,877&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;4 7,403&lt;/td&gt;
      &lt;td&gt;48,723&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
      &lt;td&gt;50,381&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;4 8,304&lt;/td&gt;
      &lt;td&gt;50,544&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
      &lt;td&gt;49,822&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;7 9,128&lt;/td&gt;
      &lt;td&gt;80,680&lt;/td&gt;
      &lt;td&gt;83,059&lt;/td&gt;
      &lt;td&gt;84,659&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id=&#34;numeric-conversion&#34;&gt;Numeric Conversion&lt;/h3&gt;
&lt;p&gt;We&amp;rsquo;re almost done! Notice that we still have not dealt with the fact that every column is still treated as a string. Before we can use the &lt;code&gt;to_numeric&lt;/code&gt; function, we still need to take care of two issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The commas in the numbers. While they are nice for us human eyes, Pandas doesn&amp;rsquo;t like them.&lt;/li&gt;
&lt;li&gt;In the 2017 salary column, there appears to be extraneous white space after the first digit for some entries.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Luckily, both of these problems can be remedied with a simple string replacement operation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge_df.iloc[:,1:] = merge_df.iloc[:,1:].replace(r&amp;quot;[,| ]&amp;quot;, &#39;&#39;, regex=True)

for col in merge_df.columns[1:]:
    merge_df[col] = pd.to_numeric(merge_df[col])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we&amp;rsquo;re done! We have created an overview of annual teacher salaries from the 2016-17 school year until 2019-20 extracted from a series of PDFs published by the NEA. We have cleaned up the data and converted everything to numerical values. We can now get summary statistics and do any analysis of interest with this data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge_df.describe() # summary stats of our numeric columns
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Salary 2017&lt;/th&gt;
      &lt;th&gt;Salary 2018&lt;/th&gt;
      &lt;th&gt;Salary 2019&lt;/th&gt;
      &lt;th&gt;Salary 2020&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
      &lt;td&gt;51.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;td&gt;56536.196078&lt;/td&gt;
      &lt;td&gt;57313.039216&lt;/td&gt;
      &lt;td&gt;58983.254902&lt;/td&gt;
      &lt;td&gt;60170.647059&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;td&gt;9569.444674&lt;/td&gt;
      &lt;td&gt;9795.914601&lt;/td&gt;
      &lt;td&gt;10286.843230&lt;/td&gt;
      &lt;td&gt;10410.259274&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;min&lt;/th&gt;
      &lt;td&gt;42925.000000&lt;/td&gt;
      &lt;td&gt;44926.000000&lt;/td&gt;
      &lt;td&gt;45105.000000&lt;/td&gt;
      &lt;td&gt;45192.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25%&lt;/th&gt;
      &lt;td&gt;49985.000000&lt;/td&gt;
      &lt;td&gt;50451.500000&lt;/td&gt;
      &lt;td&gt;51100.500000&lt;/td&gt;
      &lt;td&gt;52441.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;50%&lt;/th&gt;
      &lt;td&gt;54308.000000&lt;/td&gt;
      &lt;td&gt;53815.000000&lt;/td&gt;
      &lt;td&gt;54935.000000&lt;/td&gt;
      &lt;td&gt;57091.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;75%&lt;/th&gt;
      &lt;td&gt;61038.000000&lt;/td&gt;
      &lt;td&gt;61853.000000&lt;/td&gt;
      &lt;td&gt;64393.500000&lt;/td&gt;
      &lt;td&gt;66366.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;max&lt;/th&gt;
      &lt;td&gt;81902.000000&lt;/td&gt;
      &lt;td&gt;84227.000000&lt;/td&gt;
      &lt;td&gt;85889.000000&lt;/td&gt;
      &lt;td&gt;87543.000000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;table-b-6&#34;&gt;Table B-6&lt;/h2&gt;
&lt;p&gt;As mentioned above, table B-6 in the 2020 Report presents slightly greater challenges. A lot of the cleaning is similar or identical, so I will not reproduce it in full. Instead, I have created loaded and subsetted part of table B-6 and I will show how this can be cleaned up as well. But first, let&amp;rsquo;s look at the first several entries:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;b6
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;2017-18 (Revised)&lt;/th&gt;
      &lt;th&gt;2018-19&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;State&lt;/td&gt;
      &lt;td&gt;Salary($) Rank&lt;/td&gt;
      &lt;td&gt;Salary($)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;50,568 36&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;69,682 7&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;48,315 45&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,096 44&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;80,680 2&lt;/td&gt;
      &lt;td&gt;83,059 *&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Colorado&lt;/td&gt;
      &lt;td&gt;52,695 32&lt;/td&gt;
      &lt;td&gt;54,935&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Connecticut&lt;/td&gt;
      &lt;td&gt;74,517 * 5&lt;/td&gt;
      &lt;td&gt;76,465 *&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Delaware&lt;/td&gt;
      &lt;td&gt;62,422 13&lt;/td&gt;
      &lt;td&gt;63,662&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;We can see that there is an additional hurdle compared to the previous tables: the second column now contains data from two columns, both the Salary information as well as a ranking of the salary as it compares to the different states. For a few states, there is additionally a &amp;lsquo;*&amp;rsquo; to denote values that were estimated as opposed to received. We can again use a simple regex replace together with a capture group to parse out only those values that we are interested in, while dropping the extraneous information using the code below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;b6.iloc[:,1:] = b6.iloc[:,1:].replace(r&amp;quot;([\d,]+).*&amp;quot;, r&amp;quot;\1&amp;quot;, regex=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we&amp;rsquo;re back to where we were above before we did the string conversion. This is what it looks like after also dropping the first row and renaming the columns:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;b6
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;State&lt;/th&gt;
      &lt;th&gt;Salary 2018&lt;/th&gt;
      &lt;th&gt;Salary 2019&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Alabama&lt;/td&gt;
      &lt;td&gt;50,568&lt;/td&gt;
      &lt;td&gt;52,009&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Alaska&lt;/td&gt;
      &lt;td&gt;69,682&lt;/td&gt;
      &lt;td&gt;70,277&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Arizona&lt;/td&gt;
      &lt;td&gt;48,315&lt;/td&gt;
      &lt;td&gt;50,353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Arkansas&lt;/td&gt;
      &lt;td&gt;49,096&lt;/td&gt;
      &lt;td&gt;49,438&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;80,680&lt;/td&gt;
      &lt;td&gt;83,059&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Colorado&lt;/td&gt;
      &lt;td&gt;52,695&lt;/td&gt;
      &lt;td&gt;54,935&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Connecticut&lt;/td&gt;
      &lt;td&gt;74,517&lt;/td&gt;
      &lt;td&gt;76,465&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Delaware&lt;/td&gt;
      &lt;td&gt;62,422&lt;/td&gt;
      &lt;td&gt;63,662&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From here on out, we can proceed as in the previous example.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A semi-automated finite difference mesh creation method for use with immersed boundary software IB2d and IBAMR</title>
      <link>https://dmsenter89.github.io/publication/senter-2020-meshmerizeme/</link>
      <pubDate>Mon, 03 Aug 2020 11:27:55 -0400</pubDate>
      <guid>https://dmsenter89.github.io/publication/senter-2020-meshmerizeme/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Basics of Web Scraping with Python</title>
      <link>https://dmsenter89.github.io/talk/webscraping-tutorial/</link>
      <pubDate>Thu, 30 Jul 2020 13:00:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/talk/webscraping-tutorial/</guid>
      <description>&lt;p&gt;This workshop covers data acquisition and basic data preparation with a focus on using Python with Jupyter Notebooks. To avoid having to install Python locally during the workshop, we will be utilizing an 
&lt;a href=&#34;https://notebooks.azure.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azure notebook&lt;/a&gt; project. The example files are located 
&lt;a href=&#34;https://notebooks.azure.com/dmsenter/projects/datacollectiontutorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Please note that the free Azure notebooks will only be available until early October. To continue using Python and Jupyter notebooks, you may want to consider using a local installation. For Windows and Mac users, I recommend using 
&lt;a href=&#34;https://www.anaconda.com/products/individual&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anaconda&lt;/a&gt;. For continued cloud usage, you may consider 
&lt;a href=&#34;https://cocalc.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cocalc&lt;/a&gt;. Please note that you will need a subscription for your Cocalc notebooks to be able to download data from external sources.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Additional Links:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://markummitchell.github.io/engauge-digitizer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Engauge Digitizer&lt;/a&gt; (software to extract data points from graphs).&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Markdown Cheatsheet&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Webscraping Tutorial</title>
      <link>https://dmsenter89.github.io/slides/webscraping-tutorial/</link>
      <pubDate>Thu, 30 Jul 2020 12:30:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/slides/webscraping-tutorial/</guid>
      <description>&lt;h1 id=&#34;basics-of-web-scraping-with-python&#34;&gt;Basics of Web Scraping with Python&lt;/h1&gt;
&lt;p&gt;Michael Senter&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;goals-for-today&#34;&gt;Goals for Today&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Understand what tools and methods are available.
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Be able to create a new project using Python and Jupyter.
&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;fragment &#34; &gt;
Be able to edit existing code snippets to gather data.
&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;easy to learn, reads like &amp;ldquo;pseudocode&amp;rdquo;&lt;/li&gt;
&lt;li&gt;widely used in a variety of fields&lt;/li&gt;
&lt;li&gt;many books, websites, etc. to help you learn&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Hello, world!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;data-sources&#34;&gt;Data Sources&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;csvexcel-downloads&#34;&gt;CSV/Excel Downloads&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;covid-related-data&#34;&gt;COVID Related Data&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;johns-hopkins-dashboard&#34;&gt;Johns Hopkins Dashboard&lt;/h2&gt;
&lt;p&gt;The Johns Hopkins data is published on 
&lt;a href=&#34;https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; and is updated regularly.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;using-sas&#34;&gt;Using SAS&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-sas&#34;&gt;filename outfile &amp;quot;~/import-data-nyt.sas&amp;quot;;

/* download official SAS script to above filename */
proc http url=&amp;quot;https://raw.githubusercontent.com/sassoftware/covid-19-sas/master/Data/import-data-nyt.sas&amp;quot; 
  method=&amp;quot;get&amp;quot; out=outfile;
run;

/* run the downloaded script */
%include &amp;quot;~/import-data-nyt.sas&amp;quot;;
/* state and county level data are now in memory */
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Porting Forward</title>
      <link>https://dmsenter89.github.io/post/porting-forward/</link>
      <pubDate>Mon, 27 Jul 2020 11:31:07 -0400</pubDate>
      <guid>https://dmsenter89.github.io/post/porting-forward/</guid>
      <description>&lt;p&gt;My website is back up and running! Some incompatabilities between my old site and updates to both Hugo and the Academic Theme have led to some downtime on this pageas I didn&amp;rsquo;t have time to look through how to rebuild my site without loosing previous content. I&amp;rsquo;m currently in the process of updating everything and will try to bring back some material as well. Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Thesis Proposal</title>
      <link>https://dmsenter89.github.io/talk/thesis-proposal/</link>
      <pubDate>Fri, 17 Apr 2020 09:00:00 -0400</pubDate>
      <guid>https://dmsenter89.github.io/talk/thesis-proposal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Clap and Fling</title>
      <link>https://dmsenter89.github.io/project/clap-and-fling/</link>
      <pubDate>Fri, 01 Jun 2018 11:23:23 -0400</pubDate>
      <guid>https://dmsenter89.github.io/project/clap-and-fling/</guid>
      <description>&lt;p&gt;Insects are ubiquitious throughout the world. Most of us are familiar with winged insects such as butterflies and bees. Insect flight is an interesting topic from a biomechanics perspective. Unlike birds, most insects (with some eceptions, such as dragonflies and others) do not have flight muscles attached to their wings. Instead, their flight muscles oscillate their thorax, which in turn makes the wings move. The aerodynamics of insect flight are also very interesting. Larger insects are able to fly by creating a leading edge vortex. This method does not work in the smallest insect fliers. Such insects include the thrips and chalcid wasps, some of which have wingspans as small as 1 mm. These insects have unusual wing structures, as can be seen in this image:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/b/b8/Thysanoptera-thripidae-sp.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The solid part of the wing is rather small and narrow, with many large bristles projecting from the solid part of the wing. Insects such as thrips do not create a leading edge vortex; instead, they fly using the &amp;ldquo;Clap-and-Fling&amp;rdquo; method. This method is common amongst insects who fly in the intermediate Reynolds number regime, $1\leq \mathrm{Re} \leq 100$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MeshmerizeMe</title>
      <link>https://dmsenter89.github.io/project/meshmerizeme/</link>
      <pubDate>Wed, 20 Sep 2017 11:35:06 -0400</pubDate>
      <guid>https://dmsenter89.github.io/project/meshmerizeme/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://github.com/nickabattista/IB2d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IB2d&lt;/a&gt; and

&lt;a href=&#34;https://github.com/IBAMR/IBAMR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IBAMR&lt;/a&gt; are two software packages implementing
the immersed boundary method (see below). These packages model fluid-structure
interaction problems based on user given parameters and geometry. The manual
creation of the initial geometry mesh can be difficult and time consuming,
especially for the complex shapes encountered in biological applications.
Oftentimes we have images of the geometry we wish to explore.
I am developing software to help automate the creation of such CFD meshes for
2D simulations with a file-format suitable for use with IB2d and IBAMR from
images. An initial prototype version is available on

&lt;a href=&#34;https://github.com/dmsenter89/MeshmerizeMe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;. A paper
exploring the use of MeshmerizeMe in conjuction with IB2d for simulations is
in preparation.&lt;/p&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;MeshmerizeMe needs two input files per experimental geometry: an SVG image file
with the geometry of interest and an input2d file with the experiment parameters.
When selecting an SVG for use with MeshmerizeMe it will automatically look for
the input2d file in the same folder. It will then parse the paths, transform
them into the correct coordinate system and appropriately sample the paths based
on the size of the Cartesian grid set in the input2d file. The geometry will be
exported as a vertex file. This file is readable by both IB2d and IBAMR.&lt;/p&gt;
&lt;p&gt;SVGs were chosen as the image source as the are an open, text-based format
making them very accesible to work with. They are standardized for web use and
many tools exist for creating and manipulating SVG images. They can be created
from source images such as photographs or scans by means of edge detection tools
and by manually tracing the outline of a shape of interest
Consider optimizing the SVG prior to processing to save time.&lt;/p&gt;
&lt;p&gt;As the current version of MeshmerizeMe only handles a subset of SVG, tools that
optimize the SVG files created by your editor are very useful. Examples of such
software include  
&lt;a href=&#34;https://github.com/svg/svgo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SVGO&lt;/a&gt;, which also offers a
webapp  called 
&lt;a href=&#34;https://jakearchibald.github.io/svgomg/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SVGOMG&lt;/a&gt;.
Another software is 
&lt;a href=&#34;https://github.com/RazrFalcon/svgcleaner&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;svgcleaner&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;ibm-background&#34;&gt;IBM Background&lt;/h2&gt;
&lt;p&gt;One aspect of computational fluid dynamics is the investigation of
fluid-structure interactions. One method developed for the study of such
interactions is the immersed boundary method (IBM) developed by Peskin&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
It is well known that fluids can be studied from both a Eulerian and a
Lagrangian view. The IBM combines these - the domain of the problem is resolved
as a Cartesian grid on which Eulerian equations are solved for fluid velocity
and pressure. In the case of Newtonian fluids the incompressible Navier-Stokes
equations comprising of&lt;/p&gt;
&lt;p&gt;$$ \rho  \left( \frac{\partial \mathbf{u}}{\partial t} + \mathbf{u} \cdot \nabla \mathbf{u} \right)  = - \nabla \mathbf{p} + \mu \nabla^2 \mathbf{u} + \mathbf{f}$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\nabla \cdot \mathbf{u} = 0$$&lt;/p&gt;
&lt;p&gt;need to be solved.&lt;/p&gt;
&lt;p&gt;The immersed structures are modeled as fibers in the form of parametric
curves $X(s,t)$, where $s$ is a parameter and $t$ is time. The fiber experiences
force distributions $F(s,t)$, and we can derive the force the fiber exerts on
the fluid from the momentum equation. For the fibers we then solve&lt;/p&gt;
&lt;p&gt;$$\mathbf{f} = \int_\Gamma \mathbf{F}(s,t),\delta\left(\mathbf{x}-\mathbf{X}(s,t)\right),ds$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\frac{\partial \mathbf{X}}{\partial t} = \int_\Omega \mathbf{u}(\mathbf{x},t), \delta \left( \mathbf{x}-\mathbf{X}(s,t)\right),d\mathbf{x}.$$&lt;/p&gt;
&lt;p&gt;Here, $\Gamma$ is the immersed structure and $\Omega$ is the fluid domain.&lt;/p&gt;
&lt;p&gt;The immersed structures are discretized not on a Cartesian grid but on a
separate Lagrangian grid on the fiber itself. Of import to CFD software users
is that the initial discretization of the immersed structure has to be
supplied by the user. While this is not too difficult for simple geometries,
the often complex structures encountered in mathematical biology can present
a significant time investment. This is the part where MeshmerizeMe comes in
handy.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Charles S Peskin. 2002. &amp;ldquo;The immersed boundary method.&amp;rdquo; &lt;em&gt;Acta numerica&lt;/em&gt; 11:479-517. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Mean first passage time in a thermally fluctuating viscoelastic fluid</title>
      <link>https://dmsenter89.github.io/publication/hohenegger-2017-mean/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://dmsenter89.github.io/publication/hohenegger-2017-mean/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
